#!/usr/bin/env python3
"""
æ•™å­¸ 28 ç¯„ä¾‹ï¼šMulti-LLM Agent ç¯„ä¾‹
å±•ç¤ºå¦‚ä½•é€é LiteLLM æ­é…ç¯„ä¾‹æŸ¥è©¢ä½¿ç”¨ä¸åŒçš„ LLM
"""

import asyncio
import os
import sys
from typing import Dict, Any

# å°‡çˆ¶ç›®éŒ„åŠ å…¥ Python è·¯å¾‘ä»¥ä¾¿åŒ¯å…¥
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google.adk.runners import Runner
from google.adk.agents.run_config import RunConfig, StreamingMode
from google.adk.sessions import InMemorySessionService
from google.genai import types

from multi_llm_agent.agent import root_agent, gpt4o_agent, claude_agent, ollama_agent


async def run_query(agent, query: str, description: str) -> Dict[str, Any]:
    """ä½¿ç”¨æŒ‡å®šçš„ agent åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›çµæœã€‚"""
    print(f"\nğŸ¤– {description}")
    print(f"ğŸ’¬ æŸ¥è©¢: {query}")
    print("-" * 50)

    try:
        # å»ºç«‹ runner å’Œ session æœå‹™
        session_service = InMemorySessionService()
        runner = Runner(app_name="multi_llm_demo", agent=agent, session_service=session_service)

        # ç‚ºæ­¤å°è©±å»ºç«‹ä¸€å€‹ session
        session = await session_service.create_session(
            app_name="multi_llm_demo",
            user_id="demo_user"
        )

        # è¨­å®šç‚ºéä¸²æµæ¨¡å¼ (å–å¾—å®Œæ•´å›æ‡‰)
        run_config = RunConfig(
            streaming_mode=StreamingMode.NONE,
            max_llm_calls=50
        )

        # æ”¶é›†æ‰€æœ‰å›æ‡‰ç‰‡æ®µ
        response_parts = []

        # ä½¿ç”¨æŸ¥è©¢åŸ·è¡Œ agent
        async for event in runner.run_async(
            user_id="demo_user",
            session_id=session.id,
            new_message=types.Content(role="user", parts=[types.Part(text=query)]),
            run_config=run_config
        ):
            if event.content and event.content.parts:
                for part in event.content.parts:
                    if part.text:
                        response_parts.append(part.text)

            if event.turn_complete:
                break

        result = ''.join(response_parts)
        print(f"ğŸ“ å›æ‡‰: {result}")
        return {"success": True, "result": result, "description": description}

    except Exception as e:
        error_msg = f"âŒ ä½¿ç”¨ {description} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(error_msg)
        return {"success": False, "error": str(e), "description": description}


async def demo_basic_math():
    """å±•ç¤ºä½¿ç”¨ä¸åŒ LLM é€²è¡ŒåŸºæœ¬æ•¸å­¸è¨ˆç®—ã€‚"""
    print("\n" + "="*60)
    print("ğŸ§® ç¯„ä¾‹ 1: æ•¸å­¸è¨ˆç®—")
    print("="*60)

    query = "15 çš„å¹³æ–¹æ˜¯å¤šå°‘ï¼Ÿè«‹ä½¿ç”¨ calculate_square å·¥å…·ã€‚"

    # ä½¿ç”¨ä¸åŒçš„ agent é€²è¡Œæ¸¬è©¦
    agents = [
        (root_agent, "OpenAI GPT-4o-mini (é è¨­)"),
        (gpt4o_agent, "OpenAI GPT-4o-mini (æ›¿ä»£æ–¹æ¡ˆ)"),
        (claude_agent, "Claude 3.7 Sonnet"),
        (ollama_agent, "Ollama Granite 4 (æœ¬åœ°)"),
    ]

    results = []
    for agent, desc in agents:
        result = await run_query(agent, query, desc)
        results.append(result)

    return results


async def demo_weather_info():
    """å±•ç¤ºå¤©æ°£è³‡è¨Šæª¢ç´¢ã€‚"""
    print("\n" + "="*60)
    print("ğŸŒ¤ï¸  ç¯„ä¾‹ 2: å¤©æ°£è³‡è¨Š")
    print("="*60)

    query = "èˆŠé‡‘å±±ç›®å‰çš„å¤©æ°£å¦‚ä½•ï¼Ÿè«‹ä½¿ç”¨ get_weather å·¥å…·ã€‚"

    agents = [
        (root_agent, "OpenAI GPT-4o-mini"),
        (claude_agent, "Claude 3.7 Sonnet"),
        (ollama_agent, "Ollama Granite 4 (æœ¬åœ°)"),
    ]

    results = []
    for agent, desc in agents:
        result = await run_query(agent, query, desc)
        results.append(result)

    return results


async def demo_sentiment_analysis():
    """å±•ç¤ºæ–‡å­—æƒ…ç·’åˆ†æã€‚"""
    print("\n" + "="*60)
    print("ğŸ˜Š ç¯„ä¾‹ 3: æƒ…ç·’åˆ†æ")
    print("="*60)

    query = "åˆ†æé€™æ®µæ–‡å­—çš„æƒ…ç·’ï¼š'æˆ‘éå¸¸å–œæ­¡é€™å€‹æ–°çš„äººå·¥æ™ºæ…§æŠ€è¡“ï¼å®ƒæ¥µå…·å‰µæ–°æ€§ï¼Œå¾¹åº•æ”¹è®Šäº†æˆ‘çš„å·¥ä½œæ–¹å¼ã€‚'"

    agents = [
        (root_agent, "OpenAI GPT-4o-mini"),
        (gpt4o_agent, "OpenAI GPT-4o-mini"),
        (claude_agent, "Claude 3.7 Sonnet"),
        (ollama_agent, "Ollama Granite 4 (æœ¬åœ°)"),
    ]

    results = []
    for agent, desc in agents:
        result = await run_query(agent, query, desc)
        results.append(result)

    return results


async def demo_comparison():
    """å±•ç¤ºæ¯”è¼ƒä¸åŒ LLM çš„å›æ‡‰ã€‚"""
    print("\n" + "="*60)
    print("âš–ï¸  ç¯„ä¾‹ 4: LLM æ¯”è¼ƒ")
    print("="*60)

    query = "ç”¨ä¸€å€‹ 10 æ­²å°å­©èƒ½ç†è§£çš„ç°¡å–®è©å½™è§£é‡‹é‡å­è¨ˆç®—ã€‚"

    print(f"ğŸ¯ æŸ¥è©¢: {query}")
    print("\n" + "-"*60)

    agents = [
        (root_agent, "OpenAI GPT-4o-mini"),
        (claude_agent, "Claude 3.7 Sonnet"),
        (ollama_agent, "Ollama Granite 4 (æœ¬åœ°)"),
    ]

    responses = {}
    for agent, desc in agents:
        try:
            # å»ºç«‹ runner å’Œ session æœå‹™
            session_service = InMemorySessionService()
            runner = Runner(app_name="multi_llm_demo", agent=agent, session_service=session_service)

            # å»ºç«‹ä¸€å€‹ session
            session = await session_service.create_session(
                app_name="multi_llm_demo",
                user_id="demo_user"
            )

            # è¨­å®šç‚ºéä¸²æµæ¨¡å¼
            run_config = RunConfig(
                streaming_mode=StreamingMode.NONE,
                max_llm_calls=50
            )

            # æ”¶é›†å›æ‡‰
            response_parts = []
            async for event in runner.run_async(
                user_id="demo_user",
                session_id=session.id,
                new_message=types.Content(role="user", parts=[types.Part(text=query)]),
                run_config=run_config
            ):
                if event.content and event.content.parts:
                    for part in event.content.parts:
                        if part.text:
                            response_parts.append(part.text)

                if event.turn_complete:
                    break

            result = ''.join(response_parts)
            responses[desc] = result
            print(f"\nğŸ¤– {desc}:")
            print(f"   {result}")
        except Exception as e:
            print(f"\nâŒ {desc}: éŒ¯èª¤ - {str(e)}")

    return responses


async def main():
    """ä¸»è¦ç¯„ä¾‹å‡½å¼ã€‚"""
    print("ğŸš€ æ•™å­¸ 28: Multi-LLM Agent ç¯„ä¾‹")
    print("ä½¿ç”¨ LiteLLM å­˜å– OpenAI, Claude, ä»¥åŠå…¶ä»– LLM")
    print("="*60)

    # æª¢æŸ¥å¿…è¦çš„ API é‡‘é‘°
    has_openai = bool(os.getenv("OPENAI_API_KEY"))
    has_anthropic = bool(os.getenv("ANTHROPIC_API_KEY"))
    has_ollama = True  # å‡è¨­ Ollama åœ¨æœ¬åœ°å¯ç”¨

    print("ğŸ”‘ API é‡‘é‘°ç‹€æ…‹:")
    print(f"   OpenAI: {'âœ…' if has_openai else 'âŒ'} (GPT æ¨¡å‹éœ€è¦)")
    print(f"   Anthropic: {'âœ…' if has_anthropic else 'âŒ'} (Claude éœ€è¦)")
    print(f"   Ollama: {'âœ…' if has_ollama else 'âŒ'} (æœ¬åœ° Granite 4 æ¨¡å‹)")
    print()

    if not has_openai and not has_anthropic and not has_ollama:
        print("âš ï¸  è­¦å‘Šï¼šæœªåµæ¸¬åˆ° API é‡‘é‘°æˆ–æœ¬åœ°æ¨¡å‹ã€‚ç¯„ä¾‹å¯èƒ½æœƒåŸ·è¡Œå¤±æ•—ã€‚")
        print("   è«‹è¨­å®š OPENAI_API_KEY, ANTHROPIC_API_KEY, æˆ–ç¢ºä¿ Ollama æ­£åœ¨åŸ·è¡Œã€‚")
        print()

    # åŸ·è¡Œç¯„ä¾‹
    try:
        await demo_basic_math()
        await demo_weather_info()
        await demo_sentiment_analysis()
        await demo_comparison()

        print("\n" + "="*60)
        print("âœ… ç¯„ä¾‹åŸ·è¡Œå®Œç•¢ï¼")
        print("="*60)
        print("ğŸ’¡ æ ¸å¿ƒè¦é»:")
        print("   â€¢ LiteLLM è®“åˆ‡æ› LLM ä¾›æ‡‰å•†è®Šå¾—ç°¡å–®")
        print("   â€¢ æ¯å€‹ LLM éƒ½æœ‰ä¸åŒçš„å„ªå‹¢ (æˆæœ¬ã€é€Ÿåº¦ã€æ¨ç†èƒ½åŠ›)")
        print("   â€¢ å·¥å…·åœ¨ä¸åŒæ¨¡å‹ä¹‹é–“èƒ½ä¸€è‡´åœ°é‹ä½œ")
        print("   â€¢ åƒ Ollama (Granite 4) é€™æ¨£çš„æœ¬åœ°æ¨¡å‹æä¾›éš±ç§å’Œé›¢ç·šèƒ½åŠ›")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·ç¯„ä¾‹åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ ç¯„ä¾‹å› éŒ¯èª¤è€Œå¤±æ•—: {str(e)}")
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
