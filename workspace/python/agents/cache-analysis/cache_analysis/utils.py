# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""ç”¨æ–¼å¿«å–åˆ†æå¯¦é©—ï¼ˆCache Analysis Experimentsï¼‰çš„å…¬ç”¨å‡½å¼ã€‚"""

import asyncio
import time
from typing import Any, Dict, List

from google.adk.runners import InMemoryRunner


async def call_agent_async(
    runner: InMemoryRunner, user_id: str, session_id: str, prompt: str
) -> Dict[str, Any]:
    """
    ä»¥éåŒæ­¥æ–¹å¼å‘¼å«ä»£ç†ï¼ˆAgentï¼‰ï¼Œä¸¦å‚³å›åŒ…å« Token ä½¿ç”¨é‡ï¼ˆToken Usageï¼‰çš„éŸ¿æ‡‰ã€‚

    ç¨‹å¼ç¢¼æµç¨‹ï¼š
    1. åˆå§‹åŒ–éŸ¿æ‡‰å…§å®¹åˆ—è¡¨èˆ‡ Token ä½¿ç”¨é‡è¨ˆæ•¸å™¨ã€‚
    2. ä½¿ç”¨ runner.run_async å•Ÿå‹•ä»£ç†ã€‚
    3. è¿­ä»£éåŒæ­¥ä¸²æµä¸­çš„æ¯å€‹äº‹ä»¶ï¼ˆEventï¼‰ã€‚
    4. æå–æ–‡å­—å…§å®¹ï¼ˆText Contentï¼‰ä¸¦ç´¯åŠ  Token ä½¿ç”¨é‡æ•¸æ“šï¼ˆæç¤ºã€å€™é¸ã€å¿«å–ã€ç¸½é‡ï¼‰ã€‚
    5. çµ„åˆæœ€çµ‚æ–‡å­—ä¸¦å‚³å›ã€‚
    """
    from google.genai import types

    response_parts = []
    # åˆå§‹åŒ– Token ä½¿ç”¨é‡å­—å…¸
    token_usage = {
        "prompt_token_count": 0,          # æç¤º Token æ•¸
        "candidates_token_count": 0,      # å€™é¸ï¼ˆå›ç­”ï¼‰Token æ•¸
        "cached_content_token_count": 0,  # å¿«å–å…§å®¹ Token æ•¸
        "total_token_count": 0,           # ç¸½ Token æ•¸
    }

    # åŸ·è¡ŒéåŒæ­¥ä»£ç†èª¿ç”¨
    async for event in runner.run_async(
        user_id=user_id,
        session_id=session_id,
        new_message=types.Content(parts=[types.Part(text=prompt)], role="user"),
    ):
        # è™•ç†è¼¸å‡ºçš„æ–‡å­—ç‰‡æ®µ
        if event.content and event.content.parts:
            for part in event.content.parts:
                if hasattr(part, "text") and part.text:
                    response_parts.append(part.text)

        # æ”¶é›†ä¸¦ç´¯è¨ˆ Token ä½¿ç”¨é‡è³‡è¨Šï¼ˆUsage Metadataï¼‰
        if event.usage_metadata:
            if (
                hasattr(event.usage_metadata, "prompt_token_count")
                and event.usage_metadata.prompt_token_count
            ):
                token_usage["prompt_token_count"] += event.usage_metadata.prompt_token_count
            if (
                hasattr(event.usage_metadata, "candidates_token_count")
                and event.usage_metadata.candidates_token_count
            ):
                token_usage["candidates_token_count"] += event.usage_metadata.candidates_token_count
            if (
                hasattr(event.usage_metadata, "cached_content_token_count")
                and event.usage_metadata.cached_content_token_count
            ):
                token_usage["cached_content_token_count"] += event.usage_metadata.cached_content_token_count
            if (
                hasattr(event.usage_metadata, "total_token_count")
                and event.usage_metadata.total_token_count
            ):
                token_usage["total_token_count"] += event.usage_metadata.total_token_count

    response_text = "".join(response_parts)

    return {"response_text": response_text, "token_usage": token_usage}


def get_test_prompts() -> List[str]:
    """
    ç²å–å¿«å–åˆ†æå¯¦é©—çš„æ¨™æº–æ¸¬è©¦æç¤ºï¼ˆTest Promptsï¼‰é›†ã€‚

    è¨­è¨ˆç”¨æ–¼ä¸€è‡´çš„è¡Œç‚ºè§€å¯Ÿï¼š
    - æç¤º 1-5ï¼šä¸æœƒè§¸ç™¼å‡½å¼èª¿ç”¨ï¼ˆFunction Callsï¼‰ï¼Œåƒ…ç‚ºä¸€èˆ¬å•é¡Œã€‚
    - æç¤º 6-10ï¼šæœƒè§¸ç™¼å‡½å¼èª¿ç”¨ï¼ŒåŒ…å«å…·é«”çš„å·¥å…·è«‹æ±‚ã€‚
    """
    return [
        # === ä¸æœƒè§¸ç™¼å‡½å¼èª¿ç”¨çš„æç¤º ===
        #ï¼ˆä¸åŒ¹é…å…·é«”å·¥å…·èªªæ˜çš„é€šç”¨å•é¡Œï¼‰
        "ä½ å¥½ï¼Œä½ èƒ½ç‚ºæˆ‘åšä»€éº¼ï¼Ÿ",
        "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿå®ƒåœ¨ç¾ä»£æ‡‰ç”¨ä¸­æ˜¯å¦‚ä½•é‹ä½œçš„ï¼Ÿ",
        "è«‹è§£é‡‹æ©Ÿå™¨å­¸ç¿’ï¼ˆMachine Learningï¼‰èˆ‡æ·±åº¦å­¸ç¿’ï¼ˆDeep Learningï¼‰ä¹‹é–“çš„å·®ç•°ã€‚",
        "åœ¨å¤§è¦æ¨¡å¯¦æ–½ AI ç³»çµ±æ™‚ï¼Œä¸»è¦çš„æŒ‘æˆ°æœ‰å“ªäº›ï¼Ÿ",
        "ç¾ä»£é›»å­å•†å‹™å¹³å°ä¸­çš„æ¨è–¦ç³»çµ±ï¼ˆRecommendation Systemsï¼‰æ˜¯å¦‚ä½•é‹ä½œçš„ï¼Ÿ",
        # === æœƒè§¸ç™¼å‡½å¼èª¿ç”¨çš„æç¤º ===
        #ï¼ˆæ˜ç¢ºæŒ‡å®šäº†æ‰€æœ‰å¿…è¦åƒæ•¸çš„å…·é«”è«‹æ±‚ï¼‰
        (
            "ä½¿ç”¨ benchmark_performance ä¸¦è¨­å®š system_name='E-commerce Platform',"
            " metrics=['latency', 'throughput'], duration='standard',"
            " load_profile='realistic'ã€‚"
        ),
        (
            "å‘¼å« analyze_user_behavior_patterns ä¸¦è¨­å®š"
            " user_segment='premium_customers', time_period='last_30_days',"
            " metrics=['engagement', 'conversion']ã€‚"
        ),
        (
            "é‡å° industry='fintech', focus_areas=['user_experience', 'security'],"
            " report_depth='comprehensive' åŸ·è¡Œ market_research_analysisã€‚"
        ),
        (
            "å° competitors=['Netflix', 'Disney+'], analysis_type='feature_comparison',"
            " output_format='detailed' åŸ·è¡Œ competitive_analysisã€‚"
        ),
        (
            "å° content_type='video', platform='social_media', "
            "success_metrics=['views', 'engagement'] é€²è¡Œ content_performance_evaluationã€‚"
        ),
    ]


async def run_experiment_batch(
    agent_name: str,
    runner: InMemoryRunner,
    user_id: str,
    session_id: str,
    prompts: List[str],
    experiment_name: str,
    request_delay: float = 2.0,
) -> Dict[str, Any]:
    """
    åŸ·è¡Œä¸€æ‰¹æç¤ºï¼ˆBatch Promptsï¼‰ä¸¦æ”¶é›†å¿«å–æŒ‡æ¨™ï¼ˆCache Metricsï¼‰ã€‚

    ç¨‹å¼ç¢¼æµç¨‹ï¼š
    1. éæ­·æç¤ºåˆ—è¡¨ï¼Œé€ä¸€å‘¼å« call_agent_asyncã€‚
    2. è¨˜éŒ„æ¯å€‹æç¤ºçš„åŸ·è¡Œçµæœã€Token ä½¿ç”¨æƒ…æ³èˆ‡æ˜¯å¦æˆåŠŸã€‚
    3. åœ¨è«‹æ±‚ä¹‹é–“æ’å…¥å¯é…ç½®çš„å»¶é²ï¼ˆDelayï¼‰ä»¥é¿å… API éè¼‰ã€‚
    4. çµ±è¨ˆè©²æ‰¹æ¬¡ï¼ˆBatchï¼‰çš„å¿«å–å‘½ä¸­ç‡ï¼ˆCache Hit Ratioï¼‰èˆ‡å¿«å–åˆ©ç”¨ç‡ï¼ˆCache Utilizationï¼‰ã€‚
    5. ç”¢å‡ºä¸¦å°å‡ºå®Œæ•´çš„å¯¦é©—æ‘˜è¦å ±å‘Šã€‚
    """
    results = []

    print(f"ğŸ§ª æ­£åœ¨åŸ·è¡Œ {experiment_name}")
    print(f"ä»£ç†åç¨±: {agent_name}")
    print(f"æœƒè©± ID: {session_id}")
    print(f"æç¤ºæ•¸é‡: {len(prompts)}")
    print(f"è«‹æ±‚é–“éš”å»¶é²: {request_delay} ç§’")
    print("-" * 60)

    for i, prompt in enumerate(prompts, 1):
        print(f"[{i}/{len(prompts)}] æ­£åœ¨åŸ·è¡Œæ¸¬è©¦æç¤º...")
        print(f"æç¤ºå…§å®¹: {prompt[:100]}...")

        try:
            agent_response = await call_agent_async(
                runner, user_id, session_id, prompt
            )

            result = {
                "prompt_number": i,
                "prompt": prompt,
                "response_length": len(agent_response["response_text"]),
                "success": True,
                "error": None,
                "token_usage": agent_response["token_usage"],
            }

            # æå–å€‹åˆ¥æç¤ºçµ±è¨ˆçš„ Token ä½¿ç”¨é‡
            prompt_tokens = agent_response["token_usage"].get("prompt_token_count", 0)
            cached_tokens = agent_response["token_usage"].get(
                "cached_content_token_count", 0
            )

            print(
                f"âœ… å®Œæˆ (éŸ¿æ‡‰é•·åº¦: {len(agent_response['response_text'])} å­—å…ƒ)"
            )
            print(
                f"   ğŸ“Š Tokens - æç¤º: {prompt_tokens:,}, å¿«å–: {cached_tokens:,}"
            )

        except Exception as e:
            result = {
                "prompt_number": i,
                "prompt": prompt,
                "response_length": 0,
                "success": False,
                "error": str(e),
                "token_usage": {
                    "prompt_token_count": 0,
                    "candidates_token_count": 0,
                    "cached_content_token_count": 0,
                    "total_token_count": 0,
                },
            }

            print(f"âŒ å¤±æ•—: {e}")

        results.append(result)

        # åœ¨è«‹æ±‚ä¹‹é–“é€²è¡Œå¯é…ç½®çš„æš«åœï¼Œä»¥é¿å… API è¶…è¼‰
        if i < len(prompts):  # æœ€å¾Œä¸€å€‹è«‹æ±‚å¾Œä¸éœ€è¦ç¡çœ 
            print(f"   Wait â¸ï¸  ç­‰å¾… {request_delay} ç§’å¾Œé€²è¡Œä¸‹ä¸€å€‹è«‹æ±‚...")
            await asyncio.sleep(request_delay)

    successful_requests = sum(1 for r in results if r["success"])

    # è¨ˆç®—æ­¤æ‰¹æ¬¡çš„å¿«å–çµ±è¨ˆæ•¸æ“š
    total_prompt_tokens = sum(
        r.get("token_usage", {}).get("prompt_token_count", 0) for r in results
    )
    total_cached_tokens = sum(
        r.get("token_usage", {}).get("cached_content_token_count", 0)
        for r in results
    )

    # è¨ˆç®—å¿«å–å‘½ä¸­ç‡ (Cache Hit Ratio)
    if total_prompt_tokens > 0:
        cache_hit_ratio = (total_cached_tokens / total_prompt_tokens) * 100
    else:
        cache_hit_ratio = 0.0

    # è¨ˆç®—å¿«å–åˆ©ç”¨ç‡ (Cache Utilization)
    requests_with_cache_hits = sum(
        1
        for r in results
        if r.get("token_usage", {}).get("cached_content_token_count", 0) > 0
    )
    cache_utilization_ratio = (
        (requests_with_cache_hits / len(prompts)) * 100 if prompts else 0.0
    )

    # å¹³å‡æ¯æ¬¡è«‹æ±‚çš„å¿«å– Token æ•¸
    avg_cached_tokens_per_request = (
        total_cached_tokens / len(prompts) if prompts else 0.0
    )

    summary = {
        "experiment_name": experiment_name,
        "agent_name": agent_name,
        "total_requests": len(prompts),
        "successful_requests": successful_requests,
        "results": results,
        "cache_statistics": {
            "cache_hit_ratio_percent": cache_hit_ratio,
            "cache_utilization_ratio_percent": cache_utilization_ratio,
            "total_prompt_tokens": total_prompt_tokens,
            "total_cached_tokens": total_cached_tokens,
            "avg_cached_tokens_per_request": avg_cached_tokens_per_request,
            "requests_with_cache_hits": requests_with_cache_hits,
        },
    }

    print("-" * 60)
    print(f"âœ… {experiment_name} åŸ·è¡Œå®Œç•¢:")
    print(f"   ç¸½è«‹æ±‚æ•¸: {len(prompts)}")
    print(f"   æˆåŠŸæ¬¡æ•¸: {successful_requests}/{len(prompts)}")
    print("   ğŸ“Š æ‰¹æ¬¡å¿«å–çµ±è¨ˆ (BATCH CACHE STATISTICS):")
    print(
        f"      å¿«å–å‘½ä¸­ç‡: {cache_hit_ratio:.1f}%"
        f" ({total_cached_tokens:,} / {total_prompt_tokens:,} tokens)"
    )
    print(
        f"      å¿«å–åˆ©ç”¨ç‡: {cache_utilization_ratio:.1f}%"
        f" ({requests_with_cache_hits}/{len(prompts)} è«‹æ±‚)"
    )
    print(f"      å¹³å‡æ¯æ¬¡è«‹æ±‚å¿«å– Token: {avg_cached_tokens_per_request:.0f}")
    print()

    return summary
