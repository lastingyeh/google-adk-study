"""
èªéŸ³åŠ©ç† Agent
å®Œæ•´çš„ VoiceAssistant å¯¦ä½œï¼ŒåŒ…å«éŒ„éŸ³ã€æ’­æ”¾èˆ‡å°è©±åŠŸèƒ½ã€‚
"""

import asyncio
import os
from typing import Optional
from google.adk.agents import Agent, LiveRequestQueue
from google.adk.agents.run_config import RunConfig, StreamingMode
from google.adk.apps import App
from google.adk.runners import Runner
from google.genai import Client, types, errors

try:
    import pyaudio

    PYAUDIO_AVAILABLE = True
except ImportError:
    PYAUDIO_AVAILABLE = False


class VoiceAssistant:
    """
    ä½¿ç”¨ Live API çš„å³æ™‚èªéŸ³åŠ©ç†ã€‚

    åŠŸèƒ½ï¼š
    - å¾éº¥å…‹é¢¨éŒ„è£½éŸ³è¨Š
    - é€éå–‡å­æ’­æ”¾éŸ³è¨Š
    - é›™å‘ä¸²æµå°è©±
    - å¤šç¨®èªéŸ³è¨­å®š
    """

    def __init__(
        self,
        model: Optional[str] = None,
        voice_name: str = "Puck",
        sample_rate: int = 16000,
        audio_mode: bool = False,
    ):
        """
        åˆå§‹åŒ–èªéŸ³åŠ©ç†ã€‚

        Args:
            model: è¦ä½¿ç”¨çš„ Live API æ¨¡å‹
            voice_name: èªéŸ³è¨­å®š (Puck, Charon, Kore, Fenrir, Aoede)
            sample_rate: éŸ³è¨Šå–æ¨£ç‡ (Hz)
            audio_mode: è‹¥ç‚º Trueï¼Œä½¿ç”¨éŸ³è¨Šæ¨¡æ…‹ã€‚è‹¥ç‚º Falseï¼Œä½¿ç”¨æ–‡å­—æ¨¡æ…‹ã€‚
        """

        # --- éŸ³è¨Šè¨­å®š ---
        self.chunk_size = 1024  # éŸ³è¨Šç·©è¡å€å¤§å°
        self.sample_rate = sample_rate  # å–æ¨£ç‡
        self.channels = 1  # å–®è²é“
        self.format = pyaudio.paInt16 if PYAUDIO_AVAILABLE else None  # 16-bit PCM æ ¼å¼
        self.voice_name = voice_name  # èªéŸ³åç¨±
        self.audio_mode = audio_mode  # æ˜¯å¦å•Ÿç”¨éŸ³è¨Šæ¨¡å¼

        # PyAudio å¯¦ä¾‹ (å»¶é²åˆå§‹åŒ–)
        self._audio = None

        # --- æ¨¡å‹è¨­å®š ---
        # æ±ºå®šè¦ä½¿ç”¨çš„ Live API æ¨¡å‹ï¼Œå„ªå…ˆä½¿ç”¨å‚³å…¥çš„åƒæ•¸ï¼Œå…¶æ¬¡æ˜¯ç’°å¢ƒè®Šæ•¸ï¼Œæœ€å¾Œæ˜¯é è¨­å€¼
        self.live_model = model or os.getenv(
            "VOICE_ASSISTANT_LIVE_MODEL", "gemini-2.0-flash-live-001"
        )

        # --- Agent è¨­å®š ---
        # å»ºç«‹ Agentï¼Œå®šç¾©å…¶æ¨¡å‹ã€åç¨±ã€æè¿°èˆ‡æŒ‡ä»¤
        self.agent = Agent(
            model=self.live_model,
            name="voice_assistant",
            description="å³æ™‚èªéŸ³åŠ©ç†",
            instruction="""
            ä½ æ˜¯å€‹æ¨‚æ–¼åŠ©äººçš„èªéŸ³åŠ©ç†ã€‚è«‹éµå®ˆä»¥ä¸‹æº–å‰‡ï¼š

            - è‡ªç„¶ä¸”å£èªåŒ–åœ°å›æ‡‰
            - ç‚ºäº†èªéŸ³äº’å‹•ï¼Œå›æ‡‰è«‹ä¿æŒç°¡æ½”ï¼ˆæœ€å¤š 2-3 å¥è©±ï¼‰
            - å¿…è¦æ™‚æå‡ºæ¾„æ¸…å•é¡Œ
            - ä¿æŒå‹å–„èˆ‡è¦ªåˆ‡çš„æ…‹åº¦
            - ä½¿ç”¨é©åˆå£èªå°è©±çš„éæ­£å¼èªè¨€
            - é™¤éç‰¹åˆ¥è¦æ±‚ï¼Œå¦å‰‡é¿å…å†—é•·çš„è§£é‡‹
            """.strip(),
            generate_content_config=types.GenerateContentConfig(
                temperature=0.8,  # è®“å°è©±æ›´è‡ªç„¶
                max_output_tokens=150,  # ç‚ºäº†èªéŸ³äº’å‹•ï¼Œé™åˆ¶è¼¸å‡ºçš„ token æ•¸é‡
            ),
        )

        # --- åŸ·è¡Œèˆ‡ä¸²æµè¨­å®š ---
        # æ ¹æ“š audio_mode è¨­å®š live streaming çš„åƒæ•¸
        if audio_mode:
            # éŸ³è¨Šæ¨¡å¼ï¼šæ¥æ”¶éŸ³è¨Šå›æ‡‰
            self.run_config = RunConfig(
                streaming_mode=StreamingMode.BIDI,  # é›™å‘ä¸²æµ
                speech_config=types.SpeechConfig(
                    voice_config=types.VoiceConfig(
                        prebuilt_voice_config=types.PrebuiltVoiceConfig(
                            voice_name=voice_name
                        )
                    )
                ),
                response_modalities=["audio"],  # æŒ‡å®šå›æ‡‰æ¨¡æ…‹ç‚ºéŸ³è¨Š
            )
        else:
            # æ–‡å­—æ¨¡å¼ï¼šæ¥æ”¶æ–‡å­—å›æ‡‰ (å‚™æ´)
            self.run_config = RunConfig(
                streaming_mode=StreamingMode.BIDI,
                speech_config=types.SpeechConfig(
                    voice_config=types.VoiceConfig(
                        prebuilt_voice_config=types.PrebuiltVoiceConfig(
                            voice_name=voice_name
                        )
                    )
                ),
                response_modalities=["text"],  # æŒ‡å®šå›æ‡‰æ¨¡æ…‹ç‚ºæ–‡å­—
            )

        # --- èªè­‰ç­–ç•¥ ---
        # æ±ºå®šä½¿ç”¨ Vertex AI é‚„æ˜¯ Google AI çš„èªè­‰æ–¹å¼
        self.project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
        self.vertex_location = (
            os.getenv("GOOGLE_CLOUD_LOCATION")
            or os.getenv("GOOGLE_GENAI_VERTEXAI_LOCATION")
            or "us-central1"
        )
        self.use_vertex_live = bool(
            os.getenv("GOOGLE_GENAI_USE_VERTEXAI") and self.project_id
        )
        if self.use_vertex_live:
            # ç¢ºä¿ä¸‹æ¸¸å‡½å¼åº«èƒ½å–å¾— location
            if not os.getenv("GOOGLE_GENAI_VERTEXAI_LOCATION"):
                os.environ["GOOGLE_GENAI_VERTEXAI_LOCATION"] = self.vertex_location
            if not os.getenv("GOOGLE_CLOUD_LOCATION"):
                os.environ["GOOGLE_CLOUD_LOCATION"] = self.vertex_location
        self._api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        self._client: Optional[Client] = None
        self.text_model = os.getenv("VOICE_ASSISTANT_TEXT_MODEL", "gemini-2.5-flash")

        # --- App èˆ‡ Runner ---
        # å»ºç«‹ App (Runner åœ¨éœ€è¦æ™‚å»¶é²å»ºç«‹)
        self.app = App(name="voice_assistant_app", root_agent=self.agent)
        self._runner: Optional[Runner] = None

        # --- Session ç®¡ç† ---
        self._session_id: Optional[str] = None
        self._user_id = "voice_user"

    async def _fallback_generate_text(self, text: str) -> str:
        """ç•¶ Live API ä¸²æµç„¡æ³•ä½¿ç”¨æ™‚ï¼Œæ”¹ç”¨ Responses API ä½œç‚ºå‚™æ´ã€‚"""
        # å»¶é²åˆå§‹åŒ– Client
        if self._client is None:
            if self.use_vertex_live:
                # ä½¿ç”¨ Vertex AI ç«¯é»èˆ‡ ADC æ†‘è­‰
                self._client = Client(
                    vertexai=True,
                    project=self.project_id,
                    location=self.vertex_location,
                )
            elif self._api_key:
                # ç›´æ¥ä½¿ç”¨ API é‡‘é‘°æ¨¡å¼ (Google hosted endpoint)
                self._client = Client(api_key=self._api_key)
            else:
                # ä½¿ç”¨é è¨­ Client (ä¾‹å¦‚ï¼šgcloud auth application-default login)
                self._client = Client()

        # æº–å‚™ä½¿ç”¨è€…è¼¸å…¥å…§å®¹
        user_content = types.Content(
            role="user", parts=[types.Part.from_text(text=text)]
        )
        model_name = self.text_model
        # å¦‚æœä¸æ˜¯ä½¿ç”¨ Vertex AI ä¸”æ¨¡å‹åç¨±ä¸åŒ…å« "/"ï¼Œå‰‡è‡ªå‹•åŠ ä¸Š "models/" å‰ç¶´
        if not self.use_vertex_live and "/" not in model_name:
            model_name = f"models/{model_name}"

        try:
            # é€ééåŒæ­¥æ–¹å¼å‘¼å« generate_content
            response = await asyncio.to_thread(
                self._client.models.generate_content,
                model=model_name,
                contents=[user_content],
            )
        except errors.ClientError as err:
            print(f"âŒ å‚™æ´æ¨¡å‹éŒ¯èª¤ ({err})ã€‚è«‹æª¢æŸ¥ VOICE_ASSISTANT_TEXT_MODELã€‚")
            return "æˆ‘ç¾åœ¨ç„¡æ³•é€£ç·šåˆ°æ–‡å­—æ¨¡å‹ã€‚è«‹ç¢ºèªæ‚¨çš„ API å­˜å–æ¬Šé™ã€‚"

        # è§£æå›æ‡‰ä¸¦çµ„åˆæˆå–®ä¸€å­—ä¸²
        parts: list[str] = []
        for candidate in getattr(response, "candidates", []) or []:
            content = getattr(candidate, "content", None)
            if not content or not getattr(content, "parts", None):
                continue
            for part in content.parts:
                value = getattr(part, "text", None)
                if value:
                    parts.append(value)
        return "".join(parts)

    @property
    def runner(self) -> Runner:
        """å»¶é²åˆå§‹åŒ– Runnerã€‚"""
        if self._runner is None:
            from google.adk.sessions import InMemorySessionService

            session_service = InMemorySessionService()
            self._runner = Runner(app=self.app, session_service=session_service)
        return self._runner

    @property
    def audio(self):
        """å»¶é²åˆå§‹åŒ– PyAudioã€‚"""
        if not PYAUDIO_AVAILABLE:
            raise RuntimeError(
                "PyAudio ç„¡æ³•ä½¿ç”¨ã€‚è«‹é€éä»¥ä¸‹æŒ‡ä»¤å®‰è£ï¼špip install pyaudio"
            )

        if self._audio is None:
            self._audio = pyaudio.PyAudio()
        return self._audio

    async def record_audio(self, duration_seconds: int = 5) -> bytes:
        """
        å¾éº¥å…‹é¢¨éŒ„è£½éŸ³è¨Šã€‚

        Args:
            duration_seconds: éŒ„éŸ³æ™‚é•·

        Returns:
            éŸ³è¨Šè³‡æ–™ (bytes)
        """

        print(f"ğŸ¤ æ­£åœ¨éŒ„éŸ³ {duration_seconds} ç§’...")

        # é–‹å•ŸéŸ³è¨Šä¸²æµ
        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
        )

        frames = []

        # éŒ„è£½æŒ‡å®šæ™‚é•·çš„éŸ³è¨Š
        for _ in range(0, int(self.sample_rate / self.chunk_size * duration_seconds)):
            data = stream.read(self.chunk_size)
            frames.append(data)

        # åœæ­¢ä¸¦é—œé–‰ä¸²æµ
        stream.stop_stream()
        stream.close()

        print("âœ… éŒ„éŸ³å®Œæˆ")

        return b"".join(frames)

    def play_audio(self, audio_data: bytes):
        """
        é€éå–‡å­æ’­æ”¾éŸ³è¨Šã€‚

        Args:
            audio_data: è¦æ’­æ”¾çš„éŸ³è¨Š bytes
        """

        # é–‹å•Ÿè¼¸å‡ºä¸²æµ
        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.sample_rate,
            output=True,
        )

        # å¯«å…¥éŸ³è¨Šè³‡æ–™ä¸¦æ’­æ”¾
        stream.write(audio_data)
        stream.stop_stream()
        stream.close()

    async def _ensure_session(self):
        """ç¢ºä¿ session å·²è¢«å»ºç«‹ã€‚"""
        if self._session_id is None:
            session = await self.runner.session_service.create_session(
                app_name=self.app.name, user_id=self._user_id
            )
            self._session_id = session.id

    async def send_text(self, text: str) -> str:
        """
        å‚³é€æ–‡å­—è¨Šæ¯ä¸¦å–å¾—å›æ‡‰ã€‚

        Args:
            text: ä½¿ç”¨è€…è¨Šæ¯

        Returns:
            Agent çš„æ–‡å­—å›æ‡‰
        """

        await self._ensure_session()

        # å¦‚æœä¸æ˜¯ä½¿ç”¨ Vertex AIï¼Œå‰‡ä½¿ç”¨å‚™æ´çš„æ–‡å­—ç”Ÿæˆæ–¹æ³•
        if not self.use_vertex_live:
            return await self._fallback_generate_text(text)

        # å»ºç«‹ç”¨æ–¼ live streaming çš„ä½‡åˆ—
        queue = LiveRequestQueue()
        queue.send_content(
            types.Content(role="user", parts=[types.Part.from_text(text=text)])
        )
        queue.close()

        response_text: list[str] = []
        try:
            # åŸ·è¡Œ live run ä¸¦è™•ç†äº‹ä»¶
            async for event in self.runner.run_live(
                live_request_queue=queue,
                user_id=self._user_id,
                session_id=self._session_id,
                run_config=self.run_config,
            ):
                if event.content and event.content.parts:
                    for part in event.content.parts:
                        if part.text:
                            response_text.append(part.text)
        except Exception as exc:
            print(f"âš ï¸  Live session éŒ¯èª¤ ({exc})ï¼›åˆ‡æ›è‡³å‚™æ´æ–‡å­—å›æ‡‰ã€‚")
            return await self._fallback_generate_text(text)

        # å¦‚æœä¸²æµæ²’æœ‰å›å‚³ä»»ä½•å…§å®¹ï¼Œå‰‡ä½¿ç”¨å‚™æ´æ–¹æ³•
        if not response_text:
            return await self._fallback_generate_text(text)

        return "".join(response_text)

    async def send_audio(self, audio_data: bytes) -> tuple[str, list[bytes]]:
        """
        å‚³é€éŸ³è¨Šä¸¦å–å¾—å›æ‡‰ã€‚

        Args:
            audio_data: éŸ³è¨Š bytes

        Returns:
            ä¸€å€‹åŒ…å« (æ–‡å­—å›æ‡‰, éŸ³è¨Šå›æ‡‰å€å¡Š) çš„ tuple
        """

        await self._ensure_session()

        # å»ºç«‹ä½‡åˆ—
        queue = LiveRequestQueue()

        # ä½¿ç”¨ send_realtime å‚³é€éŸ³è¨Š
        queue.send_realtime(
            blob=types.Blob(
                data=audio_data, mime_type=f"audio/pcm;rate={self.sample_rate}"
            )
        )

        # é—œé–‰ä½‡åˆ—
        queue.close()

        # æ”¶é›†å›æ‡‰
        text_response = []
        audio_response = []

        async for event in self.runner.run_live(
            live_request_queue=queue,
            user_id=self._user_id,
            session_id=self._session_id,
            run_config=self.run_config,
        ):
            if event.content and event.content.parts:
                for part in event.content.parts:
                    if part.text:
                        text_response.append(part.text)
                    if part.inline_data:
                        audio_response.append(part.inline_data.data)

        return "".join(text_response), audio_response

    async def conversation_turn(self, user_audio: bytes):
        """
        åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„éŸ³è¨Šå°è©±å›åˆã€‚

        Args:
            user_audio: ä½¿ç”¨è€…çš„éŸ³è¨Šè¼¸å…¥
        """

        print("\nğŸ¤– Agent å›æ‡‰ä¸­...")

        # å‚³é€éŸ³è¨Šä¸¦å–å¾—å›æ‡‰
        text_response, audio_response = await self.send_audio(user_audio)

        # å°å‡ºæ–‡å­—å›æ‡‰
        print(text_response)

        # å¦‚æœæœ‰éŸ³è¨Šå›æ‡‰ï¼Œå‰‡æ’­æ”¾
        if audio_response:
            print("ğŸ”Š æ­£åœ¨æ’­æ”¾å›æ‡‰...")
            combined_audio = b"".join(audio_response)
            self.play_audio(combined_audio)

    def cleanup(self):
        """æ¸…ç†è³‡æºã€‚"""
        if self._audio is not None:
            self._audio.terminate()


# ç‚ºäº†è®“ ADK èƒ½å¤ ç™¼ç¾ï¼ŒåŒ¯å‡º root_agent
root_agent = Agent(
    model=os.getenv("VOICE_ASSISTANT_LIVE_MODEL", "gemini-2.0-flash-live-001"),
    name="voice_assistant",
    description="æ”¯æ´ Live API çš„å³æ™‚èªéŸ³åŠ©ç†",
    instruction="""
    ä½ æ˜¯å€‹æ¨‚æ–¼åŠ©äººçš„èªéŸ³åŠ©ç†ã€‚è«‹éµå®ˆä»¥ä¸‹æº–å‰‡ï¼š

    - è‡ªç„¶ä¸”å£èªåŒ–åœ°å›æ‡‰
    - ç‚ºäº†èªéŸ³äº’å‹•ï¼Œå›æ‡‰è«‹ä¿æŒç°¡æ½”ï¼ˆæœ€å¤š 2-3 å¥è©±ï¼‰
    - å¿…è¦æ™‚æå‡ºæ¾„æ¸…å•é¡Œ
    - ä¿æŒå‹å–„èˆ‡è¦ªåˆ‡çš„æ…‹åº¦
    - ä½¿ç”¨é©åˆå£èªå°è©±çš„éæ­£å¼èªè¨€
    - é™¤éç‰¹åˆ¥è¦æ±‚ï¼Œå¦å‰‡é¿å…å†—é•·çš„è§£é‡‹
    """.strip(),
    generate_content_config=types.GenerateContentConfig(
        temperature=0.8, max_output_tokens=150
    ),
)
