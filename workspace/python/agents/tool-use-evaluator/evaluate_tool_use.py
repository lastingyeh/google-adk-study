#!/usr/bin/env python3
"""
ä½¿ç”¨ RUBRIC_BASED_TOOL_USE_QUALITY_V1 é€²è¡Œå·¥å…·ä½¿ç”¨å“è³ªçš„çœŸå¯¦è©•ä¼°ã€‚

æ­¤è…³æœ¬é‡å°è‡ªå®šç¾©è©•é‡è¡¨åŸ·è¡Œä»£ç†å·¥å…·ä½¿ç”¨æƒ…æ³çš„å¯¦éš›è©•ä¼°ã€‚
å®ƒå±•ç¤ºäº†å¦‚ä½•ï¼š
1. å»ºç«‹åŒ…å«é æœŸå·¥å…·é †åºçš„æ¸¬è©¦æ¡ˆä¾‹
2. ä½¿ç”¨åŸºæ–¼è©•é‡è¡¨çš„å·¥å…·ä½¿ç”¨å“è³ªæŒ‡æ¨™é…ç½®è©•ä¼°
3. åŸ·è¡Œè©•ä¼°ä¸¦è§£è®€çµæœ
4. è™•ç†è‰¯å¥½èˆ‡ä¸è‰¯çš„å·¥å…·æ’åºæ¨¡å¼

ç”¨æ³•ï¼š
    python evaluate_tool_use.py

éœ€æ±‚ï¼š
    - è¨­å®š GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸
    - google-genai >= 1.16.0
    - åœ¨ evalset.json ä¸­å®šç¾©çš„æ¸¬è©¦æ¡ˆä¾‹
"""

import asyncio
import json
import os
from pathlib import Path

from google.adk.evaluation.agent_evaluator import AgentEvaluator


async def create_evalset_file():
    """å»ºç«‹ç”¨æ–¼è©•ä¼°çš„ evalset.json æ¸¬è©¦æ¡ˆä¾‹æª”æ¡ˆã€‚"""
    evalset_path = Path(__file__).parent / "tool_use_quality.evalset.json"
    config_path = Path(__file__).parent / "test_config.json"

    # å®šç¾©åŒ…å«è‰¯å¥½å’Œä¸è‰¯å·¥å…·ä½¿ç”¨é †åºçš„è©•ä¼°é›†
    evalset_data = {
        "eval_set_id": "tool_use_quality_evaluation",
        "name": "å·¥å…·ä½¿ç”¨å“è³ªè©•ä¼°",
        "description": "è©•ä¼°ä»£ç†å·¥å…·çš„æ’åºå’Œå“è³ª",
        "eval_cases": [
            {
                "eval_id": "good_sequence_complete_pipeline",
                "conversation": [
                    {
                        "invocation_id": "inv-001",
                        "user_content": {
                            "parts": [
                                {
                                    "text": "åˆ†æéŠ·å”®è³‡æ–™é›†ä¸¦æ‡‰ç”¨é æ¸¬æ¨¡å‹"
                                }
                            ],
                            "role": "user",
                        },
                        "final_response": {
                            "parts": [
                                {
                                    "text": "æˆ‘å·²ç¶“åˆ†æäº†éŠ·å”®è³‡æ–™é›†ï¼Œæå–äº†ç‰¹å¾µï¼Œé©—è­‰äº†å“è³ªï¼Œä¸¦æ‡‰ç”¨äº†é æ¸¬æ¨¡å‹ã€‚è©²æ¨¡å‹åœ¨ç‰¹å¾µä¸Šé”åˆ°äº† 87% çš„æº–ç¢ºç‡ã€‚"
                                }
                            ],
                            "role": "model",
                        },
                        "intermediate_data": {
                            "tool_uses": [
                                {
                                    "id": "call-001",
                                    "name": "analyze_data",
                                    "args": {"dataset": "sales_dataset"},
                                },
                                {
                                    "id": "call-002",
                                    "name": "extract_features",
                                    "args": {"data": {"type": "analysis_result"}},
                                },
                                {
                                    "id": "call-003",
                                    "name": "validate_quality",
                                    "args": {"features": {"type": "features"}},
                                },
                                {
                                    "id": "call-004",
                                    "name": "apply_model",
                                    "args": {
                                        "features": {"type": "validated_features"},
                                        "model": "random_forest",
                                    },
                                },
                            ],
                            "intermediate_responses": [],
                        },
                    }
                ],
                "session_input": {
                    "app_name": "tool_use_evaluator",
                    "user_id": "test_user",
                    "state": {},
                },
            },
            {
                "eval_id": "bad_sequence_skipped_validation",
                "conversation": [
                    {
                        "invocation_id": "inv-002",
                        "user_content": {
                            "parts": [
                                {
                                    "text": "è™•ç†å®¢æˆ¶è³‡æ–™é›†ä»¥é€²è¡Œå»ºæ¨¡"
                                }
                            ],
                            "role": "user",
                        },
                        "final_response": {
                            "parts": [
                                {
                                    "text": "æˆ‘å·²ç¶“æå–äº†ç‰¹å¾µä¸¦æ‡‰ç”¨äº†æ¨¡å‹ã€‚"
                                }
                            ],
                            "role": "model",
                        },
                        "intermediate_data": {
                            "tool_uses": [
                                {
                                    "id": "call-101",
                                    "name": "extract_features",
                                    "args": {"data": {"type": "raw_data"}},
                                },
                                {
                                    "id": "call-102",
                                    "name": "apply_model",
                                    "args": {
                                        "features": {"type": "features"},
                                        "model": "linear_regression",
                                    },
                                },
                            ],
                            "intermediate_responses": [],
                        },
                    }
                ],
                "session_input": {
                    "app_name": "tool_use_evaluator",
                    "user_id": "test_user",
                    "state": {},
                },
            },
            {
                "eval_id": "good_sequence_proper_analysis",
                "conversation": [
                    {
                        "invocation_id": "inv-003",
                        "user_content": {
                            "parts": [
                                {
                                    "text": "åˆ†æä¸¦æº–å‚™æ©Ÿå™¨å­¸ç¿’ç”¨çš„è³‡æ–™é›†"
                                }
                            ],
                            "role": "user",
                        },
                        "final_response": {
                            "parts": [
                                {
                                    "text": "è³‡æ–™é›†å·²åˆ†æä¸¦æº–å‚™å¥½ï¼Œå…·å‚™å·²é©—è­‰çš„ç‰¹å¾µï¼Œå¯éš¨æ™‚é€²è¡Œå»ºæ¨¡ã€‚"
                                }
                            ],
                            "role": "model",
                        },
                        "intermediate_data": {
                            "tool_uses": [
                                {
                                    "id": "call-201",
                                    "name": "analyze_data",
                                    "args": {"dataset": "customer_data"},
                                },
                                {
                                    "id": "call-202",
                                    "name": "extract_features",
                                    "args": {"data": {"type": "analysis"}},
                                },
                                {
                                    "id": "call-203",
                                    "name": "validate_quality",
                                    "args": {"features": {"type": "extracted_features"}},
                                },
                            ],
                            "intermediate_responses": [],
                        },
                    }
                ],
                "session_input": {
                    "app_name": "tool_use_evaluator",
                    "user_id": "test_user",
                    "state": {},
                },
            },
        ],
    }

    # å®šç¾©å…·æœ‰åŸºæ–¼è©•é‡è¡¨çš„å·¥å…·ä½¿ç”¨å“è³ªæŒ‡æ¨™çš„è©•ä¼°é…ç½®
    eval_config = {
        "criteria": {
            "rubric_based_tool_use_quality_v1": {
                "threshold": 0.7,
                "judge_model_options": {
                    "judge_model": "gemini-2.5-flash",
                    "num_samples": 3,
                },
                "rubrics": [
                    {
                        "rubric_id": "proper_tool_order",
                        "rubric_content": {
                            "text_property": "ä»£ç†åœ¨ extract_features ä¹‹å‰å‘¼å« analyze_dataã€‚é€™éµå®ˆäº†å·¥å…·ä¾è³´é—œä¿‚ã€‚"
                        },
                    },
                    {
                        "rubric_id": "complete_pipeline",
                        "rubric_content": {
                            "text_property": "å°æ–¼å®Œæ•´çš„åˆ†æï¼Œä»£ç†æ‡‰è©²å‘¼å«ï¼šanalyze â†’ extract â†’ validate â†’ apply (å…¨éƒ¨ 4 å€‹æ­¥é©Ÿ)"
                        },
                    },
                    {
                        "rubric_id": "validation_before_model",
                        "rubric_content": {
                            "text_property": "ä»£ç†åœ¨æ‡‰ç”¨æ¨¡å‹ä¹‹å‰é©—è­‰ç‰¹å¾µå“è³ª"
                        },
                    },
                    {
                        "rubric_id": "no_tool_failures",
                        "rubric_content": {
                            "text_property": "æ‰€æœ‰å·¥å…·å‘¼å«çš†æˆåŠŸä¸”å…·æœ‰é©ç•¶çš„åƒæ•¸ (ç„¡éŒ¯èª¤æˆ–ç¼ºå°‘åƒæ•¸)"
                        },
                    },
                ],
            }
        }
    }

    # å°‡ evalset å¯«å…¥æª”æ¡ˆ
    with open(evalset_path, "w") as f:
        json.dump(evalset_data, f, indent=2, ensure_ascii=False)

    # å°‡ config å¯«å…¥æª”æ¡ˆ
    with open(config_path, "w") as f:
        json.dump(eval_config, f, indent=2, ensure_ascii=False)

    return evalset_path


async def run_evaluation(evalset_path: Path):
    """ä½¿ç”¨ RUBRIC_BASED_TOOL_USE_QUALITY_V1 æŒ‡æ¨™åŸ·è¡Œè©•ä¼°ã€‚

    Args:
        evalset_path: evalset.json æª”æ¡ˆçš„è·¯å¾‘
    """
    print("\n" + "=" * 80)
    print("çœŸå¯¦è©•ä¼°: åŸºæ–¼è©•é‡è¡¨çš„å·¥å…·ä½¿ç”¨å“è³ª V1 (RUBRIC BASED TOOL USE QUALITY V1)")
    print("=" * 80 + "\n")

    # è©•é‡è¡¨å·²åœ¨ test_config.json ä¸­å®šç¾©
    rubrics = [
        ("proper_tool_order", "ä»£ç†åœ¨ extract_features ä¹‹å‰å‘¼å« analyze_data"),
        ("complete_pipeline", "å°æ–¼å®Œæ•´åˆ†æï¼šanalyze â†’ extract â†’ validate â†’ apply"),
        ("validation_before_model", "ä»£ç†åœ¨å»ºæ¨¡å‰é©—è­‰ç‰¹å¾µå“è³ª"),
        ("no_tool_failures", "æ‰€æœ‰å·¥å…·å‘¼å«çš†æˆåŠŸä¸”åƒæ•¸æ­£ç¢º"),
    ]

    print("ğŸ“‹ è©•ä¼°é…ç½®")
    print("-" * 80)
    print("é–¾å€¼: 0.7")
    print("è©•å¯©æ¨¡å‹: gemini-2.5-flash")
    print(f"è©•é‡è¡¨: {len(rubrics)}")

    for rubric_id, rubric_desc in rubrics:
        print(f"  â€¢ {rubric_id}: {rubric_desc[:55]}...")

    print("\nğŸ” æ­£åœ¨åŸ·è¡Œè©•ä¼°")
    print("-" * 80)

    try:
        # åŸ·è¡Œè©•ä¼°
        results = await AgentEvaluator.evaluate(
            agent_module="tool_use_evaluator",
            eval_dataset_file_path_or_dir=str(evalset_path),
        )

        print("âœ… è©•ä¼°æˆåŠŸå®Œæˆï¼")
        print("\nğŸ“Š è©•ä¼°çµæœ")
        print("-" * 80)
        print(json.dumps(results, indent=2, default=str, ensure_ascii=False))

        # è§£è®€çµæœ
        print("\nğŸ§  çµæœè§£è®€")
        print("-" * 80)
        print(
            """
            è©•ä¼°åˆ†æ•¸èªªæ˜ï¼š
            - åˆ†æ•¸ 1.0ï¼šå®Œç¾çš„å·¥å…·æ’åº (æ»¿è¶³æ‰€æœ‰è©•é‡è¡¨)
            - åˆ†æ•¸ 0.8-0.99ï¼šå„ªç§€ï¼Œ1-2 å€‹å°å•é¡Œ
            - åˆ†æ•¸ 0.7-0.79ï¼šè‰¯å¥½ï¼Œå¯æ¥å—ä½†éœ€è¦æ”¹é€²
            - åˆ†æ•¸ 0.6-0.69ï¼šå¯æ¥å—ä½†æœ‰é‡å¤§å•é¡Œ
            - åˆ†æ•¸ <0.6ï¼šå·®ï¼Œå·¥å…·æ’åºæœ‰æ ¹æœ¬æ€§å•é¡Œ

            æ¯å€‹è©•é‡è¡¨è©•ä¼°çš„å…§å®¹ï¼š
            1. proper_tool_orderï¼šæ˜¯å¦éµå®ˆä¾è³´é—œä¿‚ï¼Ÿ (åˆ†æåœ¨æå–ä¹‹å‰)
            2. complete_pipelineï¼šæ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦æ­¥é©Ÿï¼Ÿ
            3. validation_before_modelï¼šæ˜¯å¦åœ¨å»ºæ¨¡å‰é©—è­‰å“è³ªï¼Ÿ
            4. no_tool_failuresï¼šæ˜¯å¦æ‰€æœ‰å·¥å…·å‘¼å«éƒ½åŸ·è¡ŒæˆåŠŸï¼Ÿ
            """
        )

    except Exception as e:
        error_msg = str(e)
        if "Expected" in error_msg and "got" in error_msg:
            # é€™æ˜¯è©•åˆ†å¤±æ•—ï¼Œå¯¦éš›ä¸Šæ„å‘³è‘—è©•ä¼°å·¥ä½œæ­£å¸¸ï¼
            print("âš ï¸  è©•ä¼°å·²åŸ·è¡Œä½†æ¸¬è©¦æ¡ˆä¾‹æœªé”åˆ°è©•åˆ†é–¾å€¼ï¼š")
            print(f"   {error_msg}\n")
            print("é€™æ„å‘³è‘—è©•ä¼°æ¡†æ¶é‹ä½œæ­£å¸¸ï¼")
            print("æ¸¬è©¦ä»£ç†ä¸ç¬¦åˆé æœŸçš„å·¥å…·é †åºã€‚")
            print("åœ¨çœŸå¯¦å ´æ™¯ä¸­ï¼Œæ‚¨å°‡æœƒï¼š\n")
            print("1. æª¢è¦–ä¸Šè¿°é æœŸèˆ‡å¯¦éš›å·¥å…·å‘¼å«")
            print("2. èª¿æ•´ä»£ç†æŒ‡ä»¤ä»¥ç¬¦åˆé æœŸè¡Œç‚º")
            print("3. é‡æ–°åŸ·è¡Œè©•ä¼°ä»¥æŸ¥çœ‹åˆ†æ•¸æ˜¯å¦æé«˜")
        else:
            print(f"âŒ è©•ä¼°å¤±æ•—: {e}")
            print("\næ³¨æ„: ç¢ºä¿å·²è¨­å®š GOOGLE_API_KEYï¼š")
            print("  export GOOGLE_API_KEY=your_key")


def show_test_case_details():
    """é¡¯ç¤ºæœ‰é—œæ¸¬è©¦æ¡ˆä¾‹çš„è©³ç´°è³‡è¨Šã€‚"""
    print("\nğŸ“ æ¸¬è©¦æ¡ˆä¾‹æ‘˜è¦")
    print("-" * 80)

    test_cases = [
        {
            "name": "good_sequence_complete_pipeline",
            "description": "å®Œæ•´çš„ 4 æ­¥é©Ÿæµç¨‹ (åˆ†æ â†’ æå– â†’ é©—è­‰ â†’ æ‡‰ç”¨)",
            "expected_score": "0.95-1.0 (å„ªç§€)",
            "why": "åŒ…å«æ­£ç¢ºé †åºçš„æ‰€æœ‰æ­¥é©Ÿï¼Œæ»¿è¶³æ‰€æœ‰è©•é‡è¡¨",
        },
        {
            "name": "bad_sequence_skipped_validation",
            "description": "ç¼ºå°‘æ­¥é©Ÿ (æå– â†’ æ‡‰ç”¨ï¼Œç„¡åˆ†ææˆ–é©—è­‰)",
            "expected_score": "0.25-0.4 (å·®)",
            "why": "è·³éé—œéµæ­¥é©Ÿï¼Œé•å proper_tool_order å’Œ validation_before_model è©•é‡è¡¨",
        },
        {
            "name": "good_sequence_proper_analysis",
            "description": "è‰¯å¥½çš„åˆ†ææµç¨‹ (åˆ†æ â†’ æå– â†’ é©—è­‰)",
            "expected_score": "0.8-0.9 (å¥½)",
            "why": "é †åºæ­£ç¢ºä¸”åŒ…å«é‡è¦æ­¥é©Ÿï¼Œä½†æœªæ‡‰ç”¨æ¨¡å‹ (å°æ–¼åƒ…åˆ†æä»»å‹™å¯æ¥å—)",
        },
    ]

    for i, case in enumerate(test_cases, 1):
        print(f"\næ¸¬è©¦æ¡ˆä¾‹ {i}: {case['name']}")
        print(f"  æè¿°: {case['description']}")
        print(f"  é æœŸåˆ†æ•¸: {case['expected_score']}")
        print(f"  åŸå› : {case['why']}")


async def main():
    """ä¸»è©•ä¼°å·¥ä½œæµç¨‹ã€‚"""
    # æª¢æŸ¥ API é‡‘é‘°
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        print("âš ï¸  è­¦å‘Š: æœªè¨­å®š GOOGLE_API_KEY")
        print("    è‹¥è¦é€²è¡ŒçœŸå¯¦è©•ä¼°ï¼Œè«‹è¨­å®š: export GOOGLE_API_KEY=your_key")
        print("    ç¹¼çºŒåŸ·è¡Œå±•ç¤ºæ¨¡å¼...\n")

    # é¡¯ç¤ºæ¸¬è©¦æ¡ˆä¾‹è©³ç´°è³‡è¨Š
    show_test_case_details()

    # å»ºç«‹ evalset æª”æ¡ˆ
    print("\nğŸ“ æ­£åœ¨å»ºç«‹æ¸¬è©¦ evalset æª”æ¡ˆ...")
    evalset_path = await create_evalset_file()
    print(f"   âœ“ å·²å»ºç«‹: {evalset_path}")

    # åŸ·è¡Œè©•ä¼°
    await run_evaluation(evalset_path)

    print("\n" + "=" * 80)
    print("è©•ä¼°å®Œæˆ")
    print("=" * 80 + "\n")
    print("ğŸ’¡ é—œéµæ´å¯Ÿï¼š")
    print("""
        RUBRIC_BASED_TOOL_USE_QUALITY_V1 æŒ‡æ¨™é€šéè®“ LLM è©•å¯©
        æ ¹æ“šæ‚¨çš„è‡ªå®šç¾©è©•é‡è¡¨è©•ä¼°å·¥å…·å‘¼å«ï¼Œä¾†è©•ä¼°ä»£ç†çš„å·¥å…·æ’åºã€‚

        ä¸»è¦å„ªé»ï¼š
        â€¢ å„˜æ—©ç™¼ç¾å·¥å…·ä¾è³´é—œä¿‚é•è¦
        â€¢ ç¢ºä¿ä»£ç†éµå¾ªè¦å®šçš„å·¥ä½œæµç¨‹
        â€¢ åµæ¸¬éºæ¼æˆ–é‡æ–°æ’åºçš„æ­¥é©Ÿ
        â€¢ é‡å°æ‚¨çš„ç‰¹å®šéœ€æ±‚éˆæ´»å®šç¾©è©•é‡è¡¨

        ä¸‹ä¸€æ­¥ï¼š
        1. ç‚ºæ‚¨çš„ç‰¹å®šå·¥ä½œæµç¨‹å®šç¾©è©•é‡è¡¨
        2. å»ºç«‹åŒ…å«é æœŸå’Œå¯¦éš›å·¥å…·é †åºçš„æ¸¬è©¦æ¡ˆä¾‹
        3. åœ¨æ‚¨çš„ CI/CD ç®¡é“ä¸­åŸ·è¡Œè©•ä¼°
        4. ä½¿ç”¨çµæœè­˜åˆ¥ä»£ç†è¡Œç‚ºå•é¡Œ
        5. è¿­ä»£ä»£ç†æŒ‡ä»¤ä»¥æé«˜åˆ†æ•¸
    """
    )


if __name__ == "__main__":
    asyncio.run(main())
