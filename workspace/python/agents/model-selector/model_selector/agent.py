"""
æ•™å­¸ 22: æ¨¡å‹é¸æ“‡èˆ‡æœ€ä½³åŒ–
ä¸€å€‹ç”¨æ–¼é¸æ“‡ã€è©•æ¸¬åŠæ¯”è¼ƒ AI æ¨¡å‹çš„æ¡†æ¶ã€‚
"""


# ============================================================================
# é‡é»æ‘˜è¦
# ============================================================================
# - **æ ¸å¿ƒæ¦‚å¿µ**: æ­¤è…³æœ¬æä¾›ä¸€å€‹å®Œæ•´çš„æ¡†æ¶ï¼Œç”¨æ–¼å°å¤šå€‹ AI æ¨¡å‹é€²è¡ŒåŸºæº–æ¸¬è©¦ã€æ¯”è¼ƒå’Œé¸æ“‡ã€‚å®ƒèƒ½è©•ä¼°æ¨¡å‹çš„å»¶é²ã€Token ä½¿ç”¨é‡ã€æˆæœ¬å’Œå“è³ªï¼Œä¸¦æ ¹æ“šå…·é«”ä½¿ç”¨å ´æ™¯æä¾›æ¨è–¦ã€‚
# - **é—œéµæŠ€è¡“**:
#   - **éåŒæ­¥è™•ç† (Asyncio)**: ç”¨æ–¼ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æ¨¡å‹çš„æ¸¬è©¦ï¼Œæé«˜æ•ˆç‡ã€‚
#   - **è³‡æ–™é¡åˆ¥ (Dataclasses)**: ä½¿ç”¨ `ModelBenchmark` ä¾†çµæ§‹åŒ–åœ°å„²å­˜æ¯å€‹æ¨¡å‹çš„è©•æ¸¬çµæœã€‚
#   - **Google ADKæ•´åˆ**: è…³æœ¬è¢«è¨­è¨ˆç‚ºä¸€å€‹ ADK Agentï¼ŒåŒ…å«å¯ä¾› Agent å‘¼å«çš„å·¥å…·å‡½å¼ (`recommend_model_for_use_case`, `get_model_info`)ã€‚
#   - **ç›´æ¥æ¨¡å‹å‘¼å«**: é€é `google.genai.Client` ç›´æ¥èˆ‡æ¨¡å‹ API äº’å‹•ä»¥é€²è¡Œç²¾ç¢ºçš„æ•ˆèƒ½è©•ä¼°ã€‚
# - **é‡è¦çµè«–**:
#   - è…³æœ¬ä¸åƒ…åƒ…æ˜¯åŸ·è¡Œæ¸¬è©¦ï¼Œé‚„æœƒæ ¹æ“šçµæœï¼ˆé€Ÿåº¦ã€æˆæœ¬ã€å“è³ªï¼‰æä¾›æ˜ç¢ºçš„å»ºè­°ï¼Œå¹«åŠ©ä½¿ç”¨è€…åšå‡ºæ±ºç­–ã€‚
#   - æä¾›äº†åŸºæ–¼è¦å‰‡çš„æ¨è–¦ç³»çµ±ï¼Œå¯ä»¥å¿«é€Ÿç‚ºå¸¸è¦‹çš„ä½¿ç”¨å ´æ™¯ï¼ˆå¦‚å³æ™‚èªéŸ³ã€è¤‡é›œæ¨ç†ï¼‰åŒ¹é…æœ€ä½³æ¨¡å‹ã€‚
# - **è¡Œå‹•é …ç›®**:
#   - ä½¿ç”¨è€…éœ€è¦æä¾›è‡ªå·±çš„ `GOOGLE_API_KEY`ã€‚
#   - å¯ä»¥é€šéä¿®æ”¹ `models_to_test` å’Œ `test_queries` åˆ—è¡¨ä¾†è‡ªè¨‚è¦æ¯”è¼ƒçš„æ¨¡å‹å’Œæ¸¬è©¦æ¡ˆä¾‹ã€‚
#
# ============================================================================

import asyncio
import time
from dataclasses import dataclass
from typing import Dict, List, Any
from google.adk.agents import Agent
from google.adk.tools.tool_context import ToolContext
from google.genai import types


# ============================================================================
# è³‡æ–™çµæ§‹ (DATA STRUCTURES)
# ============================================================================

@dataclass
class ModelBenchmark:
    """æ¨¡å‹çš„åŸºæº–æ¸¬è©¦çµæœã€‚"""
    model: str                # æ¨¡å‹åç¨±
    avg_latency: float        # å¹³å‡å»¶é² (ç§’)
    avg_tokens: int           # å¹³å‡ token æ•¸
    quality_score: float      # å“è³ªåˆ†æ•¸
    cost_estimate: float      # æˆæœ¬ä¼°ç®—
    success_rate: float       # æˆåŠŸç‡


# ============================================================================
# æ¨¡å‹é¸æ“‡å™¨é¡åˆ¥ (MODEL SELECTOR CLASS)
# ============================================================================

class ModelSelector:
    """ç”¨æ–¼é¸æ“‡å’Œè©•æ¸¬æ¨¡å‹çš„æ¡†æ¶ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹é¸æ“‡å™¨ã€‚"""
        # ä½¿ç”¨å­—å…¸ä¾†å„²å­˜æ¯å€‹æ¨¡å‹çš„è©•æ¸¬çµæœ
        self.benchmarks: Dict[str, ModelBenchmark] = {}

    async def benchmark_model(
        self,
        model: str,
        test_queries: List[str],
        instruction: str
    ) -> ModelBenchmark:
        """
        å°æŒ‡å®šæ¨¡å‹ä½¿ç”¨æ¸¬è©¦æŸ¥è©¢é€²è¡ŒåŸºæº–æ¸¬è©¦ã€‚

        Args:
            model: è¦æ¸¬è©¦çš„æ¨¡å‹åç¨±
            test_queries: æ¸¬è©¦æŸ¥è©¢çš„åˆ—è¡¨
            instruction: çµ¦äºˆ Agent çš„æŒ‡ä»¤

        Returns:
            åŒ…å«çµæœçš„ ModelBenchmark ç‰©ä»¶
        """
        from google.genai import Client

        print(f"\n{'='*70}")
        print(f"é–‹å§‹è©•æ¸¬: {model}")
        print(f"{'='*70}\n")

        # ç‚ºäº†è©•æ¸¬ï¼Œç›´æ¥å»ºç«‹ä¸€å€‹å®¢æˆ¶ç«¯ä¾†å‘¼å«æ¨¡å‹ï¼Œé€™æ¯”ä½¿ç”¨ Runner æ›´ç°¡å–®
        client = Client()

        latencies = []      # å„²å­˜æ¯æ¬¡æŸ¥è©¢çš„å»¶é²
        token_counts = []   # å„²å­˜æ¯æ¬¡æŸ¥è©¢çš„ token æ•¸
        successes = 0       # è¨ˆç®—æˆåŠŸçš„æŸ¥è©¢æ¬¡æ•¸

        # éæ­·æ‰€æœ‰æ¸¬è©¦æŸ¥è©¢
        for query in test_queries:
            try:
                start = time.time() # è¨˜éŒ„é–‹å§‹æ™‚é–“

                # ç›´æ¥å‘¼å«æ¨¡å‹ä»¥é€²è¡Œè©•æ¸¬
                response = await client.aio.models.generate_content(
                    model=model,
                    contents=f"{instruction}\n\n{query}",
                    config=types.GenerateContentConfig(
                        temperature=0.5,
                        max_output_tokens=1024
                    )
                )

                latency = time.time() - start # è¨ˆç®—å»¶é²
                latencies.append(latency)

                # å¦‚æœå›æ‡‰ä¸­ç¹¼è³‡æ–™ä¸­æä¾›å¯¦éš› token æ•¸ï¼Œå‰‡ä½¿ç”¨å®ƒï¼Œå¦å‰‡é€²è¡Œä¼°ç®—
                text = response.text if hasattr(response, 'text') else ""
                if hasattr(response, "usage_metadata") and response.usage_metadata and "total_tokens" in response.usage_metadata:
                    token_count = response.usage_metadata["total_tokens"]
                else:
                    # ä¼°ç®— token æ•¸
                    token_count = len(text.split())
                token_counts.append(token_count)

                successes += 1

                print(f"âœ… æŸ¥è©¢æˆåŠŸ: {query[:50]}...")
                print(f"   å»¶é²: {latency:.2f}s, Tokens: ~{token_count}")

            except Exception as e:
                print(f"âŒ æŸ¥è©¢å¤±æ•—: {query[:50]}... - {e}")

        # è¨ˆç®—å„é …æŒ‡æ¨™
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
        success_rate = successes / len(test_queries)

        # ä¼°ç®—æˆæœ¬ (æˆªè‡³ 2025 å¹´çš„ç°¡åŒ–å®šåƒ¹)
        cost_per_1k_tokens = {
            'gemini-2.5-flash': 0.00008,
            'gemini-2.5-flash-lite': 0.00004,
            'gemini-2.5-pro': 0.0005,
            'gemini-2.0-flash': 0.0001,
            'gemini-2.0-flash-live': 0.00015,
        }

        # è¨ˆç®—æ¯æ¬¡æŸ¥è©¢çš„ä¼°è¨ˆæˆæœ¬
        cost_estimate = (avg_tokens / 1000) * cost_per_1k_tokens.get(model, cost_per_1k_tokens['gemini-2.5-flash'])

        # è¨ˆç®—å“è³ªåˆ†æ•¸ (åŸºæ–¼æˆåŠŸç‡å’Œå»¶é²)
        quality_score = success_rate * (1.0 / (1.0 + avg_latency))

        benchmark = ModelBenchmark(
            model=model,
            avg_latency=avg_latency,
            avg_tokens=int(avg_tokens),
            quality_score=quality_score,
            cost_estimate=cost_estimate,
            success_rate=success_rate
        )

        self.benchmarks[model] = benchmark

        print("\nğŸ“Š è©•æ¸¬çµæœ:")
        print(f"   å¹³å‡å»¶é²: {avg_latency:.2f}s")
        print(f"   å¹³å‡ Tokens: {avg_tokens:.0f}")
        print(f"   æˆåŠŸç‡: {success_rate*100:.1f}%")
        print(f"   ä¼°è¨ˆæˆæœ¬: ${cost_estimate:.6f} /æ¯æ¬¡æŸ¥è©¢")
        print(f"   å“è³ªåˆ†æ•¸: {quality_score:.3f}")

        return benchmark

    async def compare_models(
        self,
        models: List[str],
        test_queries: List[str],
        instruction: str
    ):
        """
        åœ¨ç›¸åŒçš„æŸ¥è©¢ä¸Šæ¯”è¼ƒå¤šå€‹æ¨¡å‹ã€‚

        Args:
            models: è¦æ¯”è¼ƒçš„æ¨¡å‹åˆ—è¡¨
            test_queries: æ¸¬è©¦æŸ¥è©¢
            instruction: Agent æŒ‡ä»¤
        """

        print(f"\n{'#'*70}")
        print("æ¨¡å‹æ¯”è¼ƒ")
        print(f"{'#'*70}\n")

        # é€ä¸€å°åˆ—è¡¨ä¸­çš„æ¨¡å‹é€²è¡Œè©•æ¸¬
        for model in models:
            await self.benchmark_model(model, test_queries, instruction)
            await asyncio.sleep(2)  # ç­‰å¾… 2 ç§’ï¼Œé¿å…é”åˆ°é€Ÿç‡é™åˆ¶

        # å°å‡ºæ¯”è¼ƒçµæœ
        self._print_comparison()

    def _print_comparison(self):
        """å°å‡ºæ¯”è¼ƒè¡¨æ ¼ã€‚"""

        print(f"\n\n{'='*70}")
        print("æ¯”è¼ƒç¸½çµ")
        print(f"{'='*70}\n")

        # è¡¨é ­
        print(f"{'æ¨¡å‹':<30} {'å»¶é²':>10} {'Tokens':>8} {'æˆæœ¬':>10} {'å“è³ª':>10}")
        print(f"{'-'*70}")

        # è¡¨æ ¼å…§å®¹
        for model, bench in self.benchmarks.items():
            print(f"{model:<30} {bench.avg_latency:>9.2f}s {bench.avg_tokens:>8} "
                  f"${bench.cost_estimate:>9.6f} {bench.quality_score:>10.3f}")

        print(f"\n{'='*70}")

        # æ¨è–¦å»ºè­°
        print("\nğŸ¯ æ¨è–¦å»ºè­°:\n")

        fastest = min(self.benchmarks.items(), key=lambda x: x[1].avg_latency)
        print(f"âš¡ æœ€å¿«æ¨¡å‹: {fastest[0]} ({fastest[1].avg_latency:.2f}s)")

        cheapest = min(self.benchmarks.items(), key=lambda x: x[1].cost_estimate)
        print(f"ğŸ’° æœ€ä¾¿å®œæ¨¡å‹: {cheapest[0]} (${cheapest[1].cost_estimate:.6f})")

        best_quality = max(self.benchmarks.items(), key=lambda x: x[1].quality_score)
        print(f"ğŸ† å“è³ªæœ€ä½³: {best_quality[0]} ({best_quality[1].quality_score:.3f})")


# ============================================================================
# å·¥å…·å‡½å¼ (TOOL FUNCTIONS) (ä¾› ADK agent ä½¿ç”¨)
# ============================================================================

def recommend_model_for_use_case(
    use_case: str,
    tool_context: ToolContext
) -> Dict[str, Any]:
    """
    æ ¹æ“šä½¿ç”¨å ´æ™¯æè¿°æ¨è–¦æ¨¡å‹ã€‚

    Args:
        use_case: ä½¿ç”¨å ´æ™¯æè¿° (ä¾‹å¦‚ï¼š"å³æ™‚èªéŸ³åŠ©ç†")
        tool_context: ADK å·¥å…·ä¸Šä¸‹æ–‡

    Returns:
        åŒ…å«ç‹€æ…‹ã€å ±å‘Šå’Œæ¨è–¦æ¨¡å‹çš„å­—å…¸
    """
    use_case_lower = use_case.lower()

    # åŸºæ–¼è¦å‰‡çš„æ¨è–¦ (Gemini 2.5 ç³»åˆ—)
    if 'real-time' in use_case_lower or 'voice' in use_case_lower or 'å³æ™‚' in use_case_lower or 'èªéŸ³' in use_case_lower:
        recommendation = 'gemini-2.0-flash-live'
        reason = 'æ”¯æ´å³æ™‚é›™å‘ä¸²æµ'

    elif 'complex' in use_case_lower or 'reasoning' in use_case_lower or 'stem' in use_case_lower or 'è¤‡é›œ' in use_case_lower or 'æ¨ç†' in use_case_lower:
        recommendation = 'gemini-2.5-pro'
        reason = 'æœ€é©åˆè™•ç†è¤‡é›œå•é¡Œå’Œé€²éšæ¨ç†'

    elif 'high-volume' in use_case_lower or 'simple' in use_case_lower or 'ultra-fast' in use_case_lower or 'é«˜æµé‡' in use_case_lower or 'ç°¡å–®' in use_case_lower:
        recommendation = 'gemini-2.5-flash-lite'
        reason = 'è™•ç†é«˜æµé‡ç°¡å–®ä»»å‹™æ™‚æœ€å¿«ä¸”æœ€ä¾¿å®œ'

    elif 'critical' in use_case_lower or 'important' in use_case_lower or 'é—œéµ' in use_case_lower or 'é‡è¦' in use_case_lower:
        recommendation = 'gemini-2.5-pro'
        reason = 'ç‚ºé—œéµæ¥­å‹™æ“ä½œæä¾›æœ€é«˜å“è³ª'

    elif 'extended context' in use_case_lower or 'large document' in use_case_lower or 'é•·æ–‡æœ¬' in use_case_lower or 'å¤§æ–‡ä»¶' in use_case_lower:
        recommendation = 'gemini-2.5-pro'
        reason = 'æ“æœ‰ 200 è¬ token çš„ä¸Šä¸‹æ–‡è¦–çª—ï¼Œé©åˆè™•ç†å¤§å‹æ–‡ä»¶'

    else:
        recommendation = 'gemini-2.5-flash'
        reason = 'åœ¨ä¸€èˆ¬ç”¨é€”ä¸Šå…·æœ‰æœ€ä½³çš„æ€§åƒ¹æ¯”'

    return {
        'status': 'success',
        'report': f'ç‚ºä½¿ç”¨å ´æ™¯ "{use_case}" æ¨è–¦æ¨¡å‹ {recommendation}',
        'model': recommendation,
        'reason': reason,
        'use_case': use_case
    }


def get_model_info(
    model_name: str,
    tool_context: ToolContext
) -> Dict[str, Any]:
    """
    ç²å–ç‰¹å®šæ¨¡å‹çš„è©³ç´°è³‡è¨Šã€‚

    Args:
        model_name: æ¨¡å‹åç¨±
        tool_context: ADK å·¥å…·ä¸Šä¸‹æ–‡

    Returns:
        åŒ…å«ç‹€æ…‹ã€å ±å‘Šå’Œæ¨¡å‹è³‡è¨Šçš„å­—å…¸
    """
    models_info = {
        'gemini-2.5-flash': {
            'context_window': '1M tokens',
            'features': ['å¤šæ¨¡æ…‹', 'å¿«é€Ÿ', 'é«˜æ•ˆ'],
            'best_for': 'é€šç”¨ç›®çš„ï¼Œæ¨è–¦ç”¨æ–¼å¤§å¤šæ•¸ä½¿ç”¨å ´æ™¯',
            'pricing': 'ä½',
            'speed': 'å¿«'
        },
        'gemini-2.5-flash-lite': {
            'context_window': '1M tokens',
            'features': ['è¶…å¿«é€Ÿ', 'ç°¡å–®ä»»å‹™', 'é«˜æµé‡'],
            'best_for': 'é«˜æµé‡çš„ç°¡å–®ä»»å‹™ï¼Œå¦‚å…§å®¹å¯©æ ¸',
            'pricing': 'éå¸¸ä½',
            'speed': 'è¶…å¿«'
        },
        'gemini-2.5-pro': {
            'context_window': '2M tokens',
            'features': ['é€²éšæ¨ç†', 'è¤‡é›œå•é¡Œ', 'é«˜å“è³ª'],
            'best_for': 'è¤‡é›œæ¨ç†ã€ç§‘å­¸å·¥ç¨‹ã€é—œéµæ¥­å‹™æ“ä½œ',
            'pricing': 'é«˜',
            'speed': 'ä¸­ç­‰'
        },
        'gemini-2.0-flash': {
            'context_window': '1M tokens',
            'features': ['å¤šæ¨¡æ…‹', 'å¹³è¡¡', 'èˆŠç‰ˆæ”¯æ´'],
            'best_for': 'éœ€è¦èˆŠç‰ˆç›¸å®¹æ€§çš„é€šç”¨ç›®çš„',
            'pricing': 'ä½',
            'speed': 'å¿«'
        },
        'gemini-2.0-flash-live': {
            'context_window': '1M tokens',
            'features': ['å³æ™‚', 'é›™å‘ä¸²æµ', 'èªéŸ³'],
            'best_for': 'å³æ™‚èªéŸ³æ‡‰ç”¨å’Œä¸²æµ',
            'pricing': 'ä¸­',
            'speed': 'å³æ™‚'
        }
    }

    if model_name not in models_info:
        return {
            'status': 'error',
            'report': f'åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°æ¨¡å‹ {model_name}',
            'error': 'æ‰¾ä¸åˆ°æ¨¡å‹'
        }

    info = models_info[model_name]
    return {
        'status': 'success',
        'report': f'é—œæ–¼ {model_name} çš„è³‡è¨Š',
        'model': model_name,
        'info': info
    }


# ============================================================================
# æ ¹ AGENT (ROOT AGENT) (ADK è¦å®š)
# ============================================================================

root_agent = Agent(
    name="model_selector_agent",
    model="gemini-2.5-flash",
    description="ç”¨æ–¼é¸æ“‡å’Œæ¯”è¼ƒ AI æ¨¡å‹çš„å°ˆå®¶ agent",
    instruction="""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI æ¨¡å‹é¸æ“‡é¡§å•ã€‚ä½ å¹«åŠ©ä½¿ç”¨è€…ï¼š
    1. ç‚ºä»–å€‘çš„ä½¿ç”¨å ´æ™¯é¸æ“‡åˆé©çš„æ¨¡å‹
    2. äº†è§£æ¨¡å‹çš„èƒ½åŠ›èˆ‡é™åˆ¶
    3. æœ€ä½³åŒ–æˆæœ¬èˆ‡æ•ˆèƒ½
    4. æ¯”è¼ƒä¸åŒçš„æ¨¡å‹

    åœ¨æ¨è–¦æ¨¡å‹æ™‚ï¼š
    - ä»”ç´°è€ƒæ…®ä½¿ç”¨å ´æ™¯çš„éœ€æ±‚
    - è§£é‡‹æ¨è–¦èƒŒå¾Œçš„åŸå› 
    - æåŠæ¬Šè¡¡ä¹‹è™• (æˆæœ¬ vs æ•ˆèƒ½ vs åŠŸèƒ½)
    - é©ç•¶æ™‚å»ºè­°æ›¿ä»£æ–¹æ¡ˆ

    å¯ç”¨æ¨¡å‹ (2025å¹´):
    - gemini-2.5-flash: æ¨è–¦ - é€šç”¨ç›®çš„çš„æœ€ä½³æ€§åƒ¹æ¯”
    - gemini-2.5-flash-lite: è™•ç†ç°¡å–®/é«˜æµé‡ä»»å‹™æœ€å¿«ä¸”æœ€ä¾¿å®œ
    - gemini-2.5-pro: è™•ç†è¤‡é›œæ¨ç†å’Œé—œéµä»»å‹™çš„æœ€é«˜å“è³ª
    - gemini-2.0-flash-live: ç”¨æ–¼èªéŸ³æ‡‰ç”¨çš„å³æ™‚é›™å‘ä¸²æµ
    - gemini-2.0-flash: å…·å‚™èˆŠç‰ˆç›¸å®¹æ€§çš„é€šç”¨æ¨¡å‹

    æ°¸é è¦å‹å–„ã€æ¸…æ™°ï¼Œä¸¦æä¾›å¯è¡Œçš„å»ºè­°ã€‚
    """.strip(),
    tools=[
        recommend_model_for_use_case,
        get_model_info
    ]
)


# ============================================================================
# ç¨ç«‹å±•ç¤ºå‡½å¼ (STANDALONE DEMO FUNCTION)
# ============================================================================

async def demo_model_comparison():
    """ç”¨æ–¼æ¯”è¼ƒæ¨¡å‹çš„ç¨ç«‹å±•ç¤ºå‡½å¼ã€‚"""
    selector = ModelSelector()

    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "æ³•åœ‹çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ",
        "ç”¨ç°¡å–®çš„è¡“èªè§£é‡‹é‡å­è¨ˆç®—",
        "å¯«ä¸€é¦–é—œæ–¼äººå·¥æ™ºæ…§çš„ä¿³å¥",
        "è¨ˆç®—ä¸€è¬ç¾å…ƒä»¥ 5% çš„å¹´åˆ©ç‡è¤‡åˆ©åå¹´å¾Œçš„æœ¬åˆ©å’Œ",
        "åˆ—å‡º 2025 å¹´æ’åå‰äº”çš„ç¨‹å¼èªè¨€"
    ]

    instruction = """
    ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©ç†ã€‚è«‹æº–ç¢ºã€ç°¡æ½”åœ°å›ç­”å•é¡Œã€‚
    """.strip()

    # æ¯”è¼ƒæ¨¡å‹ (ä½¿ç”¨ 2025 å¹´å¯ç”¨çš„æ¨¡å‹)
    models_to_test = [
        'gemini-2.5-flash',      # æ–°é è¨­ - æœ€ä½³æ€§åƒ¹æ¯”
        'gemini-2.0-flash',      # èˆŠç‰ˆä½†ä»å¯ç”¨
        'gemini-2.5-flash-lite', # è¶…å¿«é€Ÿè™•ç†ç°¡å–®ä»»å‹™
    ]

    await selector.compare_models(models_to_test, test_queries, instruction)

    # ä½¿ç”¨å ´æ™¯æ¨è–¦
    print(f"\n\n{'='*70}")
    print("ä½¿ç”¨å ´æ™¯æ¨è–¦")
    print(f"{'='*70}\n")

    use_cases = [
        "å³æ™‚èªéŸ³åŠ©ç†",
        "è¤‡é›œçš„ç­–ç•¥è¦åŠƒ",
        "é«˜æµé‡çš„å…§å®¹å¯©æ ¸",
        "é—œéµæ¥­å‹™æ±ºç­–æ”¯æ´",
        "ä¸€èˆ¬å®¢æˆ¶æœå‹™"
    ]

    for use_case in use_cases:
        # å‘¼å«å·¥å…·å‡½å¼ä»¥ç²å–æ¨è–¦
        result = recommend_model_for_use_case(use_case, None)
        print(f"ğŸ“Œ {use_case}")
        print(f"   â†’ æ¨è–¦æ¨¡å‹: {result['model']}")
        print(f"   â†’ åŸå› : {result['reason']}\n")


def compare_models_detailed():
    """
    æ¨¡å‹æ¯”è¼ƒçš„åŒæ­¥åŒ…è£å‡½å¼ã€‚
    è¿”å›ä¸€å€‹åŒ…å«æ¯”è¼ƒçµæœå’Œé—œéµç™¼ç¾çš„å­—å…¸ã€‚
    """
    import asyncio

    async def run_comparison():
        selector = ModelSelector()

        test_queries = [
            "æ³•åœ‹çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ",
            "ç”¨ç°¡å–®çš„è¡“èªè§£é‡‹é‡å­è¨ˆç®—",
            "å¯«ä¸€é¦–é—œæ–¼äººå·¥æ™ºæ…§çš„ä¿³å¥"
        ]

        instruction = "ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©ç†ã€‚è«‹æº–ç¢ºã€ç°¡æ½”åœ°å›ç­”å•é¡Œã€‚"

        models_to_test = [
            'gemini-2.5-flash',
            'gemini-2.0-flash',
            'gemini-2.5-flash-lite'
        ]

        await selector.compare_models(models_to_test, test_queries, instruction)

        # æ ¹æ“šè©•æ¸¬çµæœç”¢ç”Ÿé—œéµç™¼ç¾
        key_findings = []
        if selector.benchmarks:
            fastest = min(selector.benchmarks.items(), key=lambda x: x[1].avg_latency)
            slowest = max(selector.benchmarks.items(), key=lambda x: x[1].avg_latency)
            best_quality = max(selector.benchmarks.items(), key=lambda x: x[1].quality_score)
            cheapest = min(selector.benchmarks.items(), key=lambda x: x[1].cost_estimate)

            key_findings = [
                f"{fastest[0]} çš„é€Ÿåº¦æ¯” {slowest[0]} å¿« {slowest[1].avg_latency/fastest[1].avg_latency:.1f} å€",
                f"{best_quality[0]} æä¾›æœ€é«˜çš„å“è³ªåˆ†æ•¸ ({best_quality[1].quality_score:.3f})",
                f"{cheapest[0]} æ˜¯æœ€å…·æˆæœ¬æ•ˆç›Šçš„é¸æ“‡",
                "gemini-2.5-flash æä¾›æœ€ä½³çš„æ€§åƒ¹æ¯”å¹³è¡¡"
            ]

        return {
            'key_findings': key_findings,
            'benchmarks': {k: v.__dict__ for k, v in selector.benchmarks.items()}
        }

    return asyncio.run(run_comparison())


if __name__ == '__main__':
    # åŸ·è¡Œç¨ç«‹å±•ç¤º
    asyncio.run(demo_model_comparison())
