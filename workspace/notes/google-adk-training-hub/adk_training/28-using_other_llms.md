# æ•™å­¸ 28ï¼šé€é LiteLLM ä½¿ç”¨å…¶ä»–å¤§å‹èªè¨€æ¨¡å‹ (Tutorial 28: Using Other LLMs with LiteLLM)

**ç›®æ¨™**ï¼šé€é LiteLLM åœ¨æ‚¨çš„ ADK ä»£ç†ç¨‹å¼ä¸­ä½¿ç”¨ OpenAIã€Claudeã€Ollama åŠå…¶ä»–å¤§å‹èªè¨€æ¨¡å‹

**å…ˆæ±ºæ¢ä»¶**ï¼š

- æ•™å­¸ 01 (Hello World ä»£ç†ç¨‹å¼)
- æ•™å­¸ 22 (æ¨¡å‹é¸æ“‡èˆ‡è¨­å®š)
- å° API é‡‘é‘°èˆ‡ç’°å¢ƒè®Šæ•¸æœ‰åŸºæœ¬äº†è§£

**æ‚¨å°‡å­¸åˆ°**ï¼š

- åœ¨ ADK ä¸­ä½¿ç”¨ OpenAI æ¨¡å‹ (GPT-4o-mini)
- åœ¨ ADK ä¸­ä½¿ç”¨ Anthropic Claude æ¨¡å‹ (3.7 Sonnet)
- é€é Ollama åŸ·è¡Œæœ¬åœ°æ¨¡å‹ (Granite 4) ä»¥ä¿è­·éš±ç§
- å¤šä¾›æ‡‰å•†æ¯”è¼ƒèˆ‡æˆæœ¬å„ªåŒ–
- ä½•æ™‚ä¸è©²ä½¿ç”¨ LiteLLM
- è·¨ä¾›æ‡‰å•†é–‹ç™¼çš„æœ€ä½³å¯¦è¸

**ä¾†æº**ï¼š`google/adk/models/lite_llm.py`ã€
`contributing/samples/hello_world_litellm/`ã€
`contributing/samples/hello_world_ollama/`

---

## ç‚ºä»€éº¼è¦ä½¿ç”¨ LiteLLMï¼Ÿ (Why Use LiteLLM?)

**LiteLLM** è®“ ADK ä»£ç†ç¨‹å¼èƒ½å¤ ä»¥çµ±ä¸€çš„ä»‹é¢ä½¿ç”¨ **è¶…é 100 ç¨® LLM ä¾›æ‡‰å•†**ã€‚

**ä½•æ™‚ä½¿ç”¨ LiteLLM**ï¼š

- âœ… éœ€è¦ OpenAI æ¨¡å‹ (GPT-4o, GPT-4o-mini)
- âœ… æƒ³è¦ Anthropic Claude (3.7 Sonnet, Opus, Haiku)
- âœ… é€é Ollama åŸ·è¡Œæœ¬åœ°æ¨¡å‹ (éš±ç§ã€æˆæœ¬ã€é›¢ç·š)
- âœ… Azure OpenAI (ä¼æ¥­åˆç´„)
- âœ… å¤šä¾›æ‡‰å•†ç­–ç•¥ (å‚™æ´ã€æˆæœ¬å„ªåŒ–)
- âœ… æ¯”è¼ƒè·¨ä¾›æ‡‰å•†çš„æ¨¡å‹æ€§èƒ½

**ä½•æ™‚ä¸è©²ä½¿ç”¨ LiteLLM**ï¼š

- âŒ **ä½¿ç”¨ Gemini æ¨¡å‹** â†’ è«‹ä½¿ç”¨åŸç”Ÿçš„ `GoogleGenAI` (æ€§èƒ½æ›´ä½³ã€åŠŸèƒ½æ›´å®Œæ•´)
- âŒ åƒ…ä½¿ç”¨ Gemini çš„ç°¡å–®åŸå‹
- âŒ éœ€è¦ Gemini ç‰¹å®šåŠŸèƒ½æ™‚ (ä¾‹å¦‚ `thinking_config`, `grounding`)

---

## 1. OpenAI æ•´åˆ (OpenAI Integration)

**OpenAI çš„ GPT æ¨¡å‹** å› å…¶å¼·å¤§çš„æ¨ç†èˆ‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›è€Œè¢«å»£æ³›ä½¿ç”¨ã€‚

### è¨­å®š (Setup)

**1. å®‰è£ä¾è³´å¥—ä»¶**ï¼š

```bash
pip install google-adk[litellm]
# æˆ–æ‰‹å‹•å®‰è£ï¼š
pip install litellm openai
```

**2. å¾ [OpenAI Platform](https://platform.openai.com/api-keys) å–å¾— API é‡‘é‘°**

**3. è¨­å®šç’°å¢ƒè®Šæ•¸**ï¼š

```bash
export OPENAI_API_KEY='sk-...'
```

### ç¯„ä¾‹ï¼šGPT-4o ä»£ç†ç¨‹å¼ (Example: GPT-4o Agent)

```python
"""
ä½¿ç”¨ LiteLLM çš„ ADK ä»£ç†ç¨‹å¼ï¼Œæ­é… OpenAI GPT-4oã€‚
ä¾†æºï¼šcontributing/samples/hello_world_litellm/agent.py
"""
import asyncio
import os
from google.adk.agents import Agent
from google.adk.runners import InMemoryRunner
from google.adk.models import LiteLlm
from google.adk.tools import FunctionTool
from google.genai import types

# è¨­å®šç’°å¢ƒ
os.environ['OPENAI_API_KEY'] = 'sk-...'  # æ‚¨çš„ OpenAI API é‡‘é‘°


def calculate_square(number: int) -> int:
    """è¨ˆç®—ä¸€å€‹æ•¸å­—çš„å¹³æ–¹ã€‚"""
    return number ** 2


async def main():
    """ä½¿ç”¨ OpenAI GPT-4o çš„ä»£ç†ç¨‹å¼ã€‚"""

    # å»ºç«‹ LiteLLM æ¨¡å‹ - æ ¼å¼ï¼š"openai/æ¨¡å‹åç¨±"
    gpt4o_model = LiteLlm(model='openai/gpt-4o-mini')  # æˆ– 'openai/gpt-4o'

    # é€é OpenAI æ¨¡å‹å»ºç«‹ä»£ç†ç¨‹å¼
    agent = Agent(
        model=gpt4o_model,  # ä½¿ç”¨ LiteLlm å¯¦ä¾‹ï¼Œè€Œéå­—ä¸²
        name='gpt4o_agent',
        description='ç”± OpenAI GPT-4o é©…å‹•çš„ä»£ç†ç¨‹å¼',
        instruction='æ‚¨æ˜¯ä¸€ä½æ¨‚æ–¼åŠ©äººçš„åŠ©ç†ã€‚',
        tools=[FunctionTool(calculate_square)]
    )

    # å»ºç«‹ runner å’Œ session
    runner = InMemoryRunner(agent=agent, app_name='gpt4o_app')
    session = await runner.session_service.create_session(
        app_name='gpt4o_app',
        user_id='user_001'
    )

    # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
    query = "12 çš„å¹³æ–¹æ˜¯å¤šå°‘ï¼Ÿ"
    new_message = types.Content(
        role='user',
        parts=[types.Part(text=query)]
    )

    async for event in runner.run_async(
        user_id='user_001',
        session_id=session.id,
        new_message=new_message
    ):
        if event.content and event.content.parts:
            print(event.content.parts[0].text)


if __name__ == '__main__':
    asyncio.run(main())
```

**è¼¸å‡º**ï¼š

```
12 çš„å¹³æ–¹æ˜¯ 144ã€‚
```

### GPT-4o-mini (æˆæœ¬å„ªåŒ–) (Cost-Optimized)

å°æ–¼ç°¡å–®ä»»å‹™ï¼Œ**GPT-4o-mini** æ¯” GPT-4o **ä¾¿å®œ 60 å€**ã€‚

```python
from google.adk.models import LiteLlm

# GPT-4oï¼š$2.50/1M è¼¸å…¥ tokensï¼Œ$10/1M è¼¸å‡º tokens
gpt4o = LiteLlm(model='openai/gpt-4o')

# GPT-4o-miniï¼š$0.15/1M è¼¸å…¥ tokensï¼Œ$0.60/1M è¼¸å‡º tokens
gpt4o_mini = LiteLlm(model='openai/gpt-4o-mini')

# ä½¿ç”¨ mini è™•ç†æ—¥å¸¸ä»»å‹™
routine_agent = Agent(
    model=gpt4o_mini,
    instruction='æ‚¨èƒ½å¿«é€Ÿè™•ç†ç°¡å–®çš„æŸ¥è©¢ã€‚'
)

# ä½¿ç”¨å®Œæ•´çš„ GPT-4o è™•ç†è¤‡é›œæ¨ç†
complex_agent = Agent(
    model=gpt4o,
    instruction='æ‚¨èƒ½è§£æ±ºè¤‡é›œçš„å¤šæ­¥é©Ÿå•é¡Œã€‚'
)
```

### å¯ç”¨çš„ OpenAI æ¨¡å‹ (Available OpenAI Models)

| æ¨¡å‹ (Model)       | è¼¸å…¥æˆæœ¬ (Input Cost) | è¼¸å‡ºæˆæœ¬ (Output Cost) | æœ€é©ç”¨é€” (Best For)       |
| ------------------ | --------------------- | --------------------- | ------------------------- |
| `openai/gpt-4o`    | $2.50/1M tokens       | $10/1M tokens         | è¤‡é›œæ¨ç†ã€ç·¨ç¢¼ (Complex reasoning, coding) |
| `openai/gpt-4o-mini` | $0.15/1M tokens       | $0.60/1M tokens         | ç°¡å–®ä»»å‹™ã€é«˜æµé‡ (Simple tasks, high volume) |
| `openai/o1`        | $15/1M tokens         | $60/1M tokens         | é€²éšæ¨ç†éˆ (Advanced reasoning chains) |
| `openai/o1-mini`   | $3/1M tokens          | $12/1M tokens         | STEM æ¨ç† (STEM reasoning) |

**æ¨¡å‹å­—ä¸²æ ¼å¼**ï¼š`openai/[æ¨¡å‹åç¨±]`

---

## 2. Anthropic Claude æ•´åˆ (Anthropic Claude Integration)

**Anthropic çš„ Claude** åœ¨é•·ç¯‡å…§å®¹ã€åˆ†æä»¥åŠéµå¾ªè¤‡é›œæŒ‡ä»¤æ–¹é¢è¡¨ç¾å‡ºè‰²ã€‚

### Claude è¨­å®š (Claude Setup)

**1. å®‰è£ä¾è³´å¥—ä»¶**ï¼š

```bash
pip install google-adk[litellm] anthropic
```

**2. å¾ [Anthropic Console](https://console.anthropic.com/) å–å¾— API é‡‘é‘°**

**3. è¨­å®šç’°å¢ƒè®Šæ•¸**ï¼š

```bash
export ANTHROPIC_API_KEY='sk-ant-...'
```

### ç¯„ä¾‹ï¼šClaude 3.7 Sonnet ä»£ç†ç¨‹å¼ (Example: Claude 3.7 Sonnet Agent)

```python
"""
ä½¿ç”¨ LiteLLM çš„ ADK ä»£ç†ç¨‹å¼ï¼Œæ­é… Anthropic Claude 3.7 Sonnetã€‚
"""
import asyncio
import os
from google.adk.agents import Agent
from google.adk.runners import InMemoryRunner
from google.adk.models import LiteLlm
from google.adk.tools import FunctionTool
from google.genai import types

# è¨­å®šç’°å¢ƒ
os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'  # æ‚¨çš„ Anthropic API é‡‘é‘°


def analyze_sentiment(text: str) -> dict:
    """åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿï¼ˆæ¨¡æ“¬å¯¦ç¾ï¼‰ã€‚"""
    # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œæ‡‰ä½¿ç”¨å¯¦éš›çš„æƒ…æ„Ÿåˆ†æå·¥å…·
    return {
        'sentiment': 'positive',
        'confidence': 0.85,
        'key_phrases': ['exciting', 'innovative', 'breakthrough']
    }


async def main():
    """ä½¿ç”¨ Claude 3.7 Sonnet çš„ä»£ç†ç¨‹å¼ã€‚"""

    # å»ºç«‹ LiteLLM æ¨¡å‹ - æ ¼å¼ï¼š"anthropic/æ¨¡å‹åç¨±"
    claude_model = LiteLlm(model='anthropic/claude-3-7-sonnet-20250219')

    # é€é Claude æ¨¡å‹å»ºç«‹ä»£ç†ç¨‹å¼
    agent = Agent(
        model=claude_model,
        name='claude_agent',
        description='ç”± Claude 3.7 Sonnet é©…å‹•çš„ä»£ç†ç¨‹å¼',
        instruction="""
        æ‚¨æ˜¯ä¸€ä½æ·±æ€ç†Ÿæ…®çš„åˆ†æå¸«ï¼Œèƒ½æä¾›è©³ç´°ä¸”ç´°ç·»çš„å›ç­”ã€‚
        æ‚¨æ“…é•·ï¼š
        - è¤‡é›œæ¨ç†
        - é•·ç¯‡å…§å®¹
        - å€«ç†è€ƒé‡
        - éµå¾ªè©³ç´°æŒ‡ä»¤
        """.strip(),
        tools=[FunctionTool(analyze_sentiment)]
    )

    # å»ºç«‹ runner å’Œ session
    runner = InMemoryRunner(agent=agent, app_name='claude_app')
    session = await runner.session_service.create_session(
        app_name='claude_app',
        user_id='user_001'
    )

    query = """
    åˆ†ææ­¤ç”¢å“è©•è«–çš„æƒ…æ„Ÿä¸¦è§£é‡‹æ‚¨çš„æ¨ç†ï¼š
    "é€™æ¬¾æ–°çš„äººå·¥æ™ºæ…§åŠ©ç†çœŸæ˜¯å¤ªæ£’äº†ï¼å®ƒèƒ½éå¸¸å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ï¼Œ
    ä¸¦æä¾›æœ‰å¹«åŠ©ä¸”æº–ç¢ºçš„å›æ‡‰ã€‚ä»‹é¢ç›´è§€ï¼Œé€Ÿåº¦ä¹Ÿä»¤äººå°è±¡æ·±åˆ»ã€‚å¼·çƒˆæ¨è–¦ï¼"
    """.strip()

    # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
    new_message = types.Content(
        role='user',
        parts=[types.Part(text=query)]
    )

    async for event in runner.run_async(
        user_id='user_001',
        session_id=session.id,
        new_message=new_message
    ):
        if event.content and event.content.parts:
            print(event.content.parts[0].text)


if __name__ == '__main__':
    asyncio.run(main())
```

**è¼¸å‡º**ï¼š

```
æˆ‘å°‡åˆ†ææ­¤ç”¢å“è©•è«–çš„æƒ…æ„Ÿï¼š

**æ•´é«”æƒ…æ„Ÿ**ï¼šéå¸¸æ­£é¢

**åˆ†æ**ï¼š
æ­¤è©•è«–é€éå¤šå€‹æŒ‡æ¨™å±•ç¾å‡ºæ¥µç‚ºæ­£é¢çš„æƒ…æ„Ÿï¼š

1. **æœ€é«˜ç´šå½¢å®¹è©**ï¼šã€ŒçœŸæ˜¯å¤ªæ£’äº†ã€ã€ã€Œéå¸¸å¥½åœ°ã€ã€ã€Œå¼·çƒˆæ¨è–¦ã€â€”â€”é€™äº›éƒ½æ˜¯å¼·èª¿æ€§çš„æ­£é¢æè¿°è©ã€‚

2. **å…·é«”è®šæš**ï¼šè©•è«–è€…å¼·èª¿äº†å¤šå€‹å„ªé»ï¼š
   - ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
   - æœ‰å¹«åŠ©ä¸”æº–ç¢ºçš„å›æ‡‰
   - ç›´è§€çš„ä»‹é¢
   - ä»¤äººå°è±¡æ·±åˆ»çš„é€Ÿåº¦

3. **é©šå˜†è™Ÿ**ï¼šå…©å€‹é©šå˜†è™Ÿ (!!) è¡¨é”äº†ç†±æƒ…ã€‚

4. **æ¨è–¦**ï¼šæ˜ç¢ºçš„èƒŒæ›¸ï¼ˆã€Œå¼·çƒˆæ¨è–¦ã€ï¼‰é¡¯ç¤ºäº†é«˜åº¦çš„æ»¿æ„åº¦ã€‚

5. **ç„¡æ‰¹è©•**ï¼šå®Œå…¨æ²’æœ‰è² é¢è©•è«–æˆ–è­¦å‘Šã€‚

**ä¿¡è³´åº¦**ï¼š95% - èªè¨€æ¸…æ™°æ˜ç¢ºï¼Œä¸”å§‹çµ‚ä¿æŒæ­£é¢ã€‚

**ä¸»è¦æƒ…æ„ŸåŸºèª¿**ï¼šç†±æƒ…çš„è®šè³èˆ‡æ»¿æ„ã€‚
```

### å¯ç”¨çš„ Claude æ¨¡å‹ (Available Claude Models)

| æ¨¡å‹ (Model)                                 | è¼¸å…¥æˆæœ¬ (Input Cost) | è¼¸å‡ºæˆæœ¬ (Output Cost) | ä¸Šä¸‹æ–‡ (Context) | æœ€é©ç”¨é€” (Best For)         |
| -------------------------------------------- | --------------------- | --------------------- | ---------------- | --------------------------- |
| `anthropic/claude-3-7-sonnet-20250219`         | $3/1M tokens          | $15/1M tokens         | 200K             | å¹³è¡¡å‹ (æœ€å—æ­¡è¿) (Balanced (most popular)) |
| `anthropic/claude-3-5-opus-20240229`           | $15/1M tokens         | $75/1M tokens         | 200K             | è¤‡é›œæ¨ç† (Complex reasoning) |
| `anthropic/claude-3-5-haiku-20241022`          | $0.80/1M tokens       | $4/1M tokens          | 200K             | å¿«é€Ÿã€ç°¡å–®ä»»å‹™ (Fast, simple tasks) |

**æ¨¡å‹å­—ä¸²æ ¼å¼**ï¼š`anthropic/[æ¨¡å‹åç¨±-å«æ—¥æœŸ]`

**æ³¨æ„**ï¼šClaude 3.7 Sonnet æ˜¯**é è¨­æ¨è–¦çš„æ¨¡å‹**ï¼ˆæˆªè‡³ 2025 å¹´ç¬¬ä¸€å­£ï¼‰ã€‚

---

## 3. Ollama æœ¬åœ°æ¨¡å‹ (Ollama Local Models)

**Ollama** è®“æ‚¨å¯ä»¥åœ¨**æœ¬åœ°**åŸ·è¡Œ LLMï¼Œä»¥ä¿è­·éš±ç§ã€ç¯€çœæˆæœ¬ä¸¦é€²è¡Œé›¢ç·šæ“ä½œã€‚

### ç‚ºä½•ä½¿ç”¨ Ollamaï¼Ÿ (Why Use Ollama?)

**å„ªé»**ï¼š

- âœ… **éš±ç§**ï¼šè³‡æ–™æ°¸é ä¸æœƒé›¢é–‹æ‚¨çš„æ©Ÿå™¨
- âœ… **æˆæœ¬**ï¼šåˆæ¬¡ä¸‹è¼‰å¾Œç„¡ API è²»ç”¨
- âœ… **é›¢ç·š**ï¼šç„¡éœ€ç¶²è·¯å³å¯é‹ä½œ
- âœ… **åˆè¦æ€§**ï¼šå°‡æ•æ„Ÿè³‡æ–™ä¿ç•™åœ¨æœ¬åœ°
- âœ… **å¯¦é©—**ï¼šè‡ªç”±å˜—è©¦å¤šç¨®æ¨¡å‹

**æ¬Šè¡¡**ï¼š

- âŒ éœ€è¦ GPU æ‰èƒ½ç²å¾—è‰¯å¥½æ€§èƒ½
- âŒ åœ¨è¤‡é›œä»»å‹™ä¸Šï¼Œå“è³ªä½æ–¼ GPT-4o/Claude/Gemini
- âŒ åœ¨ CPU ä¸Šæ¨è«–é€Ÿåº¦è¼ƒæ…¢
- âŒ æœ‰é™çš„ä¸Šä¸‹æ–‡è¦–çª—ï¼ˆé€šå¸¸ç‚º 4K-32Kï¼Œè€Œé›²ç«¯æ¨¡å‹ç‚º 200Kï¼‰

### Ollama è¨­å®š (Ollama Setup)

**1. å®‰è£ Ollama**ï¼š

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# å¾ https://ollama.com/download ä¸‹è¼‰
```

**2. å•Ÿå‹• Ollama ä¼ºæœå™¨**ï¼š

```bash
ollama serve
# é è¨­åœ¨ http://localhost:11434 åŸ·è¡Œ
```

**3. ä¸‹è¼‰æ¨¡å‹**ï¼š

```bash
# Granite 4 (IBMï¼Œå¼·æ¨ç†èƒ½åŠ›ï¼Œ8B åƒæ•¸)
ollama pull granite4:latest

# Llama 3.3 (Metaï¼Œé«˜å“è³ªï¼Œ70B åƒæ•¸)
ollama pull llama3.3

# Mistral (7B åƒæ•¸ï¼Œé€Ÿåº¦å¿«)
ollama pull mistral

# Phi-4 (14B åƒæ•¸ï¼ŒMicrosoftï¼Œæ“…é•·ç·¨ç¢¼)
ollama pull phi4
```

**4. å®‰è£ Python ä¾è³´å¥—ä»¶**ï¼š

```bash
pip install google-adk[litellm]
```

### âš ï¸ é—œéµï¼šä½¿ç”¨ `ollama_chat`ï¼Œè€Œé `ollama` (CRITICAL: Use `ollama_chat`, NOT `ollama`)

**éŒ¯èª¤** âŒï¼š

```python
# é€™å°‡ç„¡æ³•æ­£å¸¸é‹ä½œï¼
model = LiteLlm(model='ollama/llama3.3')  # âŒ éŒ¯èª¤
```

**æ­£ç¢º** âœ…ï¼š

```python
# å‹™å¿…ä½¿ç”¨ ollama_chat å‰ç¶´ï¼
model = LiteLlm(model='ollama_chat/llama3.3')  # âœ… æ­£ç¢º
```

**ç‚ºä»€éº¼ï¼Ÿ** LiteLLM æœ‰å…©ç¨® Ollama ä»‹é¢ï¼š

- `ollama/` - ä½¿ç”¨ completion API (èˆŠç‰ˆï¼ŒåŠŸèƒ½æœ‰é™)
- `ollama_chat/` - ä½¿ç”¨ chat API (æ¨è–¦ï¼ŒåŠŸèƒ½å®Œæ•´)

ADK ä»£ç†ç¨‹å¼éœ€è¦**èŠå¤© API** æ‰èƒ½æ­£å¸¸é€²è¡Œå‡½å¼å‘¼å«èˆ‡å¤šè¼ªå°è©±ã€‚

### ç¯„ä¾‹ï¼šGranite 4 æœ¬åœ°ä»£ç†ç¨‹å¼ (Example: Granite 4 Local Agent)

```python
"""
ä½¿ç”¨æœ¬åœ° Granite 4 é€é Ollama çš„ ADK ä»£ç†ç¨‹å¼ã€‚
ä¾†æºï¼štutorial_implementation/tutorial28/multi_llm_agent/agent.py
"""
import asyncio
import os
from google.adk.agents import Agent
from google.adk.runners import InMemoryRunner
from google.adk.models import LiteLlm
from google.adk.tools import FunctionTool
from google.genai import types

# Ollama ç’°å¢ƒè¨­å®š
os.environ['OLLAMA_API_BASE'] = 'http://localhost:11434'


def get_weather(city: str) -> dict:
    """å–å¾—æŒ‡å®šåŸå¸‚ç›®å‰çš„å¤©æ°£ï¼ˆæ¨¡æ“¬ï¼‰ã€‚"""
    # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œæ‡‰å‘¼å«çœŸå¯¦çš„å¤©æ°£ API
    return {
        'city': city,
        'temperature': 72,
        'condition': 'Sunny',
        'humidity': 45
    }


async def main():
    """ä½¿ç”¨æœ¬åœ° Granite 4 æ¨¡å‹çš„ä»£ç†ç¨‹å¼ã€‚"""

    # å»ºç«‹ LiteLLM æ¨¡å‹ - æ ¼å¼ï¼š"ollama_chat/æ¨¡å‹åç¨±"
    # âš ï¸ é‡è¦ï¼šä½¿ç”¨ ollama_chatï¼Œè€Œé ollamaï¼
    granite_model = LiteLlm(model='ollama_chat/granite4:latest')

    # ä½¿ç”¨æœ¬åœ°æ¨¡å‹å»ºç«‹ä»£ç†ç¨‹å¼
    agent = Agent(
        model=granite_model,
        name='local_agent',
        description='åœ¨æœ¬åœ°åŸ·è¡Œçš„ Granite 4 ä»£ç†ç¨‹å¼',
        instruction='æ‚¨æ˜¯ç”± IBM Granite 4 é©…å‹•çš„æœ¬åœ°åŠ©ç†ã€‚æ‰€æœ‰è™•ç†éƒ½åœ¨è£ç½®ä¸Šé€²è¡Œã€‚',
        tools=[FunctionTool(get_weather)]
    )

    # å»ºç«‹ runner å’Œ session
    runner = InMemoryRunner(agent=agent, app_name='ollama_app')
    session = await runner.session_service.create_session(
        app_name='ollama_app',
        user_id='user_001'
    )

    print("\n" + "="*60)
    print("æœ¬åœ° OLLAMA ä»£ç†ç¨‹å¼ (éš±ç§å„ªå…ˆ)")
    print("="*60 + "\n")

    # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
    query = "èˆŠé‡‘å±±ç¾åœ¨å¤©æ°£å¦‚ä½•ï¼Ÿ"
    new_message = types.Content(
        role='user',
        parts=[types.Part(text=query)]
    )

    async for event in runner.run_async(
        user_id='user_001',
        session_id=session.id,
        new_message=new_message
    ):
        if event.content and event.content.parts:
            print(event.content.parts[0].text)

    print("\n" + "="*60 + "\n")


if __name__ == '__main__':
    asyncio.run(main())
```

**è¼¸å‡º**ï¼š

```
============================================================
æœ¬åœ° OLLAMA ä»£ç†ç¨‹å¼ (éš±ç§å„ªå…ˆ)
============================================================

èˆŠé‡‘å±±ç›®å‰å¤©æ°£æ™´æœ—ï¼Œæº«åº¦ç‚º
72Â°Fï¼Œæ¿•åº¦ç‚º 45%ã€‚çœŸæ˜¯ç¾å¥½çš„ä¸€å¤©ï¼

[æ‰€æœ‰è™•ç†éƒ½åœ¨æœ¬åœ°å®Œæˆ - ç„¡è³‡æ–™å‚³é€è‡³é›²ç«¯]

============================================================
```

### ç†±é–€çš„ Ollama æ¨¡å‹ (Popular Ollama Models)

| æ¨¡å‹ (Model)                | å¤§å° (Size) | æœ€é©ç”¨é€” (Best For)             | GPU RAM |
| ----------------------------- | ----------- | ------------------------------- | ------- |
| `ollama_chat/granite4:latest` | 8B          | IBM Graniteï¼Œå¼·æ¨ç†èƒ½åŠ› (IBM Granite, strong reasoning) | 12GB    |
| `ollama_chat/llama3.3`        | 70B         | ä¸€èˆ¬ä»»å‹™ï¼Œå¼·æ¨ç†èƒ½åŠ› (General tasks, strong reasoning) | 40GB+   |
| `ollama_chat/llama3.2`        | 3B          | å¿«é€Ÿï¼Œä½è³‡æºéœ€æ±‚ (Fast, low resource) | 4GB     |
| `ollama_chat/mistral`         | 7B          | é€Ÿåº¦/å“è³ªå¹³è¡¡ (Balanced speed/quality) | 8GB     |
| `ollama_chat/phi4`            | 14B         | ç·¨ç¢¼ï¼ŒSTEM (Coding, STEM)       | 16GB    |
| `ollama_chat/gemma2`          | 9B          | Googleï¼ŒæŒ‡ä»¤éµå¾ª (Google, instruction following) | 12GB    |
| `ollama_chat/qwen2.5`         | 7B-72B      | å¤šèªè¨€ (Multilingual)         | 8-40GB  |

**æ¨¡å‹å­—ä¸²æ ¼å¼**ï¼š`ollama_chat/[æ¨¡å‹åç¨±]` âš ï¸ ä¸æ˜¯ `ollama/`ï¼

### è¨­å®šé¸é … (Configuration Options)

```python
from google.adk.models import LiteLlm

# åŸºæœ¬ç”¨æ³•
model = LiteLlm(model='ollama_chat/llama3.3')

# ä½¿ç”¨è‡ªè¨‚ Ollama ä¼ºæœå™¨
os.environ['OLLAMA_API_BASE'] = 'http://192.168.1.100:11434'
model = LiteLlm(model='ollama_chat/llama3.3')

# ä½¿ç”¨é¡å¤–åƒæ•¸ï¼ˆå‚³éçµ¦ Ollamaï¼‰
model = LiteLlm(
    model='ollama_chat/llama3.3',
    temperature=0.7,
    top_p=0.9,
    max_tokens=2048
)
```

---

## 4. Azure OpenAI æ•´åˆ (Azure OpenAI Integration)

**Azure OpenAI** é©ç”¨æ–¼æœ‰ **Azure åˆç´„**æˆ–**åˆè¦æ€§è¦æ±‚**çš„ä¼æ¥­ã€‚

### Azure è¨­å®š (Azure Setup)

**1. åœ¨ Azure Portal å»ºç«‹ Azure OpenAI è³‡æº**

**2. éƒ¨ç½²æ¨¡å‹**ï¼ˆä¾‹å¦‚ gpt-4oï¼‰

**3. å–å¾—æ†‘è­‰**ï¼š

- ä¾†è‡ª Azure Portal çš„ API é‡‘é‘°
- ç«¯é» URLï¼ˆä¾‹å¦‚ `https://your-resource.openai.azure.com/`ï¼‰
- éƒ¨ç½²åç¨±ï¼ˆä¾‹å¦‚ `gpt-4o-deployment`ï¼‰

**4. è¨­å®šç’°å¢ƒè®Šæ•¸**ï¼š

```bash
export AZURE_API_KEY='your-azure-key'
export AZURE_API_BASE='https://your-resource.openai.azure.com/'
export AZURE_API_VERSION='2024-02-15-preview'
```

### ç¯„ä¾‹ï¼šAzure OpenAI ä»£ç†ç¨‹å¼ (Example: Azure OpenAI Agent)

```python
"""
ä½¿ç”¨ Azure OpenAI çš„ ADK ä»£ç†ç¨‹å¼ã€‚
"""
import asyncio
import os
from google.adk.agents import Agent
from google.adk.runners import InMemoryRunner
from google.adk.models import LiteLlm
from google.genai import types

# Azure OpenAI è¨­å®š
os.environ['AZURE_API_KEY'] = 'your-azure-key'
os.environ['AZURE_API_BASE'] = 'https://your-resource.openai.azure.com/'
os.environ['AZURE_API_VERSION'] = '2024-02-15-preview'


async def main():
    """ä½¿ç”¨ Azure OpenAI çš„ä»£ç†ç¨‹å¼ã€‚"""

    # å»ºç«‹ LiteLLM æ¨¡å‹ - æ ¼å¼ï¼š"azure/éƒ¨ç½²åç¨±"
    azure_model = LiteLlm(model='azure/gpt-4o-deployment')

    # å»ºç«‹ä»£ç†ç¨‹å¼
    agent = Agent(
        model=azure_model,
        name='azure_agent',
        description='ä½¿ç”¨ Azure OpenAI çš„ä»£ç†ç¨‹å¼',
        instruction='æ‚¨æ˜¯åœ¨ Azure ä¸Šé‹è¡Œçš„ä¼æ¥­åŠ©ç†ã€‚'
    )

    # å»ºç«‹ runner å’Œ session
    runner = InMemoryRunner(agent=agent, app_name='azure_app')
    session = await runner.session_service.create_session(
        app_name='azure_app',
        user_id='user_001'
    )

    # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
    query = "è§£é‡‹ Azure OpenAI å°ä¼æ¥­çš„å¥½è™•"
    new_message = types.Content(
        role='user',
        parts=[types.Part(text=query)]
    )

    async for event in runner.run_async(
        user_id='user_001',
        session_id=session.id,
        new_message=new_message
    ):
        if event.content and event.content.parts:
            print(event.content.parts[0].text)


if __name__ == '__main__':
    asyncio.run(main())
```

**ç‚ºä½•é¸æ“‡ Azure OpenAIï¼Ÿ**

- âœ… ä¼æ¥­ç´š SLA (99.9% æ­£å¸¸é‹è¡Œæ™‚é–“)
- âœ… è³‡æ–™è½åœ° (æ­ç›Ÿã€ç¾åœ‹ã€äºæ´²)
- âœ… ç§æœ‰ç¶²è·¯ (VNet æ•´åˆ)
- âœ… åˆè¦æ€§ (SOC 2, HIPAA, GDPR)
- âœ… èˆ‡ Azure æœå‹™çµ±ä¸€è¨ˆè²»

---

## 5. é€é Vertex AI ä½¿ç”¨ Claude (Claude via Vertex AI)

**Vertex AI ä¸Šçš„ Claude** çµåˆäº† Anthropic çš„æ¨¡å‹èˆ‡ Google Cloud çš„åŸºç¤è¨­æ–½ã€‚

### Vertex AI è¨­å®š (Vertex AI Setup)

**1. åœ¨ Google Cloud Console ä¸­å•Ÿç”¨ Vertex AI API**

**2. è¨­å®šèº«ä»½é©—è­‰**ï¼š

```bash
export GOOGLE_CLOUD_PROJECT='your-project'
export GOOGLE_CLOUD_LOCATION='us-central1'  # æˆ–æ‚¨åå¥½çš„å€åŸŸ
export GOOGLE_APPLICATION_CREDENTIALS='/path/to/service-account-key.json'
```

**3. ç¢ºä¿ Vertex AI Claude å­˜å–æ¬Šé™**ï¼ˆå¯èƒ½éœ€è¦å¯©æ‰¹ï¼‰

### ç¯„ä¾‹ï¼šé€é Vertex AI ä½¿ç”¨ Claude (Example: Claude via Vertex AI)

```python
"""
ä½¿ç”¨ Vertex AI ä¸Šçš„ Claude 3.7 Sonnet çš„ ADK ä»£ç†ç¨‹å¼ã€‚
"""
import asyncio
import os
from google.adk.agents import Agent
from google.adk.runners import InMemoryRunner
from google.adk.models import LiteLlm
from google.genai import types

# Vertex AI è¨­å®š
os.environ['GOOGLE_CLOUD_PROJECT'] = 'your-project'
os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'


async def main():
    """é€é Vertex AI ä½¿ç”¨ Claude çš„ä»£ç†ç¨‹å¼ã€‚"""

    # å»ºç«‹ LiteLLM æ¨¡å‹ - æ ¼å¼ï¼š"vertex_ai/æ¨¡å‹åç¨±"
    claude_vertex = LiteLlm(model='vertex_ai/claude-3-7-sonnet@20250219')

    # å»ºç«‹ä»£ç†ç¨‹å¼
    agent = Agent(
        model=claude_vertex,
        name='claude_vertex_agent',
        description='åœ¨ Vertex AI ä¸Šä½¿ç”¨ Claude çš„ä»£ç†ç¨‹å¼',
        instruction='æ‚¨é€é Google Cloud åŸºç¤è¨­æ–½åˆ©ç”¨ Claudeã€‚'
    )

    # å»ºç«‹ runner å’Œ session
    runner = InMemoryRunner(agent=agent, app_name='vertex_claude_app')
    session = await runner.session_service.create_session(
        app_name='vertex_claude_app',
        user_id='user_001'
    )

    # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
    query = "æ¯”è¼ƒç›´æ¥ä½¿ç”¨ Claude èˆ‡åœ¨ Vertex AI ä¸Šä½¿ç”¨ Claude"
    new_message = types.Content(
        role='user',
        parts=[types.Part(text=query)]
    )

    async for event in runner.run_async(
        user_id='user_001',
        session_id=session.id,
        new_message=new_message
    ):
        if event.content and event.content.parts:
            print(event.content.parts[0].text)


if __name__ == '__main__':
    asyncio.run(main())
```

**ç›´æ¥ä½¿ç”¨ Claude vs. Vertex AI**ï¼š

| å› ç´  (Factor)      | ç›´æ¥ (Anthropic) | é€é Vertex AI (Via Vertex AI) |
| ------------------ | ---------------- | ---------------------------- |
| **å®šåƒ¹ (Pricing)** | æŒ‰ token è¨ˆè²»    | ç›¸åŒæˆ–ç•¥é«˜ (Same or slightly higher) |
| **è³‡æ–™è½åœ° (Data residency)** | ç¾åœ‹             | å¯é¸æ“‡ GCP å€åŸŸ (Choose GCP region) |
| **SLA**            | æ¨™æº– (Standard)  | Google Cloud SLA             |
| **æ•´åˆ (Integration)** | Anthropic API    | èˆ‡ GCP çµ±ä¸€ (Unified with GCP) |
| **è¨ˆè²» (Billing)** | ç¨ç«‹ (Separate)  | çµ±ä¸€ GCP è¨ˆè²» (Unified GCP billing) |
| **è¨­å®š (Setup)**   | è¼ƒç°¡å–® (Simpler) | è¼ƒè¤‡é›œ (More complex)        |

**ä½•æ™‚ä½¿ç”¨ Vertex AI Claude**ï¼š

- âœ… å·²å¤§é‡ä½¿ç”¨ Google Cloud
- âœ… éœ€è¦åœ¨ç‰¹å®š GCP å€åŸŸé€²è¡Œè³‡æ–™è½åœ°
- âœ… å¸Œæœ›çµ±ä¸€ GCP è¨ˆè²»
- âœ… éœ€è¦ Google Cloud SLA

---

## 6. å¤šä¾›æ‡‰å•†æ¯”è¼ƒ (Multi-Provider Comparison)

**ä½¿ç”¨æƒ…å¢ƒ**ï¼šæ¯”è¼ƒå¤šå€‹ä¾›æ‡‰å•†å°åŒä¸€æŸ¥è©¢çš„å›æ‡‰å“è³ªã€‚

```python
"""
å¤šä¾›æ‡‰å•†ä»£ç†ç¨‹å¼æ¯”è¼ƒã€‚
åœ¨ Geminiã€GPT-4oã€Claude å’Œ Llama 3.3 ä¸Šæ¸¬è©¦ç›¸åŒçš„æŸ¥è©¢ã€‚
"""
import asyncio
import os
from google.adk.agents import Agent, Runner
from google.adk.models import GoogleGenAI, LiteLlm

# ç’°å¢ƒè¨­å®š
os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = '1'
os.environ['GOOGLE_CLOUD_PROJECT'] = 'your-project'
os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'
os.environ['OPENAI_API_KEY'] = 'sk-...'
os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'
os.environ['OLLAMA_API_BASE'] = 'http://localhost:11434'


async def compare_models():
    """æ¯”è¼ƒ 4 å€‹ä¾›æ‡‰å•†çš„å›æ‡‰å“è³ªã€‚"""

    # å®šç¾©æ¨¡å‹
    models = {
        'Gemini 2.5 Flash': GoogleGenAI(model='gemini-2.5-flash'),
        'GPT-4o': LiteLlm(model='openai/gpt-4o'),
        'Claude 3.7 Sonnet': LiteLlm(model='anthropic/claude-3-7-sonnet-20250219'),
        'Llama 3.3 (Local)': LiteLlm(model='ollama_chat/llama3.3')
    }

    # æ¸¬è©¦æŸ¥è©¢
    query = """
    å‘ä¸€ä½ 12 æ­²çš„å­©å­è§£é‡‹é‡å­ç³¾çºã€‚
    ä½¿ç”¨ä»–å€‘èƒ½ç†è§£çš„æ¯”å–»ã€‚
    """.strip()

    print("\n" + "="*70)
    print("å¤šä¾›æ‡‰å•†æ¨¡å‹æ¯”è¼ƒ")
    print("="*70 + "\n")
    print(f"æŸ¥è©¢: {query}\n")
    print("="*70 + "\n")

    # æ¸¬è©¦æ¯å€‹æ¨¡å‹
    for model_name, model in models.items():
        print(f"### {model_name}")
        print("-" * 70)

        agent = Agent(
            model=model,
            instruction='æ‚¨èƒ½æ¸…æ™°ç°¡å–®åœ°è§£é‡‹è¤‡é›œçš„ä¸»é¡Œã€‚'
        )

        # ç‚ºæ­¤æ¨¡å‹å»ºç«‹ runner å’Œ session
        runner = InMemoryRunner(agent=agent, app_name='compare_app')
        session = await runner.session_service.create_session(
            app_name='compare_app',
            user_id='user_001'
        )

        try:
            # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
            new_message = types.Content(
                role='user',
                parts=[types.Part(text=query)]
            )

            response = ""
            async for event in runner.run_async(
                user_id='user_001',
                session_id=session.id,
                new_message=new_message
            ):
                if event.content and event.content.parts:
                    response = event.content.parts[0].text

            print(response)
            print(f"\n[é•·åº¦: {len(response)} å­—å…ƒ]")

        except Exception as e:
            print(f"éŒ¯èª¤: {e}")

        print("\n" + "="*70 + "\n")


if __name__ == '__main__':
    asyncio.run(compare_models())
```

**ç¯„ä¾‹è¼¸å‡º**ï¼š

```
======================================================================
å¤šä¾›æ‡‰å•†æ¨¡å‹æ¯”è¼ƒ
======================================================================

æŸ¥è©¢: å‘ä¸€ä½ 12 æ­²çš„å­©å­è§£é‡‹é‡å­ç³¾çºã€‚
ä½¿ç”¨ä»–å€‘èƒ½ç†è§£çš„æ¯”å–»ã€‚

======================================================================

### Gemini 2.5 Flash
----------------------------------------------------------------------
æƒ³åƒä½ æœ‰å…©å€‹é­”æ³•ç¡¬å¹£ã€‚ç•¶ä½ æ‹‹æ“²å…¶ä¸­ä¸€å€‹ï¼Œå®ƒæ­£é¢æœä¸Šæ™‚ï¼Œ
å¦ä¸€å€‹ç¡¬å¹£æœƒç«‹å³åé¢æœä¸Šâ€”â€”ç„¡è«–å®ƒå€‘ç›¸è·å¤šé ã€‚å³ä½¿ä¸€å€‹ç¡¬å¹£åœ¨åœ°çƒï¼Œ
å¦ä¸€å€‹åœ¨ç«æ˜Ÿï¼

é€™å°±æ˜¯é‡å­ç³¾çºã€‚å…©å€‹ç²’å­è®Šå¾—ã€Œç³¾çºã€ï¼Œä»¥è‡³æ–¼
æ¸¬é‡å…¶ä¸­ä¸€å€‹æœƒç«‹å³å½±éŸ¿å¦ä¸€å€‹ï¼Œå³ä½¿è·é›¢å¾ˆé ã€‚

[é•·åº¦: 387 å­—å…ƒ]

======================================================================

### GPT-4o
----------------------------------------------------------------------
æŠŠé‡å­ç³¾çºæƒ³åƒæˆæœ‰å…©å€‹ç›¸é€£çš„é­”æ³•éª°å­ã€‚ç•¶ä½ æ“²å‡ºä¸€å€‹éª°å­
é¡¯ç¤º 6 æ™‚ï¼Œå¦ä¸€å€‹éª°å­æœƒè‡ªå‹•é¡¯ç¤º 1â€”â€”ç«‹å³ï¼Œå³ä½¿å®ƒåœ¨
ä¸–ç•Œçš„å¦ä¸€ç«¯ï¼ç§‘å­¸å®¶é‚„ä¸å®Œå…¨æ˜ç™½é€™æ˜¯å¦‚ä½•ç™¼ç”Ÿçš„ï¼Œä½†
ä»–å€‘çŸ¥é“ç¢ºå¯¦å¦‚æ­¤ã€‚é€™æ˜¯ç‰©ç†å­¸ä¸­æœ€å¥‡æ€ªçš„äº‹æƒ…ä¹‹ä¸€ï¼

[é•·åº¦: 415 å­—å…ƒ]

======================================================================

### Claude 3.7 Sonnet
----------------------------------------------------------------------
æƒ³åƒä½ å’Œä½ æœ€å¥½çš„æœ‹å‹å„æœ‰ä¸€é¡†é­”æ³•å½ˆç ã€‚ç„¡è«–ä½ å€‘èµ°å¤šé â€”â€”
å³ä½¿å»äº†ä¸åŒçš„åœ‹å®¶â€”â€”ç•¶ä½ æä½ä½ çš„å½ˆç ï¼Œå®ƒè®Šæˆç´…è‰²æ™‚ï¼Œ
ä½ æœ‹å‹çš„å½ˆç æœƒåœ¨å®Œå…¨ç›¸åŒçš„ç¬é–“è®Šæˆè—è‰²ã€‚

é€™å°±æ˜¯é‡å­ç³¾çºï¼å…©å€‹ç²’å­è¢«é€£çµèµ·ä¾†ï¼Œä»¥è‡³æ–¼ç™¼ç”Ÿåœ¨å…¶ä¸­ä¸€å€‹
èº«ä¸Šçš„äº‹æœƒç«‹å³å½±éŸ¿å¦ä¸€å€‹ï¼Œç„¡è«–è·é›¢å¤šé ã€‚æ„›å› æ–¯å¦ç¨±ä¹‹ç‚º
ã€Œé¬¼é­…èˆ¬çš„è¶…è·ä½œç”¨ã€ï¼Œå› ç‚ºé€£ä»–éƒ½è¦ºå¾—é€™å¾ˆå¥‡æ€ªï¼

[é•·åº¦: 512 å­—å…ƒ]

======================================================================

### Llama 3.3 (Local)
----------------------------------------------------------------------
æŠŠé‡å­ç³¾çºæƒ³åƒæˆæœ‰å…©å€‹ç‰¹æ®Šçš„é›™èƒèƒç¡¬å¹£ã€‚å¦‚æœä½ æ‹‹æ“²ä¸€å€‹ç¡¬å¹£
å®ƒæ­£é¢æœä¸Šï¼Œå¦ä¸€å€‹ç¡¬å¹£ç¸½æœƒåé¢æœä¸Šâ€”â€”ç«‹å³ï¼å®ƒå€‘ä»¥ä¸€ç¨®
ç¥ç§˜çš„æ–¹å¼ç›¸é€£ï¼Œç§‘å­¸å®¶å€‘ä»åœ¨åŠªåŠ›å®Œå…¨ç†è§£ã€‚

[é•·åº¦: 287 å­—å…ƒ]

======================================================================
```

**è§€å¯Ÿ**ï¼š

- **Gemini 2.5 Flash**ï¼šå¿«é€Ÿã€ç°¡æ½”ã€æº–ç¢º
- **GPT-4o**ï¼šæ¸…æ™°çš„æ¯”å–»ï¼Œæ‰¿èªç¥ç§˜æ€§
- **Claude 3.7 Sonnet**ï¼šæœ€è©³ç´°ï¼ŒåŒ…å«æ„›å› æ–¯å¦çš„åè¨€
- **Llama 3.3**ï¼šæœ€çŸ­ï¼Œè¼ƒç°¡å–®ä½†å¸å¼•åŠ›è¼ƒä½

---

## 7. æˆæœ¬å„ªåŒ–ç­–ç•¥ (Cost Optimization Strategies)

### æˆæœ¬æ¯”è¼ƒï¼ˆæ¯ 1M tokensï¼‰(Cost Comparison (per 1M tokens))

| ä¾›æ‡‰å•† (Provider) | æ¨¡å‹ (Model)            | è¼¸å…¥æˆæœ¬ (Input Cost) | è¼¸å‡ºæˆæœ¬ (Output Cost) | ç¸½è¨ˆ (1M è¼¸å…¥ + 1M è¼¸å‡º) (Total (1M in + 1M out)) |
| ----------------- | ----------------------- | --------------------- | --------------------- | --------------------------------------------------- |
| **Google**        | gemini-2.5-flash        | $0.075                | $0.30                 | **$0.375** â­ æœ€ä¾¿å®œ (Cheapest)                   |
| **Google**        | gemini-2.5-pro          | $1.25                 | $5.00                 | $6.25                                               |
| **OpenAI**        | gpt-4o-mini             | $0.15                 | $0.60                 | $0.75                                               |
| **OpenAI**        | gpt-4o                  | $2.50                 | $10.00                | $12.50                                              |
| **Anthropic**     | claude-3-5-haiku        | $0.80                 | $4.00                 | $4.80                                               |
| **Anthropic**     | claude-3-7-sonnet       | $3.00                 | $15.00                | $18.00                                              |
| **Ollama**        | granite4:latest (local) | $0                    | $0                    | **$0** ğŸ‰ å…è²» (Free)                               |

### ç­–ç•¥ 1ï¼šåˆ†å±¤æ¨¡å‹é¸æ“‡ (Strategy 1: Tiered Model Selection)

```python
def get_model_for_task(complexity: str):
    """æ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡æ¨¡å‹ã€‚"""

    if complexity == 'simple':
        # å°æ–¼ç°¡å–®ä»»å‹™ï¼Œä½¿ç”¨æœ€ä¾¿å®œçš„æ¨¡å‹
        return LiteLlm(model='openai/gpt-4o-mini')  # æˆ– gemini-2.5-flash

    elif complexity == 'medium':
        # å¹³è¡¡æˆæœ¬/å“è³ª
        return GoogleGenAI(model='gemini-2.5-flash')

    elif complexity == 'complex':
        # æœ€ä½³æ¨ç†èƒ½åŠ›ï¼Œå€¼å¾—èŠ±è²»
        return LiteLlm(model='anthropic/claude-3-7-sonnet-20250219')

    elif complexity == 'local_ok':
        # éš±ç§/æˆæœ¬å„ªå…ˆ
        return LiteLlm(model='ollama_chat/llama3.3')

# ç¯„ä¾‹ç”¨æ³•
simple_agent = Agent(model=get_model_for_task('simple'))
complex_agent = Agent(model=get_model_for_task('complex'))
```

### ç­–ç•¥ 2ï¼šå‚™æ´éˆ (Strategy 2: Fallback Chain)

```python
async def run_with_fallback(query: str):
    """æŒ‰æˆæœ¬é †åºå˜—è©¦æ¨¡å‹ï¼ˆæœ€ä¾¿å®œçš„å„ªå…ˆï¼‰ã€‚"""

    models = [
        ('gemini-2.5-flash', GoogleGenAI(model='gemini-2.5-flash')),
        ('gpt-4o-mini', LiteLlm(model='openai/gpt-4o-mini')),
        ('gpt-4o', LiteLlm(model='openai/gpt-4o'))
    ]

    for model_name, model in models:
        try:
            agent = Agent(model=model)
            runner = InMemoryRunner(agent=agent, app_name='fallback_app')
            session = await runner.session_service.create_session(
                app_name='fallback_app',
                user_id='user_001'
            )

            new_message = types.Content(
                role='user',
                parts=[types.Part(text=query)]
            )

            result_text = None
            async for event in runner.run_async(
                user_id='user_001',
                session_id=session.id,
                new_message=new_message
            ):
                if event.content and event.content.parts:
                    result_text = event.content.parts[0].text

            print(f"âœ… æˆåŠŸä½¿ç”¨ {model_name}")
            return result_text

        except Exception as e:
            print(f"âŒ {model_name} å¤±æ•—: {e}")
            continue

    raise Exception("æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—äº†")
```

### ç­–ç•¥ 3ï¼šé«˜æµé‡ä½¿ç”¨æœ¬åœ°æ¨¡å‹ (Strategy 3: Local for High Volume)

```python
"""
å°é«˜æµé‡ã€ç°¡å–®çš„ä»»å‹™ä½¿ç”¨æœ¬åœ° Ollamaã€‚
åƒ…åœ¨éœ€è¦æ™‚ä½¿ç”¨é›²ç«¯æ¨¡å‹ã€‚
"""

async def process_batch(queries: list[str]):
    """ä»¥ç¬¦åˆæˆæœ¬æ•ˆç›Šçš„æ–¹å¼è™•ç†å¤§é‡æŸ¥è©¢ã€‚"""

    # ç”¨æ–¼å¤§é‡è™•ç†çš„æœ¬åœ°æ¨¡å‹
    local_model = LiteLlm(model='ollama_chat/llama3.3')
    local_agent = Agent(model=local_model)

    # ç”¨æ–¼è¤‡é›œæŸ¥è©¢çš„é›²ç«¯æ¨¡å‹
    cloud_model = GoogleGenAI(model='gemini-2.5-flash')
    cloud_agent = Agent(model=cloud_model)

    results = []

    for query in queries:
        # æ ¹æ“šè¤‡é›œåº¦è·¯ç”±ä¸¦å»ºç«‹é©ç•¶çš„ runner
        if is_simple(query):
            # å…è²»çš„æœ¬åœ°è™•ç†
            runner = InMemoryRunner(agent=local_agent, app_name='batch_app')
        else:
            # å°æ–¼è¤‡é›œæŸ¥è©¢ä½¿ç”¨é›²ç«¯æ¨¡å‹
            runner = InMemoryRunner(agent=cloud_agent, app_name='batch_app')

        # å»ºç«‹ session
        session = await runner.session_service.create_session(
            app_name='batch_app',
            user_id='batch_user'
        )

        # ä½¿ç”¨ async è¿­ä»£åŸ·è¡ŒæŸ¥è©¢
        new_message = types.Content(
            role='user',
            parts=[types.Part(text=query)]
        )

        result_text = None
        async for event in runner.run_async(
            user_id='batch_user',
            session_id=session.id,
            new_message=new_message
        ):
            if event.content and event.content.parts:
                result_text = event.content.parts[0].text

        results.append(result_text)

    return results


def is_simple(query: str) -> bool:
    """åˆ¤æ–·æŸ¥è©¢æ˜¯å¦è¶³å¤ ç°¡å–®ä»¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹ã€‚"""
    simple_keywords = ['what is', 'define', 'explain', 'summarize']
    return any(kw in query.lower() for kw in simple_keywords)
```

---

## 8. æœ€ä½³å¯¦è¸ (Best Practices)

### âœ… æ‡‰è©²åš (DO)

**1. ç›¡å¯èƒ½ä½¿ç”¨åŸç”Ÿ Gemini**ï¼š

```python
# âœ… æœ€ä½³ - åŸç”Ÿ Gemini
agent = Agent(model='gemini-2.5-flash')

# âŒ ä¸å»ºè­° - é€é LiteLLM ä½¿ç”¨ Gemini (é€Ÿåº¦è¼ƒæ…¢ï¼ŒåŠŸèƒ½ç¼ºå¤±)
agent = Agent(model=LiteLlm(model='gemini/gemini-2.5-flash'))
```

**2. å®‰å…¨åœ°è¨­å®šç’°å¢ƒè®Šæ•¸**ï¼š

```python
import os

# âœ… è‰¯å¥½ - å¾ç’°å¢ƒè®Šæ•¸è®€å–
api_key = os.environ.get('OPENAI_API_KEY')

# âŒ ä¸ä½³ - ç¡¬å¼ç·¨ç¢¼
api_key = 'sk-...'  # çµ•ä¸è¦æäº¤é€™å€‹ï¼
```

**3. è™•ç†ç‰¹å®šä¾›æ‡‰å•†çš„éŒ¯èª¤**ï¼š

```python
try:
    result = await runner.run_async(query, agent=agent)
except Exception as e:
    if 'rate_limit' in str(e).lower():
        print("é”åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…ä¸­...")
        await asyncio.sleep(60)
    elif 'quota' in str(e).lower():
        print("é…é¡å·²è¶…éï¼Œåˆ‡æ›ä¾›æ‡‰å•†...")
        agent.model = fallback_model
    else:
        raise
```

**4. æ­£ç¢ºä½¿ç”¨ Ollama**ï¼š

```python
# âœ… æ­£ç¢º - ä½¿ç”¨ ollama_chat å‰ç¶´
model = LiteLlm(model='ollama_chat/llama3.3')

# âŒ éŒ¯èª¤ - ä½¿ç”¨ ollama å‰ç¶´ (åŠŸèƒ½å—é™)
model = LiteLlm(model='ollama/llama3.3')
```

**5. ç›£æ§æˆæœ¬**ï¼š

```python
import time

class CostTracker:
    def __init__(self):
        self.total_tokens = 0
        self.model_costs = {
            'openai/gpt-4o': 2.50 / 1_000_000,  # æ¯è¼¸å…¥ token
            'anthropic/claude-3-7-sonnet-20250219': 3.00 / 1_000_000
        }

    def track(self, model: str, tokens: int):
        cost = tokens * self.model_costs.get(model, 0)
        self.total_tokens += tokens
        print(f"æˆæœ¬: ${cost:.4f} | ç¸½è¨ˆ: {self.total_tokens:,} tokens")

tracker = CostTracker()
```

### âŒ ä¸æ‡‰è©²åš (DON'T)

**1. ä¸è¦ç‚º Gemini ä½¿ç”¨ LiteLLM**ï¼š

```python
# âŒ ä¸ä½³ - å¤±å» Gemini ç‰¹å®šåŠŸèƒ½
model = LiteLlm(model='gemini/gemini-2.5-flash')

# âœ… è‰¯å¥½ - ä½¿ç”¨åŸç”Ÿ
model = 'gemini-2.5-flash'  # æˆ– GoogleGenAI('gemini-2.5-flash')
```

**2. ä¸è¦å¿˜è¨˜ `ollama_chat` å‰ç¶´**ï¼š

```python
# âŒ éŒ¯èª¤
LiteLlm(model='ollama/llama3.3')

# âœ… æ­£ç¢º
LiteLlm(model='ollama_chat/llama3.3')
```

**3. ä¸è¦å¿½ç•¥ä¾›æ‡‰å•†çš„é™åˆ¶**ï¼š

- OpenAIï¼š200K tokens/åˆ†é˜ï¼ˆä¾å±¤ç´šè€Œå®šï¼‰
- Anthropicï¼š200K tokens/åˆ†é˜ï¼ˆä¸å›ºå®šï¼‰
- Ollamaï¼šå—æ‚¨çš„ GPU é™åˆ¶

**4. ä¸è¦æ··æ·†æ†‘è­‰**ï¼š

```bash
# âŒ ä¸ä½³ - è¡çª
export OPENAI_API_KEY='key1'
export OPENAI_API_KEY='key2'  # æœƒè¦†è“‹ï¼

# âœ… è‰¯å¥½ - å¦‚æœ‰éœ€è¦ï¼Œä½¿ç”¨ä¸åŒçš„ç’°å¢ƒè®Šæ•¸åç¨±
export OPENAI_API_KEY='key1'
export AZURE_OPENAI_API_KEY='key2'
```

---

## ç¸½çµ (Summary)

æ‚¨å·²å­¸æœƒå¦‚ä½•åœ¨ ADK ä»£ç†ç¨‹å¼ä¸­é€é LiteLLM ä½¿ç”¨ OpenAIã€Claudeã€Ollama åŠå…¶ä»– LLMï¼š

**é‡é»å›é¡§**ï¼š

- âœ… **LiteLLM** è®“ ADK èƒ½ä½¿ç”¨è¶…é 100 ç¨® LLM ä¾›æ‡‰å•†
- âœ… **OpenAI**ï¼š`LiteLlm(model='openai/gpt-4o-mini')` - éœ€è¦ `OPENAI_API_KEY`
- âœ… **Claude**ï¼š`LiteLlm(model='anthropic/claude-3-7-sonnet-20250219')` - éœ€è¦ `ANTHROPIC_API_KEY`
- âœ… **Ollama**ï¼š`LiteLlm(model='ollama_chat/granite4:latest')` - âš ï¸ ä½¿ç”¨ `ollama_chat`ï¼Œè€Œé `ollama`ï¼
- âœ… **Azure OpenAI**ï¼š`LiteLlm(model='azure/deployment-name')` - ä¼æ¥­é¸é …
- âœ… **ä¸è¦**ç‚º Gemini ä½¿ç”¨ LiteLLM - è«‹æ”¹ç”¨åŸç”Ÿçš„ `GoogleGenAI`
- âœ… **æœ¬åœ°æ¨¡å‹** (Ollama) éå¸¸é©åˆéš±ç§ã€æˆæœ¬åŠé›¢ç·šä½¿ç”¨
- âœ… **æˆæœ¬å„ªåŒ–**ï¼šgemini-2.5-flash ($0.375/1M)ã€gpt-4o-mini ($0.75/1M)ã€æœ¬åœ° (å…è²»)

**æ¨¡å‹å­—ä¸²æ ¼å¼**ï¼š

| ä¾›æ‡‰å•† (Provider) | æ ¼å¼ (Format)       | ç¯„ä¾‹ (Example)                               |
| ----------------- | --------------------- | ---------------------------------------------- |
| OpenAI            | `openai/[model]`      | `openai/gpt-4o`                                |
| Anthropic         | `anthropic/[model]`   | `anthropic/claude-3-7-sonnet-20250219`         |
| Ollama            | `ollama_chat/[model]` | `ollama_chat/granite4:latest` âš ï¸ ä¸æ˜¯ `ollama/` |
| Azure             | `azure/[deployment]`  | `azure/gpt-4o-deployment`                      |
| Vertex AI         | `vertex_ai/[model]`   | `vertex_ai/claude-3-7-sonnet@20250219`         |

**ä½•æ™‚ä½¿ç”¨ä½•ç¨®æ¨¡å‹**ï¼š

| ä½¿ç”¨æƒ…å¢ƒ (Use Case)         | æ¨è–¦æ¨¡å‹ (Recommended Model)        |
| --------------------------- | ----------------------------------- |
| ç°¡å–®ä»»å‹™ï¼Œé«˜æµé‡ (Simple tasks, high volume) | gemini-2.5-flash æˆ– gpt-4o-mini     |
| è¤‡é›œæ¨ç† (Complex reasoning) | claude-3-7-sonnet æˆ– gpt-4o         |
| éš±ç§/åˆè¦æ€§ (Privacy/compliance) | ollama_chat/granite4:latest (æœ¬åœ°) |
| ä¼æ¥­ Azure (Enterprise Azure) | azure/gpt-4o-deployment             |
| æˆæœ¬å„ªåŒ– (Cost optimization) | gemini-2.5-flash (æœ€ä¾¿å®œçš„é›²ç«¯æ¨¡å‹) |
| é›¢ç·š/æ°£éš™ç’°å¢ƒ (Offline/air-gapped) | ollama_chat æ¨¡å‹                  |
| ç·¨ç¢¼ä»»å‹™ (Coding tasks)     | ollama_chat/phi4 æˆ– gpt-4o          |
| é•·ç¯‡å…§å®¹ (Long-form content) | claude-3-7-sonnet                   |

**æ‰€éœ€ç’°å¢ƒè®Šæ•¸**ï¼š

```bash
# OpenAI
export OPENAI_API_KEY='sk-...'

# Anthropic
export ANTHROPIC_API_KEY='sk-ant-...'

# Ollama
export OLLAMA_API_BASE='http://localhost:11434'

# Azure OpenAI
export AZURE_API_KEY='...'
export AZURE_API_BASE='https://your-resource.openai.azure.com/'
export AZURE_API_VERSION='2024-02-15-preview'

# Google (ç”¨æ–¼åŸç”Ÿ Geminiï¼Œé LiteLLM)
export GOOGLE_CLOUD_PROJECT='your-project'
export GOOGLE_CLOUD_LOCATION='us-central1'
```

**ç”Ÿç”¢æª¢æŸ¥æ¸…å–®**ï¼š

- [ ] ç’°å¢ƒè®Šæ•¸å·²å®‰å…¨è¨­å®šï¼ˆéç¡¬å¼ç·¨ç¢¼ï¼‰
- [ ] API é‡‘é‘°å„²å­˜åœ¨ç§˜å¯†ç®¡ç†å™¨ä¸­ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
- [ ] å·²å¯¦ä½œæˆæ•ˆè¿½è¹¤
- [ ] å·²è™•ç†é€Ÿç‡é™åˆ¶
- [ ] å·²è¨­å®šå‚™æ´æ¨¡å‹
- [ ] Ollama æ¨¡å‹ä½¿ç”¨ `ollama_chat` å‰ç¶´ï¼ˆé `ollama`ï¼‰
- [ ] æœªä½¿ç”¨ LiteLLM è™•ç† Geminiï¼ˆæ”¹ç”¨åŸç”Ÿï¼‰
- [ ] è™•ç†ç‰¹å®šä¾›æ‡‰å•†çš„éŒ¯èª¤
- [ ] æ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡æ¨¡å‹
- [ ] å·²è¨­å®šç›£æ§èˆ‡è­¦å ±

**è³‡æº**ï¼š

- [LiteLLM æ–‡ä»¶](https://docs.litellm.ai/)
- [OpenAI API åƒè€ƒ](https://platform.openai.com/docs/api-reference)
- [Anthropic Claude æ–‡ä»¶](https://docs.anthropic.com/)
- [Ollama æ¨¡å‹](https://ollama.com/library)
- [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)
- [ADK LiteLLM ç¯„ä¾‹](https://github.com/google/adk-docs/tree/main/contributing/samples/hello_world_litellm)

---

## ç¨‹å¼ç¢¼å¯¦ç¾ (Code Implementation)
- multi-llm-agent: [ç¨‹å¼ç¢¼é€£çµ](../../../python/agents/multi-llm-agent/)
