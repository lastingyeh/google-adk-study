# æ•™å­¸ 22ï¼šæ¨¡å‹é¸æ“‡èˆ‡æœ€ä½³åŒ– (Tutorial 22: Model Selection & Optimization)

**ç›®æ¨™**: æŒæ¡æ¨¡å‹é¸æ“‡ç­–ç•¥ï¼Œäº†è§£æ¨¡å‹çš„èƒ½åŠ›èˆ‡é™åˆ¶ï¼Œæœ€ä½³åŒ–æˆæœ¬èˆ‡æ•ˆèƒ½ï¼Œä¸¦ç‚ºç‰¹å®šä½¿ç”¨æƒ…å¢ƒé¸æ“‡æ­£ç¢ºçš„æ¨¡å‹ã€‚

**å…ˆæ±ºæ¢ä»¶**:

- æ•™å­¸ 01 (Hello World Agent)
- äº†è§£åŸºæœ¬çš„ Agent æ¦‚å¿µ
- ç†Ÿæ‚‰ä¸åŒçš„ Agent èƒ½åŠ›

**æ‚¨å°‡å­¸åˆ°**:

- Gemini æ¨¡å‹å®¶æ—æ¦‚è¦½èˆ‡æ¯”è¼ƒ
- æ¨¡å‹èƒ½åŠ›çŸ©é™£ (è¦–è¦ºã€æ€è€ƒã€ç¨‹å¼ç¢¼åŸ·è¡Œç­‰)
- æ•ˆèƒ½èˆ‡æˆæœ¬çš„æ¬Šè¡¡
- Context Window èˆ‡ Token é™åˆ¶
- æ¨¡å‹é¸æ“‡æ±ºç­–æ¡†æ¶
- æ¸¬è©¦èˆ‡åŸºæº–ç­–ç•¥
- æ¨¡å‹ä¹‹é–“çš„é·ç§»ç­–ç•¥

**å®Œæˆæ™‚é–“**: 45-60 åˆ†é˜

---

## ç‚ºä½•æ¨¡å‹é¸æ“‡å¦‚æ­¤é‡è¦ (Why Model Selection Matters)

**å•é¡Œ**: ä¸åŒçš„æ¨¡å‹å…·æœ‰ä¸åŒçš„èƒ½åŠ›ã€æˆæœ¬å’Œæ•ˆèƒ½ç‰¹æ€§ã€‚é¸æ“‡éŒ¯èª¤çš„æ¨¡å‹æœƒå°è‡´çµæœä¸ä½³æˆ–ä¸å¿…è¦çš„æˆæœ¬ã€‚

**è§£æ±ºæ–¹æ¡ˆ**: æ ¹æ“šéœ€æ±‚ã€ä½¿ç”¨æƒ…å¢ƒã€é ç®—å’Œæ•ˆèƒ½éœ€æ±‚é€²è¡Œ**ç­–ç•¥æ€§æ¨¡å‹é¸æ“‡**ã€‚

**å„ªé»**:

- ğŸ’° **æˆæœ¬æœ€ä½³åŒ–**: åªç‚ºæ‚¨éœ€è¦çš„èƒ½åŠ›ä»˜è²»
- âš¡ **æ•ˆèƒ½**: ç‚ºæ‚¨çš„æ‡‰ç”¨ç¨‹å¼æä¾›åˆé©çš„é€Ÿåº¦
- ğŸ¯ **èƒ½åŠ›åŒ¹é…**: æ¨¡å‹å…·å‚™æ‚¨æ‰€éœ€çš„åŠŸèƒ½
- ğŸ“Š **å“è³ª**: ç‚ºæ‚¨çš„ç‰¹å®šä½¿ç”¨æƒ…å¢ƒæä¾›æœ€ä½³çµæœ
- [FLOW]**æ“´å±•æ€§**: èƒ½å¤ è™•ç†æ‚¨çš„è² è¼‰çš„æ¨¡å‹

**æ±ºç­–å› ç´ **:

- ä»»å‹™è¤‡é›œåº¦
- å›æ‡‰æ™‚é–“è¦æ±‚
- é ç®—é™åˆ¶
- åŠŸèƒ½éœ€æ±‚ (è¦–è¦ºã€æ€è€ƒã€ç¨‹å¼ç¢¼åŸ·è¡Œ)
- Context Window éœ€æ±‚
- éƒ¨ç½²ç’°å¢ƒ

---

## 1. Gemini æ¨¡å‹å®¶æ—æ¦‚è¦½ (1. Gemini Model Family Overview)

### ç›®å‰æ¨¡å‹é™£å®¹ (2025) (Current Model Lineup (2025))

**ä¾†æº**: ADK é€é Google AI å’Œ Vertex AI æ”¯æ´æ‰€æœ‰ Gemini æ¨¡å‹

**âš ï¸ é‡è¦**: æˆªè‡³ 2025 å¹´ 10 æœˆï¼Œ**Gemini 2.5 Flash** å› å…¶å‡ºè‰²çš„æ€§åƒ¹æ¯”ï¼Œ**å»ºè­°**ç”¨æ–¼æ–°çš„ Agentã€‚è«‹æ³¨æ„ï¼ŒADK çš„é è¨­æ¨¡å‹åƒæ•¸ç‚ºç©ºå­—ä¸² (æœƒå¾çˆ¶ Agent ç¹¼æ‰¿)ï¼Œå› æ­¤**è«‹å‹™å¿…æ˜ç¢ºæŒ‡å®šæ¨¡å‹**ã€‚

| æ¨¡å‹ (Model) | Context Window | ä¸»è¦åŠŸèƒ½ (Key Features) | æœ€é©ç”¨é€” (Best For) | ç‹€æ…‹ (Status) |
| --- | --- | --- | --- | --- |
| **gemini-2.5-flash** â­ | 1M tokens | **å»ºè­°**, æ€è€ƒ, å¿«é€Ÿ, å¤šæ¨¡æ…‹ | **é€šç”¨ç›®çš„**, ç”Ÿç”¢ç’°å¢ƒ | **ç©©å®š (Stable)** |
| **gemini-2.5-pro** | 1M tokens | æœ€æ–°æŠ€è¡“, è¤‡é›œæ¨ç†, STEM | é—œéµåˆ†æ, ç ”ç©¶ | **ç©©å®š (Stable)** |
| **gemini-2.5-flash-lite** | 1M tokens | è¶…å¿«, æˆæœ¬æ•ˆç›Šé«˜, é«˜ååé‡ | å¤§é‡, ç°¡å–®ä»»å‹™ | **é è¦½ (Preview)** |
| **gemini-2.0-flash** | 1M tokens | å¿«é€Ÿ, æ€è€ƒ, ç¨‹å¼ç¢¼åŸ·è¡Œ | é€šç”¨ç›®çš„ (èˆŠç‰ˆ) | ç©©å®š (Stable) |
| **gemini-2.0-flash-thinking** | 1M tokens | æ“´å±•æ€è€ƒæ¨¡å¼ | è¤‡é›œæ¨ç† (èˆŠç‰ˆ) | ç©©å®š (Stable) |
| **gemini-1.5-flash** | 1M tokens | å¿«é€Ÿ, æˆæœ¬æ•ˆç›Šé«˜ | å¤§é‡ (èˆŠç‰ˆ) | ç©©å®š (Stable) |
| **gemini-1.5-flash-8b** | 1M tokens | è¶…å¿«, ç¶“æ¿Ÿ | ç°¡å–®æŸ¥è©¢ (èˆŠç‰ˆ) | ç©©å®š (Stable) |
| **gemini-1.5-pro** | 2M tokens | æ“´å±• Context | å¤§å‹æ–‡ä»¶ (èˆŠç‰ˆ) | ç©©å®š (Stable) |
| **gemini-2.0-flash-live** | ä¸²æµ | é›™å‘éŸ³è¨Š/è¦–è¨Š (Vertex) | å³æ™‚å°è©± | é è¦½ (Preview) |
| **gemini-live-2.5-flash** | ä¸²æµ | Live API (AI Studio) | èªéŸ³åŠ©ç† | é è¦½ (Preview) |

**æ¨¡å‹ä¸–ä»£**:

- **2.5 ç³»åˆ—** (æœ€æ–°, 2025 å¹´ 10 æœˆ): é¦–æ¬¾å…·å‚™åŸç”Ÿæ€è€ƒã€åœ–åƒç”Ÿæˆèƒ½åŠ›ï¼Œæœ€ä½³æ€§åƒ¹æ¯”
- **2.0 ç³»åˆ—** (2024 å¹´ 12 æœˆ): ç¬¬äºŒä»£ï¼Œç¨‹å¼ç¢¼åŸ·è¡Œï¼ŒGoogle Search
- **1.5 ç³»åˆ—** (2024 å¹´åˆ): ç¬¬ä¸€ä»£ï¼Œå¤šæ¨¡æ…‹åŸºç¤

### ğŸ†• Gemini 2.5 çš„æ–°åŠŸèƒ½ (What's New in Gemini 2.5)

**Gemini 2.5 Flash** æ˜¯æˆ‘å€‘åœ¨æ€§åƒ¹æ¯”æ–¹é¢æœ€ä½³çš„æ¨¡å‹ï¼š

- âœ… **åŸç”Ÿæ€è€ƒèƒ½åŠ›**: æŸ¥çœ‹æ¨¡å‹çš„æ¨ç†éç¨‹
- âœ… **åœ–åƒç”Ÿæˆ**: åŸç”Ÿç”Ÿæˆå’Œç·¨è¼¯åœ–åƒ (2.5 Flash Image è®Šé«”)
- âœ… **é•· Context**: 100 è¬ Token çš„ Context Window
- âœ… **å¤šæ¨¡æ…‹**: ç†è§£æ–‡å­—ã€åœ–åƒã€éŸ³è¨Šã€è¦–è¨Š
- âœ… **å…¨èƒ½å‹**: åœ¨ç·¨ç¢¼ã€æ¨ç†ã€å‰µæ„å¯«ä½œæ–¹é¢è¡¨ç¾å‡ºè‰²
- âœ… **æœ€é©åˆ Agent**: ç‚ºå¤§è¦æ¨¡è™•ç†å’Œ Agent ä½¿ç”¨æƒ…å¢ƒé€²è¡Œæœ€ä½³åŒ–

**Gemini 2.5 Pro** æ˜¯æœ€å…ˆé€²çš„æ€è€ƒæ¨¡å‹ï¼š

- âœ… **é€²éšæ¨ç†**: è™•ç†ç¨‹å¼ç¢¼ã€æ•¸å­¸ã€STEM ä¸­çš„è¤‡é›œå•é¡Œ
- âœ… **é•· Context åˆ†æ**: åˆ†æå¤§å‹è³‡æ–™é›†ã€ç¨‹å¼ç¢¼åº«ã€æ–‡ä»¶
- âœ… **æœ€é«˜å“è³ª**: å°æ–¼é—œéµæ‡‰ç”¨ç¨‹å¼ç‚ºåŒé¡æœ€ä½³
- âœ… **ç ”ç©¶ç´š**: é©ç”¨æ–¼å­¸è¡“å’Œå°ˆæ¥­ç ”ç©¶

**å®˜æ–¹æ–‡ä»¶**:

- Google AI: https://ai.google.dev/gemini-api/docs/models
- Vertex AI: https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash
- æŠ€è¡“å ±å‘Š: https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf

### æ¨¡å‹æ¯”è¼ƒçŸ©é™£ (Model Comparison Matrix)

```python
"""
æ¨¡å‹èƒ½åŠ›èˆ‡å®šåƒ¹æ¯”è¼ƒã€‚
"""

MODELS = {
    # === GEMINI 2.5 ç³»åˆ— (æœ€æ–° - 2025 å¹´ 10 æœˆ) ===
    'gemini-2.5-flash': {
        'context_window': 1_000_000,
        'features': ['vision', 'thinking', 'code_execution', 'audio', 'video', 'image_generation'],
        'speed': 'fast',
        'cost': 'low',
        'quality': 'excellent',
        'is_recommended': True,  # å»ºè­°ç”¨æ–¼æ–°å°ˆæ¡ˆçš„æ¨¡å‹
        'generation': '2.5',
        'recommended_for': [
            'â­ å»ºè­°æ‰€æœ‰æ–° Agent ä½¿ç”¨',
            'ä¸€èˆ¬ Agent æ‡‰ç”¨',
            'ç”Ÿç”¢ç³»çµ±',
            'Agent å·¥ä½œæµç¨‹',
            'æœ€ä½³æ€§åƒ¹æ¯”'
        ],
        'note': 'é¦–æ¬¾å…·å‚™åŸç”Ÿæ€è€ƒèƒ½åŠ›çš„ Flash æ¨¡å‹'
    },
    'gemini-2.5-pro': {
        'context_window': 1_000_000,
        'features': ['vision', 'thinking', 'code_execution', 'audio', 'video', 'advanced_reasoning'],
        'speed': 'moderate',
        'cost': 'high',
        'quality': 'state_of_the_art',
        'generation': '2.5',
        'recommended_for': [
            'è¤‡é›œæ¨ç†ä»»å‹™',
            'STEM å•é¡Œ (ç¨‹å¼ç¢¼ã€æ•¸å­¸ã€ç‰©ç†)',
            'ç ”ç©¶èˆ‡åˆ†æ',
            'éœ€è¦æœ€é«˜å“è³ªçš„é—œéµæ‡‰ç”¨'
        ],
        'note': 'æœ€å…ˆé€²çš„æ€è€ƒæ¨¡å‹'
    },
    'gemini-2.5-flash-lite': {
        'context_window': 1_000_000,
        'features': ['vision', 'audio', 'video', 'ultra_fast'],
        'speed': 'ultra_fast',
        'cost': 'ultra_low',
        'quality': 'good',
        'generation': '2.5',
        'recommended_for': [
            'è¶…é«˜ååé‡æ‡‰ç”¨',
            'å¤§è¦æ¨¡ç°¡å–®æŸ¥è©¢',
            'å°æˆæœ¬æ•æ„Ÿçš„éƒ¨ç½²',
            'å³æ™‚ä½å»¶é²ä»»å‹™'
        ],
        'note': 'æœ€å¿«çš„ flash æ¨¡å‹ï¼Œç‚ºæˆæœ¬æ•ˆç›Šæœ€ä½³åŒ–'
    },

    # === GEMINI 2.0 ç³»åˆ— (èˆŠç‰ˆ) ===
    'gemini-2.0-flash': {
        'context_window': 1_000_000,
        'features': ['vision', 'thinking', 'code_execution', 'audio', 'video'],
        'speed': 'fast',
        'cost': 'low',
        'quality': 'high',
        'generation': '2.0',
        'recommended_for': [
            'ä¸€èˆ¬ Agent æ‡‰ç”¨',
            'ç”Ÿç”¢ç³»çµ±',
            'å¤š Agent å·¥ä½œæµç¨‹',
            'è¤‡é›œæ¨ç†'
        ],
        'note': 'è€ƒæ…®å‡ç´šè‡³ gemini-2.5-flash'
    },
    'gemini-2.0-flash-thinking': {
        'context_window': 1_000_000,
        'features': ['vision', 'thinking', 'code_execution', 'extended_reasoning'],
        'speed': 'moderate',
        'cost': 'moderate',
        'quality': 'very_high',
        'generation': '2.0',
        'recommended_for': [
            'ç­–ç•¥è¦åŠƒ',
            'è¤‡é›œå•é¡Œè§£æ±º',
            'ç ”ç©¶åˆ†æ',
            'æ·±åº¦æ¨ç†ä»»å‹™'
        ],
        'note': 'è€ƒæ…®ä½¿ç”¨ gemini-2.5-pro ä»¥ç²å¾—æ›´å¥½çš„æ¨ç†èƒ½åŠ›'
    },

    # === GEMINI 1.5 ç³»åˆ— (èˆŠç‰ˆ) ===
    'gemini-1.5-flash': {
        'context_window': 1_000_000,
        'features': ['vision', 'audio', 'video'],
        'speed': 'very_fast',
        'cost': 'very_low',
        'quality': 'good',
        'generation': '1.5',
        'recommended_for': [
            'å¤§é‡æ‡‰ç”¨',
            'ç°¡å–®æŸ¥è©¢',
            'å…§å®¹å¯©æ ¸',
            'å¿«é€Ÿå›æ‡‰'
        ],
        'note': 'è€ƒæ…®ä½¿ç”¨ gemini-2.5-flash ä»¥åœ¨ç›¸ä¼¼æˆæœ¬ä¸‹ç²å¾—æ›´å¥½æ•ˆèƒ½'
    },
    'gemini-1.5-flash-8b': {
        'context_window': 1_000_000,
        'features': ['vision', 'audio'],
        'speed': 'ultra_fast',
        'cost': 'ultra_low',
        'quality': 'moderate',
        'generation': '1.5',
        'recommended_for': [
            'è¶…é«˜ååé‡',
            'ç°¡å–®åˆ†é¡',
            'åŸºæœ¬å•ç­”',
            'å°æˆæœ¬æ•æ„Ÿçš„æ‡‰ç”¨'
        ],
        'note': 'è€ƒæ…®ä½¿ç”¨ gemini-2.5-flash-lite ä»¥ç²å¾—æ›´å¥½æ•ˆèƒ½'
    },
    'gemini-1.5-pro': {
        'context_window': 2_000_000,
        'features': ['vision', 'audio', 'video', 'extended_context'],
        'speed': 'moderate',
        'cost': 'high',
        'quality': 'excellent',
        'generation': '1.5',
        'recommended_for': [
            'é—œéµæ¥­å‹™æ‡‰ç”¨',
            'è¤‡é›œåˆ†æ',
            'å¤§å‹æ–‡ä»¶è™•ç†',
            'æœ€é«˜å“è³ªè¦æ±‚'
        ],
        'note': 'é™¤ééœ€è¦ 2M Token Contextï¼Œå¦å‰‡è€ƒæ…®ä½¿ç”¨ gemini-2.5-pro'
    },

    # === ä¸²æµæ¨¡å‹ ===
    'gemini-2.0-flash-live': {
        'context_window': 'streaming',
        'features': ['vision', 'audio', 'video', 'bidirectional', 'real_time'],
        'speed': 'real_time',
        'cost': 'moderate',
        'quality': 'high',
        'generation': '2.0',
        'recommended_for': [
            'èªéŸ³åŠ©ç†',
            'å³æ™‚å°è©±',
            'å³æ™‚è¦–è¨Šåˆ†æ',
            'äº’å‹•å¼æ‡‰ç”¨'
        ],
        'note': 'åƒ…é™ Vertex AI'
    }
}


def recommend_model(requirements: dict) -> list:
    """
    æ ¹æ“šéœ€æ±‚æ¨è–¦æ¨¡å‹ã€‚

    Args:
        requirements: åŒ…å« 'features', 'speed', 'cost', 'quality' ç­‰éµçš„å­—å…¸

    Returns:
        åŒ…å«æ¨è–¦æ¨¡å‹åç¨±èˆ‡åŸå› çš„åˆ—è¡¨
    """

    recommendations = []

    required_features = set(requirements.get('features', []))
    speed_pref = requirements.get('speed', 'any')
    cost_pref = requirements.get('cost', 'any')
    quality_pref = requirements.get('quality', 'any')

    for model_name, model_info in MODELS.items():
        # æª¢æŸ¥åŠŸèƒ½ç›¸å®¹æ€§
        model_features = set(model_info['features'])
        if required_features and not required_features.issubset(model_features):
            continue

        # æª¢æŸ¥é€Ÿåº¦åå¥½
        if speed_pref != 'any' and model_info['speed'] != speed_pref:
            continue

        # æª¢æŸ¥æˆæœ¬åå¥½
        if cost_pref != 'any' and model_info['cost'] != cost_pref:
            continue

        # æª¢æŸ¥å“è³ªåå¥½
        if quality_pref != 'any' and model_info['quality'] != quality_pref:
            continue

        recommendations.append({
            'model': model_name,
            'reason': model_info['recommended_for'][0],
            'features': model_info['features'],
            'speed': model_info['speed'],
            'cost': model_info['cost']
        })

    return recommendations


# ç¯„ä¾‹ç”¨æ³•
requirements = {
    'features': ['vision', 'thinking'],
    'speed': 'fast',
    'cost': 'low'
}

recommended = recommend_model(requirements)
for rec in recommended:
    print(f"âœ… {rec['model']}")
    print(f"   åŸå› : {rec['reason']}")
    print(f"   é€Ÿåº¦: {rec['speed']}, æˆæœ¬: {rec['cost']}")
```

---

## 2. åŠŸèƒ½ç›¸å®¹æ€§ (Feature Compatibility)

### å…§å»ºå·¥å…·èˆ‡åŠŸèƒ½ (Built-in Tools & Features)

**éœ€è¦ Gemini 2.0+**:

- âœ… æ€è€ƒè¨­å®š (`types.ThinkingConfig`)
- âœ… å…§å»ºç¨‹å¼ç¢¼åŸ·è¡Œ (`BuiltInCodeExecutor`)
- âœ… Google Search åŸºç¤ (åŸç”Ÿ)
- âœ… å¢å¼·çš„å¤šæ¨¡æ…‹èƒ½åŠ›

**æ‰€æœ‰ Gemini æ¨¡å‹**:

- âœ… å‡½å¼å‘¼å«
- âœ… è¦–è¦º (åœ–åƒç†è§£)
- âœ… åŸºæœ¬å¤šæ¨¡æ…‹ (æ–‡å­— + åœ–åƒ)
- âœ… è‡ªè¨‚å·¥å…·

**åƒ…é™ Live API æ¨¡å‹**:

- âœ… é›™å‘ä¸²æµ
- âœ… å³æ™‚éŸ³è¨Š/è¦–è¨Š
- âœ… ä¸»å‹•å›æ‡‰
- âœ… æƒ…æ„Ÿå°è©± (æƒ…ç·’åµæ¸¬)

### åŠŸèƒ½ç›¸å®¹æ€§è¡¨æ ¼ (Feature Compatibility Table)

```python
FEATURE_COMPATIBILITY = {
    'function_calling': ['all'],
    'vision': ['all'],
    'audio_input': ['all'],

    # Gemini 2.5+ åŠŸèƒ½
    'thinking_config': [
        'gemini-2.5-flash',      # æ–°ï¼šé¦–æ¬¾å…·å‚™æ€è€ƒèƒ½åŠ›çš„ Flash æ¨¡å‹ï¼
        'gemini-2.5-pro',
        'gemini-2.0-flash',
        'gemini-2.0-flash-thinking'
    ],
    'image_generation': [
        'gemini-2.5-flash',      # æ–°ï¼šåŸç”Ÿåœ–åƒç”Ÿæˆ
        'gemini-2.5-flash-image'
    ],
    'code_execution': [
        'gemini-2.5-flash',
        'gemini-2.5-pro',
        'gemini-2.0-flash',
        'gemini-2.0-flash-thinking'
    ],
    'google_search': [
        'gemini-2.5-flash',
        'gemini-2.5-pro',
        'gemini-2.0-flash',
        'gemini-2.0-flash-thinking'
    ],

    # è¦–è¨Šæ”¯æ´
    'video_input': [
        'gemini-2.5-flash',
        'gemini-2.5-pro',
        'gemini-1.5-pro',
        'gemini-1.5-flash',
        'gemini-2.0-flash',
        'gemini-2.0-flash-live'
    ],

    # ä¸²æµ
    'bidirectional_streaming': [
        'gemini-2.0-flash-live',
        'gemini-live-2.5-flash'
    ],

    # Context windows
    'extended_context': ['gemini-1.5-pro'],  # 2M tokens
    'long_context': [  # 1M tokens
        'gemini-2.5-flash',
        'gemini-2.5-pro',
        'gemini-2.5-flash-lite',
        'gemini-2.0-flash',
        'gemini-1.5-flash',
        'gemini-1.5-flash-8b'
    ],

    # é€Ÿåº¦ç­‰ç´š
    'ultra_fast': ['gemini-2.5-flash-lite', 'gemini-1.5-flash-8b']
}


def check_feature_support(model: str, feature: str) -> bool:
    """æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æ´è©²åŠŸèƒ½ã€‚"""

    if feature not in FEATURE_COMPATIBILITY:
        return False

    supported_models = FEATURE_COMPATIBILITY[feature]

    if 'all' in supported_models:
        return True

    return model in supported_models


# ç¯„ä¾‹
print(check_feature_support('gemini-2.5-flash', 'thinking_config'))  # True âœ…
print(check_feature_support('gemini-2.0-flash', 'thinking_config'))  # True âœ…
print(check_feature_support('gemini-1.5-flash', 'thinking_config'))  # False âŒ
print(check_feature_support('gemini-2.5-flash', 'image_generation')) # True âœ…
```

---

## 3. çœŸå¯¦ä¸–ç•Œç¯„ä¾‹ï¼šæ¨¡å‹é¸æ“‡æ¡†æ¶ (Real-World Example: Model Selection Framework)

è®“æˆ‘å€‘å»ºç«‹ä¸€å€‹å…¨é¢çš„æ¨¡å‹é¸æ“‡èˆ‡æ¸¬è©¦æ¡†æ¶ã€‚

### å®Œæ•´å¯¦ä½œ (Complete Implementation)

```python
"""
æ¨¡å‹é¸æ“‡èˆ‡æ¸¬è©¦æ¡†æ¶
å¹«åŠ©é¸æ“‡æ­£ç¢ºçš„æ¨¡å‹ä¸¦é€²è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦ã€‚
"""

import asyncio
import time
from dataclasses import dataclass
from typing import Dict, List
from google.adk.agents import Agent, Runner
from google.genai import types


@dataclass
class ModelBenchmark:
    """æ¨¡å‹çš„åŸºæº–æ¸¬è©¦çµæœã€‚"""
    model: str
    avg_latency: float
    avg_tokens: int
    quality_score: float
    cost_estimate: float
    success_rate: float


class ModelSelector:
    """é¸æ“‡èˆ‡åŸºæº–æ¸¬è©¦æ¨¡å‹çš„æ¡†æ¶ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹é¸æ“‡å™¨ã€‚"""
        self.runner = Runner()
        self.benchmarks: Dict[str, ModelBenchmark] = {}

    async def benchmark_model(
        self,
        model: str,
        test_queries: List[str],
        instruction: str
    ) -> ModelBenchmark:
        """
        å°æ¸¬è©¦æŸ¥è©¢é€²è¡Œæ¨¡å‹åŸºæº–æ¸¬è©¦ã€‚

        Args:
            model: è¦æ¸¬è©¦çš„æ¨¡å‹
            test_queries: æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨
            instruction: Agent æŒ‡ä»¤

        Returns:
            åŒ…å«çµæœçš„ ModelBenchmark
        """

        print(f"\n{'='*70}")
        print(f"åŸºæº–æ¸¬è©¦ä¸­: {model}")
        print(f"{'='*70}\n")

        # ä½¿ç”¨æ­¤æ¨¡å‹å»ºç«‹ agent
        agent = Agent(
            model=model,
            name=f'test_agent_{model.replace(".", "_")}',
            instruction=instruction,
            generate_content_config=types.GenerateContentConfig(
                temperature=0.5,
                max_output_tokens=1024
            )
        )

        latencies = []
        token_counts = []
        successes = 0

        for query in test_queries:
            try:
                start = time.time()

                result = await self.runner.run_async(query, agent=agent)

                latency = time.time() - start
                latencies.append(latency)

                # ä¼°ç®— token æ•¸é‡ (ç²—ç•¥)
                text = result.content.parts[0].text
                token_count = len(text.split())
                token_counts.append(token_count)

                successes += 1

                print(f"âœ… æŸ¥è©¢: {query[:50]}...")
                print(f"   å»¶é²: {latency:.2f}s, Tokens: ~{token_count}")

            except Exception as e:
                print(f"âŒ æŸ¥è©¢å¤±æ•—: {query[:50]}... - {e}")

        # è¨ˆç®—æŒ‡æ¨™
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
        success_rate = successes / len(test_queries)

        # ä¼°ç®—æˆæœ¬ (ç°¡åŒ–)
        # å¯¦éš›å®šåƒ¹å„ä¸ç›¸åŒ - è«‹æŸ¥çœ‹ Google Cloud å®šåƒ¹
        cost_per_1k_tokens = {
            'gemini-2.0-flash': 0.0001,
            'gemini-1.5-flash': 0.00008,
            'gemini-1.5-flash-8b': 0.00004,
            'gemini-1.5-pro': 0.0005
        }

        model_key = model
        if model_key not in cost_per_1k_tokens:
            model_key = 'gemini-2.0-flash'

        cost_estimate = (avg_tokens / 1000) * cost_per_1k_tokens[model_key]

        # å“è³ªåˆ†æ•¸ (åŸºæ–¼æˆåŠŸç‡å’Œå»¶é²)
        quality_score = success_rate * (1.0 / (1.0 + avg_latency))

        benchmark = ModelBenchmark(
            model=model,
            avg_latency=avg_latency,
            avg_tokens=int(avg_tokens),
            quality_score=quality_score,
            cost_estimate=cost_estimate,
            success_rate=success_rate
        )

        self.benchmarks[model] = benchmark

        print(f"\nğŸ“Š çµæœ:")
        print(f"   å¹³å‡å»¶é²: {avg_latency:.2f}s")
        print(f"   å¹³å‡ Tokens: {avg_tokens:.0f}")
        print(f"   æˆåŠŸç‡: {success_rate*100:.1f}%")
        print(f"   ä¼°ç®—æˆæœ¬: ${cost_estimate:.6f} æ¯æ¬¡æŸ¥è©¢")
        print(f"   å“è³ªåˆ†æ•¸: {quality_score:.3f}")

        return benchmark

    async def compare_models(
        self,
        models: List[str],
        test_queries: List[str],
        instruction: str
    ):
        """
        åœ¨ç›¸åŒæŸ¥è©¢ä¸Šæ¯”è¼ƒå¤šå€‹æ¨¡å‹ã€‚

        Args:
            models: è¦æ¯”è¼ƒçš„æ¨¡å‹åˆ—è¡¨
            test_queries: æ¸¬è©¦æŸ¥è©¢
            instruction: Agent æŒ‡ä»¤
        """

        print(f"\n{'#'*70}")
        print(f"æ¨¡å‹æ¯”è¼ƒ")
        print(f"{'#'*70}\n")

        for model in models:
            await self.benchmark_model(model, test_queries, instruction)
            await asyncio.sleep(2)

        self._print_comparison()

    def _print_comparison(self):
        """åˆ—å°æ¯”è¼ƒè¡¨ã€‚"""

        print(f"\n\n{'='*70}")
        print("æ¯”è¼ƒæ‘˜è¦")
        print(f"{'='*70}\n")

        print(f"{'æ¨¡å‹':<30} {'å»¶é²':>10} {'Tokens':>8} {'æˆæœ¬':>10} {'å“è³ª':>10}")
        print(f"{'-'*70}")

        for model, bench in self.benchmarks.items():
            print(f"{model:<30} {bench.avg_latency:>9.2f}s {bench.avg_tokens:>8} "
                  f"${bench.cost_estimate:>9.6f} {bench.quality_score:>10.3f}")

        print(f"\n{'='*70}")

        # å»ºè­°
        print("\nğŸ¯ å»ºè­°:\n")

        fastest = min(self.benchmarks.items(), key=lambda x: x[1].avg_latency)
        print(f"âš¡ æœ€å¿«: {fastest[0]} ({fastest[1].avg_latency:.2f}s)")

        cheapest = min(self.benchmarks.items(), key=lambda x: x[1].cost_estimate)
        print(f"ğŸ’° æœ€ä¾¿å®œ: {cheapest[0]} (${cheapest[1].cost_estimate:.6f})")

        best_quality = max(self.benchmarks.items(), key=lambda x: x[1].quality_score)
        print(f"ğŸ† æœ€ä½³å“è³ª: {best_quality[0]} ({best_quality[1].quality_score:.3f})")

    def recommend_model_for_use_case(self, use_case: str) -> str:
        """
        æ ¹æ“šä½¿ç”¨æƒ…å¢ƒæ¨è–¦æ¨¡å‹ (å·²ç‚º Gemini 2.5 æ›´æ–°)ã€‚

        Args:
            use_case: ä½¿ç”¨æƒ…å¢ƒæè¿°

        Returns:
            æ¨è–¦çš„æ¨¡å‹åç¨±
        """

        use_case_lower = use_case.lower()

        # åŸºæ–¼è¦å‰‡çš„å»ºè­° (Gemini 2.5 ç³»åˆ—)
        if 'real-time' in use_case_lower or 'voice' in use_case_lower:
            return 'gemini-2.0-flash-live'

        elif 'complex' in use_case_lower or 'reasoning' in use_case_lower or 'stem' in use_case_lower:
            return 'gemini-2.5-pro'  # æ–°ï¼šæœ€é©åˆè¤‡é›œå•é¡Œ

        elif 'high-volume' in use_case_lower or 'simple' in use_case_lower or 'ultra-fast' in use_case_lower:
            return 'gemini-2.5-flash-lite'  # æ–°ï¼šæœ€å¿« + æœ€ä¾¿å®œ

        elif 'critical' in use_case_lower or 'important' in use_case_lower:
            return 'gemini-2.5-pro'  # æ–°ï¼šæœ€é«˜å“è³ª

        elif 'extended context' in use_case_lower or 'large document' in use_case_lower:
            return 'gemini-1.5-pro'  # ä»æœ‰ 2M context

        else:
            return 'gemini-2.5-flash'  # æ–°çš„é è¨­ï¼


async def main():
    """ä¸»é€²å…¥é»ã€‚"""

    selector = ModelSelector()

    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "æ³•åœ‹çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ",
        "ç”¨ç°¡å–®çš„è¡“èªè§£é‡‹é‡å­è¨ˆç®—",
        "å¯«ä¸€é¦–é—œæ–¼äººå·¥æ™ºæ…§çš„ä¿³å¥",
        "è¨ˆç®—ä¸€è¬ç¾å…ƒä»¥ 5% çš„åˆ©ç‡è¤‡åˆ© 10 å¹´çš„åˆ©æ¯",
        "åˆ—å‡º 2025 å¹´å‰äº”åçš„ç¨‹å¼èªè¨€"
    ]

    instruction = """
æ‚¨æ˜¯ä¸€ä½æ¨‚æ–¼åŠ©äººçš„åŠ©ç†ã€‚è«‹æº–ç¢ºç°¡æ½”åœ°å›ç­”å•é¡Œã€‚
    """.strip()

    # æ¯”è¼ƒæ¨¡å‹ (å·²ç‚º Gemini 2.5 æ›´æ–°)
    models_to_test = [
        'gemini-2.5-flash',      # æ–°çš„é è¨­ - æœ€ä½³æ€§åƒ¹æ¯”
        'gemini-2.5-pro',        # æ–° - æœ€é«˜å“è³ª
        'gemini-2.5-flash-lite', # æ–° - è¶…å¿«
        'gemini-2.0-flash',      # èˆŠç‰ˆ
        'gemini-1.5-flash',      # èˆŠç‰ˆ
    ]

    await selector.compare_models(models_to_test, test_queries, instruction)

    # ä½¿ç”¨æƒ…å¢ƒå»ºè­°
    print(f"\n\n{'='*70}")
    print("ä½¿ç”¨æƒ…å¢ƒå»ºè­°")
    print(f"{'='*70}\n")

    use_cases = [
        "å³æ™‚èªéŸ³åŠ©ç†",
        "è¤‡é›œç­–ç•¥è¦åŠƒ",
        "å¤§é‡å…§å®¹å¯©æ ¸",
        "é—œéµæ¥­å‹™æ±ºç­–æ”¯æ´",
        "ä¸€èˆ¬å®¢æˆ¶æœå‹™"
    ]

    for use_case in use_cases:
        recommendation = selector.recommend_model_for_use_case(use_case)
        print(f"ğŸ“Œ {use_case}")
        print(f"   â†’ å»ºè­°: {recommendation}\n")


if __name__ == '__main__':
    asyncio.run(main())
```

### é æœŸè¼¸å‡º (Expected Output)

```
======================================================================
åŸºæº–æ¸¬è©¦ä¸­: gemini-2.0-flash
======================================================================

âœ… æŸ¥è©¢: æ³•åœ‹çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ...
   å»¶é²: 0.85s, Tokens: ~8
âœ… æŸ¥è©¢: ç”¨ç°¡å–®çš„è¡“èªè§£é‡‹é‡å­è¨ˆç®—...
   å»¶é²: 1.23s, Tokens: ~95
âœ… æŸ¥è©¢: å¯«ä¸€é¦–é—œæ–¼äººå·¥æ™ºæ…§çš„ä¿³å¥...
   å»¶é²: 0.92s, Tokens: ~25
âœ… æŸ¥è©¢: è¨ˆç®—ä¸€è¬ç¾å…ƒä»¥ 5% çš„åˆ©ç‡è¤‡åˆ© 10 å¹´...
   å»¶é²: 1.15s, Tokens: ~42
âœ… æŸ¥è©¢: åˆ—å‡º 2025 å¹´å‰äº”åçš„ç¨‹å¼èªè¨€...
   å»¶é²: 0.98s, Tokens: ~35

ğŸ“Š çµæœ:
   å¹³å‡å»¶é²: 1.03s
   å¹³å‡ Tokens: 41
   æˆåŠŸç‡: 100.0%
   ä¼°ç®—æˆæœ¬: $0.000004 æ¯æ¬¡æŸ¥è©¢
   å“è³ªåˆ†æ•¸: 0.493

======================================================================
åŸºæº–æ¸¬è©¦ä¸­: gemini-1.5-flash
======================================================================

âœ… æŸ¥è©¢: æ³•åœ‹çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ...
   å»¶é²: 0.72s, Tokens: ~7
âœ… æŸ¥è©¢: ç”¨ç°¡å–®çš„è¡“èªè§£é‡‹é‡å­è¨ˆç®—...
   å»¶é²: 1.05s, Tokens: ~88
âœ… æŸ¥è©¢: å¯«ä¸€é¦–é—œæ–¼äººå·¥æ™ºæ…§çš„ä¿³å¥...
   å»¶é²: 0.78s, Tokens: ~22
âœ… æŸ¥è©¢: è¨ˆç®—ä¸€è¬ç¾å…ƒä»¥ 5% çš„åˆ©ç‡è¤‡åˆ© 10 å¹´...
   å»¶é²: 0.95s, Tokens: ~38
âœ… æŸ¥è©¢: åˆ—å‡º 2025 å¹´å‰äº”åçš„ç¨‹å¼èªè¨€...
   å»¶é²: 0.82s, Tokens: ~32

ğŸ“Š çµæœ:
   å¹³å‡å»¶é²: 0.86s
   å¹³å‡ Tokens: 37
   æˆåŠŸç‡: 100.0%
   ä¼°ç®—æˆæœ¬: $0.000003 æ¯æ¬¡æŸ¥è©¢
   å“è³ªåˆ†æ•¸: 0.537

======================================================================
åŸºæº–æ¸¬è©¦ä¸­: gemini-1.5-flash-8b
======================================================================

âœ… æŸ¥è©¢: æ³•åœ‹çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ...
   å»¶é²: 0.58s, Tokens: ~6
âœ… æŸ¥è©¢: ç”¨ç°¡å–®çš„è¡“èªè§£é‡‹é‡å­è¨ˆç®—...
   å»¶é²: 0.89s, Tokens: ~75
âœ… æŸ¥è©¢: å¯«ä¸€é¦–é—œæ–¼äººå·¥æ™ºæ…§çš„ä¿³å¥...
   å»¶é²: 0.65s, Tokens: ~20
âœ… æŸ¥è©¢: è¨ˆç®—ä¸€è¬ç¾å…ƒä»¥ 5% çš„åˆ©ç‡è¤‡åˆ© 10 å¹´...
   å»¶é²: 0.78s, Tokens: ~32
âœ… æŸ¥è©¢: åˆ—å‡º 2025 å¹´å‰äº”åçš„ç¨‹å¼èªè¨€...
   å»¶é²: 0.68s, Tokens: ~28

ğŸ“Š çµæœ:
   å¹³å‡å»¶é²: 0.72s
   å¹³å‡ Tokens: 32
   æˆåŠŸç‡: 100.0%
   ä¼°ç®—æˆæœ¬: $0.000001 æ¯æ¬¡æŸ¥è©¢
   å“è³ªåˆ†æ•¸: 0.581


======================================================================
æ¯”è¼ƒæ‘˜è¦
======================================================================

æ¨¡å‹                            å»¶é²     Tokens       æˆæœ¬      å“è³ª
----------------------------------------------------------------------
gemini-2.0-flash                    1.03s       41 $0.000004      0.493
gemini-1.5-flash                    0.86s       37 $0.000003      0.537
gemini-1.5-flash-8b                 0.72s       32 $0.000001      0.581

======================================================================

ğŸ¯ å»ºè­°:

âš¡ æœ€å¿«: gemini-1.5-flash-8b (0.72s)
ğŸ’° æœ€ä¾¿å®œ: gemini-1.5-flash-8b ($0.000001)
ğŸ† æœ€ä½³å“è³ª: gemini-1.5-flash-8b (0.581)


======================================================================
ä½¿ç”¨æƒ…å¢ƒå»ºè­°
======================================================================

ğŸ“Œ å³æ™‚èªéŸ³åŠ©ç†
   â†’ å»ºè­°: gemini-2.0-flash-live

ğŸ“Œ è¤‡é›œç­–ç•¥è¦åŠƒ
   â†’ å»ºè­°: gemini-2.0-flash-thinking

ğŸ“Œ å¤§é‡å…§å®¹å¯©æ ¸
   â†’ å»ºè­°: gemini-1.5-flash-8b

ğŸ“Œ é—œéµæ¥­å‹™æ±ºç­–æ”¯æ´
   â†’ å»ºè­°: gemini-1.5-pro

ğŸ“Œ ä¸€èˆ¬å®¢æˆ¶æœå‹™
   â†’ å»ºè­°: gemini-2.0-flash
```

---

## 4. æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹ (Model Selection Decision Tree)

### æ±ºç­–æ¡†æ¶ (Decision Framework)

```python
def select_model(requirements: dict) -> str:
    """
    æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹ (å·²ç‚º Gemini 2.5 æ›´æ–°)ã€‚

    Args:
        requirements: åŒ…å«ä»¥ä¸‹å…§å®¹çš„å­—å…¸ï¼š
            - real_time: bool
            - complex_reasoning: bool (STEM, æ•¸å­¸, ç¨‹å¼ç¢¼)
            - high_volume: bool
            - vision_required: bool
            - code_execution: bool
            - budget_sensitive: bool
            - ultra_fast: bool
            - critical: bool

    Returns:
        æ¨è–¦çš„æ¨¡å‹åç¨±
    """

    # å³æ™‚éœ€æ±‚ (ä¸²æµ)
    if requirements.get('real_time', False):
        return 'gemini-2.0-flash-live'

    # è¤‡é›œæ¨ç† (STEM, ç ”ç©¶, æ·±åº¦åˆ†æ)
    if requirements.get('complex_reasoning', False):
        return 'gemini-2.5-pro'  # æ–°ï¼šæœ€é©åˆè¤‡é›œå•é¡Œ

    # è¶…å¿«éœ€æ±‚èˆ‡é ç®—é™åˆ¶
    if requirements.get('ultra_fast', False) and requirements.get('budget_sensitive', False):
        return 'gemini-2.5-flash-lite'  # æ–°ï¼šæœ€å¿« + æœ€ä¾¿å®œ

    # å¤§é‡ + å°é ç®—æ•æ„Ÿ (ç°¡å–®ä»»å‹™)
    if requirements.get('high_volume', False) and requirements.get('budget_sensitive', False):
        return 'gemini-2.5-flash-lite'  # æ–°ï¼šå–ä»£ 1.5-flash-8b

    # éœ€è¦æœ€é«˜å“è³ªçš„é—œéµæ‡‰ç”¨
    if requirements.get('critical', False):
        return 'gemini-2.5-pro'  # æ–°ï¼šæœ€å…ˆé€²çš„å“è³ª

    # æ“´å±• context (>1M tokens)
    if requirements.get('extended_context', False):
        return 'gemini-1.5-pro'  # ä»æœ‰ 2M token context

    # é è¨­ï¼šæœ€ä½³æ€§åƒ¹æ¯” (æ–°çš„é è¨­ï¼)
    return 'gemini-2.5-flash'


# ç¯„ä¾‹ (å·²ç‚º 2.5 æ›´æ–°)
print(select_model({'real_time': True}))
# è¼¸å‡º: gemini-2.0-flash-live

print(select_model({'complex_reasoning': True}))
# è¼¸å‡º: gemini-2.5-pro  â† å¾ 2.0-flash-thinking æ›´æ”¹

print(select_model({'high_volume': True, 'budget_sensitive': True}))
# è¼¸å‡º: gemini-2.5-flash-lite  â† å¾ 1.5-flash-8b æ›´æ”¹

print(select_model({}))  # ç„¡éœ€æ±‚
# è¼¸å‡º: gemini-2.5-flash  â† æ–°çš„é è¨­ï¼
```

---

## 5. é€é LiteLLM ä½¿ç”¨å…¶ä»– LLM (Using Other LLMs with LiteLLM)

**ä¾†æº**: `google/adk/models/lite_llm.py`

é›–ç„¶ Gemini æ¨¡å‹ç‚º ADK é€²è¡Œäº†æœ€ä½³åŒ–ä¸¦æä¾›æœ€ä½³æ•´åˆï¼Œä½†æ‚¨å¯ä»¥é€é **LiteLLM** ä½¿ç”¨**ä»»ä½• LLM ä¾›æ‡‰å•†**ã€‚`LiteLlm` é¡åˆ¥åŒ…è£äº† LiteLLM å‡½å¼åº«ï¼Œä»¥æä¾›å° OpenAIã€Anthropicã€Ollamaã€Azure ç­‰çš„çµ±ä¸€å­˜å–ã€‚

### ğŸŒŸ ç‚ºä½•ä½¿ç”¨ LiteLLMï¼Ÿ (Why Use LiteLLM?)

- âœ… **ä¾›æ‡‰å•†å½ˆæ€§**: ç„¡éœ€æ›´æ”¹ç¨‹å¼ç¢¼å³å¯åœ¨ OpenAIã€Claudeã€Ollamaã€Azure ä¹‹é–“åˆ‡æ›
- âœ… **æˆæœ¬æœ€ä½³åŒ–**: æ¯”è¼ƒä¾›æ‡‰å•†ä¸¦é¸æ“‡æœ€ä½³æ€§åƒ¹æ¯”
- âœ… **æœ¬åœ°æ¨¡å‹**: åœ¨æœ¬åœ°åŸ·è¡Œ Ollama æ¨¡å‹ä»¥ä¿è­·éš±ç§/åˆè¦æ€§
- âœ… **å‚™æ´ç­–ç•¥**: ä½¿ç”¨å¤šå€‹ä¾›æ‡‰å•†ä»¥æé«˜å¯é æ€§
- âœ… **çµ±ä¸€ä»‹é¢**: ç›¸åŒçš„ ADK ç¨‹å¼ç¢¼é©ç”¨æ–¼ä»»ä½• LLM

### åŸºæœ¬ LiteLLM æ•´åˆ (Basic LiteLLM Integration)

```python
from google.adk.models.lite_llm import LiteLlm
from google.adk.agents import Agent

# ä½¿ç”¨ OpenAI GPT-4 å»ºç«‹ agent
agent = Agent(
    model=LiteLlm(model='openai/gpt-4o'),
    name='openai_agent',
    instruction='æ‚¨æ˜¯ä¸€ä½æ¨‚æ–¼åŠ©äººçš„åŠ©ç†ã€‚'
)

# æˆ–ä½¿ç”¨ Anthropic Claude
agent = Agent(
    model=LiteLlm(model='anthropic/claude-3-7-sonnet'),
    name='claude_agent',
    instruction='æ‚¨æ˜¯ä¸€ä½æ¨‚æ–¼åŠ©äººçš„åŠ©ç†ã€‚'
)
```

### æ”¯æ´çš„ä¾›æ‡‰å•† (Supported Providers)

#### OpenAI

```python
from google.adk.models.lite_llm import LiteLlm
import os

# è¨­å®š API é‡‘é‘°
os.environ['OPENAI_API_KEY'] = 'sk-...'

# ä½¿ç”¨ GPT-4o
model = LiteLlm(model='openai/gpt-4o')

# ä½¿ç”¨ GPT-4o-mini (æ›´å¿«ã€æ›´ä¾¿å®œ)
model = LiteLlm(model='openai/gpt-4o-mini')

# ä½¿ç”¨ GPT-3.5-turbo
model = LiteLlm(model='openai/gpt-3.5-turbo')
```

**ä½¿ç”¨æ™‚æ©Ÿ**:

- éœ€è¦ GPT-4 ä»¥èˆ‡ç¾æœ‰ç³»çµ±ç›¸å®¹
- OpenAI ç‰¹å®šåŠŸèƒ½ï¼Œå¦‚ DALL-E æ•´åˆ
- æˆæœ¬æ¯”è¼ƒ (GPT-4o-mini å¯èƒ½æ¯” Gemini Pro ä¾¿å®œ)

#### Anthropic Claude

```python
import os

# è¨­å®š API é‡‘é‘°
os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'

# ä½¿ç”¨ Claude 3.7 Sonnet
model = LiteLlm(model='anthropic/claude-3-7-sonnet')

# ä½¿ç”¨ Claude 3 Opus (æœ€é«˜å“è³ª)
model = LiteLlm(model='anthropic/claude-3-opus')

# ä½¿ç”¨ Claude 3 Haiku (æœ€å¿«ã€æœ€ä¾¿å®œ)
model = LiteLlm(model='anthropic/claude-3-haiku')
```

**ä½¿ç”¨æ™‚æ©Ÿ**:

- éœ€è¦ Claude ç‰¹å®šèƒ½åŠ› (é•·ç¯‡å¯«ä½œã€ç¨‹å¼ç¢¼åˆ†æ)
- Anthropic çš„æ†²æ³• AI æ–¹æ³•
- ç‰¹å®šä»»å‹™çš„æˆæœ¬æ¯”è¼ƒ

#### Ollama (æœ¬åœ°æ¨¡å‹)

```python
import os

# è¨­å®š Ollama åŸºç¤ URL
os.environ['OLLAMA_API_BASE'] = 'http://localhost:11434'

# âš ï¸ é—œéµï¼šä½¿ç”¨ 'ollama_chat' å‰ç¶´ï¼Œè€Œé 'ollama'
model = LiteLlm(model='ollama_chat/llama3.3')

# å…¶ä»–ç†±é–€æ¨¡å‹
model = LiteLlm(model='ollama_chat/mistral-small3.1')
model = LiteLlm(model='ollama_chat/codellama')
model = LiteLlm(model='ollama_chat/phi4')
```

**å¸¸è¦‹é™·é˜±**:

```python
# âŒ éŒ¯èª¤ - æœƒå‡ºç¾ç¥ç§˜éŒ¯èª¤
model = LiteLlm(model='ollama/llama3.3')

# âœ… æ­£ç¢º - å¿…é ˆä½¿ç”¨ 'ollama_chat'
model = LiteLlm(model='ollama_chat/llama3.3')
```

**ä½¿ç”¨æ™‚æ©Ÿ**:

- éš±ç§è¦æ±‚ (è³‡æ–™ä¿ç•™åœ¨æœ¬åœ°)
- åˆè¦æ€§æ³•è¦ (ç„¡è³‡æ–™å‚³é€è‡³é›²ç«¯)
- ç¯€çœæˆæœ¬ (ç„¡ API è²»ç”¨)
- é›¢ç·š/æ°£éš™ç’°å¢ƒ

**ç¯„ä¾‹**: `contributing/samples/hello_world_ollama/agent.py`

#### Azure OpenAI

```python
import os

# è¨­å®š Azure æ†‘è­‰
os.environ['AZURE_API_KEY'] = 'your-azure-key'
os.environ['AZURE_API_BASE'] = 'https://your-resource.openai.azure.com/'
os.environ['AZURE_API_VERSION'] = '2024-02-01'

# ä½¿ç”¨ Azure è¨—ç®¡çš„ GPT-4
model = LiteLlm(model='azure/gpt-4')

# ä½¿ç”¨ Azure è¨—ç®¡çš„ GPT-35-turbo
model = LiteLlm(model='azure/gpt-35-turbo')
```

**ä½¿ç”¨æ™‚æ©Ÿ**:

- ä¼æ¥­ Azure åˆç´„
- Azure åˆè¦æ€§è¦æ±‚
- èˆ‡ Azure æœå‹™æ•´åˆ

#### é€é Vertex AI ä½¿ç”¨ Claude (Claude via Vertex AI)

```python
# é€é Google Cloud ä½¿ç”¨ Anthropic Claude
model = LiteLlm(model='vertex_ai/claude-3-7-sonnet')
```

**ä½¿ç”¨æ™‚æ©Ÿ**:

- ç¾æœ‰çš„ Google Cloud è¨­å®š
- éœ€è¦ Claude ä½†åå¥½ Google è¨ˆè²»
- çµåˆ Gemini + Claude çš„å·¥ä½œæµç¨‹

### å®Œæ•´ç¯„ä¾‹ï¼šå¤šä¾›æ‡‰å•† Agent ç³»çµ± (Complete Example: Multi-Provider Agent System)

```python
"""
ç¯„ä¾‹ï¼šæ¯”è¼ƒä¾†è‡ªå¤šå€‹ LLM ä¾›æ‡‰å•†çš„å›æ‡‰ã€‚
ä¾†æºï¼šcontributing/samples/hello_world_litellm/agent.py
"""
from google.adk.models.lite_llm import LiteLlm
from google.adk.agents import Agent
import asyncio

async def compare_providers():
    """æ¯”è¼ƒç›¸åŒæŸ¥è©¢åœ¨å¤šå€‹ä¾›æ‡‰å•†ä¸Šçš„çµæœã€‚"""

    query = "ç”¨å…©å¥è©±è§£é‡‹é‡å­è¨ˆç®—ã€‚"

    # ä½¿ç”¨ä¸åŒä¾›æ‡‰å•†å»ºç«‹ agent
    gemini_agent = Agent(
        model='gemini-2.5-flash',  # åŸç”Ÿ Gemini (å»ºè­°)
        name='gemini_agent'
    )

    openai_agent = Agent(
        model=LiteLlm(model='openai/gpt-4o'),
        name='openai_agent'
    )

    claude_agent = Agent(
        model=LiteLlm(model='anthropic/claude-3-7-sonnet'),
        name='claude_agent'
    )

    ollama_agent = Agent(
        model=LiteLlm(model='ollama_chat/llama3.3'),
        name='ollama_agent'
    )

    # å¹³è¡ŒæŸ¥è©¢æ‰€æœ‰ä¾›æ‡‰å•†
    responses = await asyncio.gather(
        gemini_agent.run_async(query),
        openai_agent.run_async(query),
        claude_agent.run_async(query),
        ollama_agent.run_async(query),
        return_exceptions=True
    )

    # æ¯”è¼ƒçµæœ
    for agent_name, response in zip(
        ['Gemini', 'OpenAI', 'Claude', 'Ollama'],
        responses
    ):
        if isinstance(response, Exception):
            print(f"{agent_name}: éŒ¯èª¤ - {response}")
        else:
            print(f"\n{agent_name}:")
            print(response.output_text)
            print(f"æ™‚é–“: {response.usage.time_ms}ms")
            print(f"Tokens: {response.usage.total_tokens}")

# åŸ·è¡Œæ¯”è¼ƒ
asyncio.run(compare_providers())
```

### âš ï¸ é‡è¦è­¦å‘Š (Important Warnings)

#### ä¸è¦ï¼šé€é LiteLLM ä½¿ç”¨ Gemini (DON'T: Use Gemini via LiteLLM)

```python
# âŒ éŒ¯èª¤ - ä¸è¦é€é LiteLLM ä½¿ç”¨ Gemini
agent = Agent(
    model=LiteLlm(model='gemini/gemini-2.5-flash'),
    name='bad_agent'
)

# âœ… æ­£ç¢º - ä½¿ç”¨åŸç”Ÿ Gemini é¡åˆ¥
agent = Agent(
    model='gemini-2.5-flash',  # ç›´æ¥ä½¿ç”¨å­—ä¸²
    name='good_agent'
)
```

**åŸå› **: åŸç”Ÿ Gemini æ•´åˆé€Ÿåº¦æ›´å¿«ã€æ›´å¯é ï¼Œä¸¦æ”¯æ´ ADK ç‰¹å®šåŠŸèƒ½ (æ€è€ƒè¨­å®šã€ç¨‹å¼ç¢¼åŸ·è¡Œã€å‡½å¼å‘¼å«æœ€ä½³åŒ–)ã€‚

#### ä¸è¦ï¼šå¿˜è¨˜ `ollama_chat` å‰ç¶´ (DON'T: Forget `ollama_chat` Prefix)

```python
# âŒ éŒ¯èª¤ - æœƒå¤±æ•—
model = LiteLlm(model='ollama/llama3.3')

# âœ… æ­£ç¢º
model = LiteLlm(model='ollama_chat/llama3.3')
```

#### å‹™å¿…ï¼šè¨­å®šç’°å¢ƒè®Šæ•¸ (DO: Set Environment Variables)

```python
# âœ… è‰¯å¥½ - åœ¨å»ºç«‹ agent å‰è¨­å®šæ†‘è­‰
import os

os.environ['OPENAI_API_KEY'] = 'sk-...'
os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'
os.environ['OLLAMA_API_BASE'] = 'http://localhost:11434'

# ç¾åœ¨å»ºç«‹ agents
agent = Agent(model=LiteLlm(model='openai/gpt-4o'))
```

### å®‰è£ (Installation)

LiteLLM æ”¯æ´å…§å»ºæ–¼ ADKï¼š

```bash
# LiteLLM åŒ…å«åœ¨ ADK å®‰è£ä¸­
pip install google-adk

# å°æ–¼ Ollamaï¼Œéœ€å¦å¤–å®‰è£
# å‰å¾€ https://ollama.com ä¸‹è¼‰
ollama pull llama3.3
ollama pull mistral-small3.1
```

### ä½•æ™‚ä½¿ç”¨å„ç¨®æ–¹æ³• (When to Use Each Approach)

**ä½¿ç”¨åŸç”Ÿ Gemini** (model='gemini-2.5-flash'):

- âœ… æ–° Agent çš„é è¨­é¸æ“‡
- âœ… æœ€ä½³æ•ˆèƒ½èˆ‡åŠŸèƒ½
- âœ… æœ€ä½å»¶é²
- âœ… ADK æœ€ä½³åŒ– (æ€è€ƒã€ç¨‹å¼ç¢¼åŸ·è¡Œã€å‡½å¼å‘¼å«)
- âœ… æœ€ä½³æ€§åƒ¹æ¯”

**ä½¿ç”¨ LiteLLM** (model=LiteLlm(...)):

- âœ… éœ€è¦ç‰¹å®šä¾›æ‡‰å•† (GPT-4ã€Claude ç­‰)
- âœ… æœ¬åœ°æ¨¡å‹ä»¥ä¿è­·éš±ç§ (Ollama)
- âœ… è·¨ä¾›æ‡‰å•†çš„æˆæœ¬æ¯”è¼ƒ
- âœ… ç¾æœ‰åˆç´„ (Azure, Anthropic)
- âœ… å¤šä¾›æ‡‰å•†å‚™æ´ç­–ç•¥

---

## 6. æœ€ä½³å¯¦è¸ (Best Practices)

### âœ… å‹™å¿…ï¼šæ°¸é æ˜ç¢ºæŒ‡å®šæ¨¡å‹ (å»ºè­°ï¼šgemini-2.5-flash) (DO: Always Specify Model Explicitly (Recommended: gemini-2.5-flash))

```python
# âœ… è‰¯å¥½ - æ°¸é æ˜ç¢ºæŒ‡å®šæ¨¡å‹ä»¥æ±‚æ¸…æ™°
agent = Agent(
    model='gemini-2.5-flash',  # å»ºè­°ï¼šæœ€ä½³æ€§åƒ¹æ¯”
    name='my_agent'
)

# âŒ ä¸ä½³ - ä¾è³´é è¨­å€¼ (ç©ºå­—ä¸²ï¼Œå¾çˆ¶å±¤ç¹¼æ‰¿)
agent = Agent(name='my_agent')  # æ¨¡å‹é è¨­ç‚º ''ï¼Œå¾çˆ¶å±¤ç¹¼æ‰¿

# âœ… è‰¯å¥½ - æ˜ç¢ºä¸”æœ‰ç›®çš„åœ°é¸æ“‡æ¨¡å‹
# æ ¹æ“šæ‚¨çš„éœ€æ±‚é€²è¡Œæ¸¬è©¦å’Œæœ€ä½³åŒ–ï¼š
# - ä½¿ç”¨ 2.5-flash æ–¼é€šç”¨ç›®çš„ (å»ºè­°)
# - å¦‚æœæ˜¯è¶…ç°¡å–®ä»»å‹™ï¼Œé™ç´šè‡³ 2.5-flash-lite
# - å¦‚æœéœ€è¦è¤‡é›œæ¨ç†ï¼Œå‡ç´šè‡³ 2.5-pro
```

### âœ… å‹™å¿…ï¼šåœ¨ç”Ÿç”¢å‰é€²è¡ŒåŸºæº–æ¸¬è©¦ (DO: Benchmark Before Production)

```python
# âœ… è‰¯å¥½ - åœ¨éƒ¨ç½²å‰æ¸¬è©¦æ¨¡å‹
models = ['gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-1.5-flash-8b']

for model in models:
    agent = Agent(model=model, name='test')
    # åŸ·è¡Œä»£è¡¨æ€§æŸ¥è©¢
    # æ¸¬é‡å»¶é²ã€å“è³ªã€æˆæœ¬
    # é¸æ“‡æœ€é©åˆçš„
```

### âœ… å‹™å¿…ï¼šè€ƒæ…®åŠŸèƒ½éœ€æ±‚ (DO: Consider Feature Requirements)

```python
# âœ… è‰¯å¥½ - æª¢æŸ¥åŠŸèƒ½ç›¸å®¹æ€§
if need_code_execution:
    model = 'gemini-2.0-flash'  # æ”¯æ´ç¨‹å¼ç¢¼åŸ·è¡Œ
elif need_thinking:
    model = 'gemini-2.0-flash-thinking'  # æ“´å±•æ¨ç†
else:
    model = 'gemini-1.5-flash'  # å¿«é€Ÿä¸”ç¶“æ¿Ÿ
```

### âŒ ä¸è¦ï¼šå°ç°¡å–®ä»»å‹™ä½¿ç”¨ Pro æ¨¡å‹ (DON'T: Use Pro for Simple Tasks)

```python
# âŒ ä¸ä½³ - ç‚ºç°¡å–®æŸ¥è©¢æ”¯ä»˜éé«˜è²»ç”¨
agent = Agent(
    model='gemini-1.5-pro',  # æ˜‚è²´
    instruction="å›ç­”æ˜¯æˆ–å¦çš„å•é¡Œ"  # ç°¡å–®ä»»å‹™
)

# âœ… è‰¯å¥½ - å°‡è¤‡é›œåº¦èˆ‡æ¨¡å‹åŒ¹é…
agent = Agent(
    model='gemini-1.5-flash-8b',  # ç¶“æ¿Ÿ
    instruction="å›ç­”æ˜¯æˆ–å¦çš„å•é¡Œ"
)
```

---

## æ‘˜è¦ (Summary)

æ‚¨å·²æŒæ¡æ¨¡å‹é¸æ“‡èˆ‡æœ€ä½³åŒ–ï¼š

**é‡é»æ‘˜è¦**:

- â­ **`gemini-2.5-flash` ç‚ºå»ºè­°é¸é …** - æœ€ä½³æ€§åƒ¹æ¯”ï¼Œé¦–æ¬¾å…·å‚™æ€è€ƒèƒ½åŠ›çš„ Flash æ¨¡å‹
- âœ… **æ°¸é æ˜ç¢ºæŒ‡å®šæ¨¡å‹** - é è¨­ç‚ºç©ºå­—ä¸² (å¾çˆ¶å±¤ç¹¼æ‰¿)
- âœ… `gemini-2.5-pro` ç”¨æ–¼ç¨‹å¼ç¢¼ã€æ•¸å­¸ã€STEM çš„è¤‡é›œæ¨ç†
- âœ… `gemini-2.5-flash-lite` ç”¨æ–¼è¶…å¿«ã€æˆæœ¬æ•ˆç›Šé«˜çš„é«˜ååé‡
- âœ… `gemini-2.0-flash` å’Œ `gemini-1.5-*` æ¨¡å‹ä»ç„¶å¯ç”¨ (èˆŠç‰ˆ)
- âœ… éœ€è¦æ™‚å¯é€é **LiteLLM æ”¯æ´** OpenAIã€Claudeã€Ollamaã€Azure
- âœ… å»ºè­°ä½¿ç”¨åŸç”Ÿ Gemini è€Œéé€é LiteLLM çš„ Gemini
- âœ… åœ¨ç”Ÿç”¢éƒ¨ç½²å‰å°æ¨¡å‹é€²è¡ŒåŸºæº–æ¸¬è©¦
- âœ… è€ƒæ…®æˆæœ¬ã€æ•ˆèƒ½ã€åŠŸèƒ½èˆ‡ä¾›æ‡‰å•†éœ€æ±‚çš„æ¬Šè¡¡

**ç”Ÿç”¢æª¢æŸ¥æ¸…å–®**:

- [ ] æ ¹æ“šéœ€æ±‚é¸æ“‡æ¨¡å‹ (å»ºè­°ï¼šgemini-2.5-flash)
- [ ] åœ¨ Agent å»ºæ§‹å‡½å¼ä¸­æ˜ç¢ºæŒ‡å®šæ¨¡å‹ (ä¸è¦ä¾è³´é è¨­å€¼)
- [ ] å·²åœ¨ä»£è¡¨æ€§æŸ¥è©¢ä¸Šå®ŒæˆåŸºæº–æ¸¬è©¦
- [ ] å·²é©—è­‰åŠŸèƒ½ç›¸å®¹æ€§ (2.5 Flash å…·å‚™æ€è€ƒèƒ½åŠ›ï¼)
- [ ] å·²è¨ˆç®—æˆæœ¬é æ¸¬
- [ ] å·²å®šç¾©æ•ˆèƒ½æœå‹™ç­‰ç´šå”å®š (SLA)
- [ ] å·²é¸æ“‡ä¾›æ‡‰å•† (Gemini vs LiteLLM ä¾›æ‡‰å•†)
- [ ] å·²è¨­å®šå‚™æ´æ¨¡å‹
- [ ] å·²éƒ¨ç½²æ¨¡å‹ç›£æ§
- [ ] å·²è¦åŠƒé·ç§»ç­–ç•¥ (1.5/2.0 â†’ 2.5)

**æ‚¨å­¸åˆ°äº†**:

1. **`gemini-2.5-flash` ç‚ºå»ºè­°é¸é …** - æ–°å°ˆæ¡ˆçš„æœ€ä½³æ•ˆèƒ½èˆ‡åƒ¹å€¼
2. **æ°¸é æ˜ç¢ºæŒ‡å®šæ¨¡å‹** - é è¨­ç‚ºç©ºå­—ä¸² (å¾çˆ¶å±¤ç¹¼æ‰¿)
3. **å®Œæ•´çš„æ¨¡å‹é™£å®¹** - å¾ 2.5-flash-lite (æœ€å¿«) åˆ° 2.5-pro (æœ€è°æ˜)
4. **LiteLLM æ•´åˆ** - ç•¶éœ€è¦ä¾›æ‡‰å•†å½ˆæ€§æ™‚ä½¿ç”¨ OpenAIã€Claudeã€Ollama
5. **åŸç”Ÿ vs LiteLLM** - æ°¸é åå¥½åŸç”Ÿ Gemini ä»¥ç²å¾—æœ€ä½³æ•ˆèƒ½
6. **é¸æ“‡æ¡†æ¶** - ä½¿ç”¨ MODELS å­—å…¸å’Œ `recommend_model()` é€²è¡Œç³»çµ±æ€§é¸æ“‡

**è³‡æº**:

- [Gemini 2.5 æ–‡ä»¶](https://ai.google.dev/gemini-api/docs/models) - Google AI å®˜æ–¹æ–‡ä»¶
- [Vertex AI Gemini 2.5 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash) - é›²ç«¯æ–‡ä»¶
- [Gemini 2.5 æŠ€è¡“å ±å‘Š](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf) - ç ”ç©¶è«–æ–‡
- [LiteLLM æ–‡ä»¶](https://docs.litellm.ai/) - å¤šä¾›æ‡‰å•†æ•´åˆ
- [å®šåƒ¹è¨ˆç®—æ©Ÿ](https://cloud.google.com/products/calculator) - æˆæœ¬ä¼°ç®—

---

## ç¨‹å¼ç¢¼å¯¦ç¾ (Code Implementation)
- tutorial22ï¼š[ç¨‹å¼ç¢¼é€£çµ](../../../python/agents/tutorial22/)
