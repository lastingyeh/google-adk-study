# 適配器微調

在構建企業級 AI 系統時，我們面臨的核心挑戰之一是如何在不燒毀數百萬美元預算的狀況下，讓通用模型（Foundational Models）學會特定的業務邏輯或品牌風格。**適配器微調 (Adapter Tuning)**，特別是 **LoRA (Low-Rank Adaptation)** 技術，為我們提供了一種優雅且具備成本效益的解決方案。作為架構師，我們不應盲目追求「全量微調 (Full Fine-Tuning)」，而應將重心放在如何保護模型已有的「世界知識」，同時精確地注入「行為模式」。

---

### 情境 1：優先選擇 LoRA 微調而非全量微調以保護模型通用能力

#### 核心概念簡述
全量微調會修改模型的所有參數，這不僅計算成本極高，還容易引發「災難性遺忘 (Catastrophic Forgetting)」，導致模型喪失原本強大的推理與通識能力。LoRA 的做法是將模型權重「凍結 (Frozen)」，僅在模型層次間插入極小的可訓練矩陣（適配器），這能將可訓練參數減少高達 10,000 倍，並顯著降低 GPU 記憶體需求。

#### 程式碼範例（Bad vs. Better）

```python
# ❌ Bad: 試圖更新整個模型的數十億個參數，這需要龐大的集群與數據量
# 容易導致模型在學習新風格後，連基本的加減法都忘記
model.train() # 全量更新所有梯度

# ✅ Better: 實作 LoRA 參數高效微調 (PeFT)
# 僅訓練極小比例的矩陣，保留基礎模型的穩定性
from peft import LoraConfig, get_peft_model

# 配置 LoRA 參數
config = LoraConfig(
    r=16,           # 秩 (Rank): 矩陣的大小，越小參數越少
    lora_alpha=16,  # 縮放因子
    target_modules=["q_proj", "v_proj"], # 鎖定特定的注意力層
    lora_dropout=0.1,
    task_type="CAUSAL_LM"
)

# 注入適配器層，凍結其餘權重
lora_model = get_peft_model(base_model, config)
```

#### 底層原理探討與權衡
*   **為什麼有效 (Rationale)**：LoRA 假設微調過程中的權重更新具備「低秩 (Low Intrinsic Rank)」特性，這意味著我們不需要改變所有神經元，只需捕捉主要的變化趨勢即可。
*   **權衡**：雖然 LoRA 節省了空間，但在某些極其複雜、需要徹底改變模型認知的任務上（例如教模型一個完全不存在的語言），其表現可能略遜於全量微調。

---

### 情境 2：利用微調教導「風格與格式」而非「事實知識」

#### 核心概念簡述
這是一個經典的架構誤區：試圖透過微調讓模型「記住新發生的事實」。
*   **微調 (Fine-tuning)**：教導模型「如何釣魚」（推理邏輯、品牌語氣、JSON 輸出格式）。
*   **RAG (檢索增強)**：給模型「魚」（外部事實數據、最新新聞、保單條款）。
對於醫療摘要、法律契約檢查等任務，Adapter Tuning 應被用於學習特定領域的格式與專業語法。

#### 比較與整合表：微調技術對比

| 特性           | 全量微調 (FFT)      | 適配器微調 (LoRA/PeFT)       |
| :------------- | :------------------ | :--------------------------- |
| **可訓練參數** | 100%                | < 0.1%                       |
| **GPU 需求**   | 極高 (需多張 H100)  | 低 (單張 A100/4090 即可)     |
| **數據量需求** | 數萬筆以上          | 100 - 1,000 筆即可           |
| **部署靈活性** | 低 (需切換整個模型) | 高 (僅需加載幾 MB 的權重檔)  |
| **適合場景**   | 極深度領域特化      | 品牌風格、格式對齊、指令遵循 |

---

### 情境 3：結合 QLoRA 以在邊緣裝置或受限硬體部署

#### 核心概念簡述
當預算受限或需要在地端部署時，我們可以使用 **QLoRA**。它將凍結的基礎模型權重進一步量化 (Quantization) 至 4-bit，這大幅減少了 VRAM 佔用，同時透過 LoRA 適配器保持了微調的精度。

#### 推理流程圖

```mermaid
graph LR
    A[通用基礎模型-凍結] --> B{注入 LoRA 適配器}
    B --> C[輸入特定領域數據 100-1000 筆]
    C --> D[僅訓練適配器層權重]
    D --> E[保存微小的 Adapter 檔案]
    E --> F[推理時: 合併基礎模型與 Adapter]
    F --> G[獲得特定領域專家能力]
```

---

### 延伸思考

**1️⃣ 問題一**：如果我微調了 10 個不同的任務，是否需要部署 10 個完整的大模型？

**👆 回答**：不需要。這是 Adapter Tuning 的巨大優勢。你可以只部署一個凍結的「基礎模型」在記憶體中，並根據請求動態切換不同的「適配器檔案」（通常僅幾十 MB），這實現了多任務併發下的極致內存優化。

---

**2️⃣ 問題二**：如何評估我的 LoRA 微調是否過擬合 (Overfitting)？

**👆 回答**：微調後應使用 **LLM-as-Judge (Pattern 17)** 模式進行測試。準備一組「黃金數據集 (Golden Dataset)」。如果模型在特定任務表現優異，但對一般性問題的回答開始變得重複或單調（Perplexity 異常降低），則說明學習率過高或 Epoch 數過多，觸發了災難性遺忘。

---

**3️⃣ 問題三**：LoRA 的 `Rank (r)` 應該設定多大？

**👆 回答**：這是一門藝術。通常從 `r=8` 或 `r=16` 開始測試。如果任務非常複雜且數據量較多，可以增加到 `r=64` 或更多。但要注意，`r` 越大，可訓練參數越多，微調的計算量與過擬合風險也會隨之增加。